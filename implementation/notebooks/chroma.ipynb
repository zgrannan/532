{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"../pipeline/chroma_langchain_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 2024NOV2_1file_full\n",
      "Number records: 18\n",
      "Name: 2024NOV2_test_exceptionl\n",
      "Number records: 0\n",
      "Name: 2024NOV2_8file_full\n",
      "Number records: 420\n",
      "Name: 2024NOV2_test_exception_handling\n",
      "Number records: 22\n",
      "Name: 2024NOV2_13files_full\n",
      "Number records: 793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(f\"Name: {collection.name}\\nNumber records: {collection.count()}\") for collection in client.list_collections()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all embeddings from the database\n",
    "embeddings = client.list_collections()[0].get(include=[\"embeddings\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-nomic-embed-text-v1.5@f32\" # on LM Studio\n",
    " \n",
    "embeddings_func = OpenAIEmbeddings(\n",
    "                                        model=EMBEDDING_MODEL,\n",
    "                                        base_url=\"http://localhost:1234/v1\",\n",
    "                                        api_key=\"test\",\n",
    "                                        check_embedding_ctx_length=False # https://github.com/langchain-ai/langchain/issues/21318\n",
    "                                    )\n",
    "vector_store = Chroma(\n",
    "    collection_name=client.list_collections()[0].name,\n",
    "    embedding_function=embeddings_func,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'filename': 'Can_CLIP_Count_Stars_An_Empirical_Study_on_Quantity_Bias_in_CLIP_2409.15035v1'}, page_content=' question\\nwhether the CLIP also knows.\\nWe build a small dataset containing 25 specific\\nquantity nouns, ranging from 0 to 100. Two benchmark words for comparable description are used:\\n\\n\\n-----\\n\\nFigure 3: How “more” for selected different words.\\n\\nFigure 4: Distance between different quantity words.\\n\\n’fewer’ and ’more.’ Through this study, we aim\\nto understand how CLIP interprets the concepts of\\n’few’ and ’more.’ We report the results of similarity score for the “fewer” and “more” study in Fig.\\n2 and Fig. 3, respectively. The statistical results\\nreveal some interesting phenomena.\\nFirst, CLIP cannot distinguish different quan_tity words well. As the quantitative word becomes_\\nsmaller or larger, the similarity score with ’fewer’\\nor ’more’ doesn’t decrease or increase gradually.\\nAdditionally, we can clearly see that ’zero’ demonstrates the highest similarity with both ’fewer’ and\\n’more.’ This could be the first evidence showing\\nthat CLIP cannot count and understand quantitative\\nwords well.\\nSecond, the quantity bias in the text modality\\n_is shared across different models. We surprisingly_\\nfind that the change in similarity scores for different\\nmodels follows a similar trend, i.e., the peaks and\\ntroughs, while differing in magnitude. For example,\\nthe values of most models at around ’fifth’ and ’thirteenth’ exhibit the maximum and minimum similarity scores, respectively, when compared with\\nboth ’fewer’ and ’more.’ These results indicate\\n\\n\\nFigure 5: Examples of the images with different number\\nof circles.\\n\\nthat the quantity bias is a systematic error, not a\\nmodel-specific error.\\n\\nTo have a better understanding on the embedding\\nof quantitive words and quantity bias, we compute\\nthe pairwise distance in L2 norm between different\\nwords. The results are shown in Fig. 4. With a\\ndarker color indicating a smaller distance, we can\\nsee that closely neighboring words present high\\nsimilarity at the embedding level, while distant\\nwords demonstrate low similarity. On the one hand,\\n'),\n",
       " Document(metadata={'filename': 'Can_CLIP_Count_Stars_An_Empirical_Study_on_Quantity_Bias_in_CLIP_2409.15035v1'}, page_content='.\\n\\n\\n-----\\n\\nFigure 6: Distance between images with different number of objects.\\n\\n**Take-home message** CLIP cannot understand\\nthe concept related to quantity in either the text\\nor image modality, though for different reasons. In\\nthe text domain, there is high similarity between\\nclosely neighboring quantities, but a large semantic difference with distant quantity words. The\\nirregular and noncontinuous changes between continuous quantity words make comparison difficult,\\nleading to confusion with ’fewer’ and ’more.’ Conversely, in the image domain, images with different\\nnumbers of circles show high semantic similarity,\\nmaking it difficult to differentiate them based on\\nsemantic differences. These factors lead to CLIP’s\\nfailure in understanding quantity.\\n\\n**3.3** **Evaluation on the multi-modal capacity**\\n\\nWe further evaluate the quantity bias in multimodal capacity of CLIP models. We use the quantities comparison words “fewer” and “more” to evaluate the figures with different number of circles\\nintroduced before.\\nWe report the similarity comparison results in\\nFig. 7 and Fig. 8. It can be seen that most models cannot distinguish different images at the embedding level for the ’fewer’ or ’more’ concepts,\\nwith the similarity scores remaining smooth at a\\nlow level. Although the two ViT-L models show a\\nlarge difference in word embedding between different images, they also share the same trend for\\nthe ’fewer’ and ’more’ concepts. This indicates\\nthat these two CLIP models learn the difference\\nbetween different numbers of circles due to larger\\nmodel capacity but still fail to understand quantity.\\n\\n**3.4** **Discussion**\\n\\n**Why this happens? We argue that two factors con-**\\ntribute to the ineffective learning of the quantity\\n\\n\\nFigure 7: How “fewer” for generated objects.\\n\\nFigure 8: How “more” for generated objects.\\n\\nconcept in CLIP models. First, quantity-related\\ndata are heavily limited. There are not enough\\ndata containing explicit quantity information for\\nmodel learning. Many quantity words and quantityrelated visual information are not considered in the\\nmodel learning process, effectively making them\\nout-of-distribution data. Second'),\n",
       " Document(metadata={'filename': 'Can_CLIP_Count_Stars_An_Empirical_Study_on_Quantity_Bias_in_CLIP_2409.15035v1'}, page_content=' images with their\\nnames for contrastive learning, the textual descriptions should include more detailed attributes such as quantity, color, and shape.\\nThis enriched information can help distinguish the embeddings from each other in the\\nlatent space, thereby reducing embedding bias\\nand minimizing confusion for downstream\\ntask models.\\n\\n  - Model-centric: Mitigation strategies should\\nbe tailored to specific real-world applications.\\nFor instance, addressing the counting problem highlighted in our paper, while it may\\nbe time- and computation-intensive to modify the foundational model, fine-tuning downstream task models like Stable-diffusion with\\ncarefully designed prompts, such as \"many\"\\nand \"fewer,\" can be more practical. Additionally, developers can include a regularization\\nterm that distinguishes and group different\\nsets of quantitative words, like \"one,\" \"two\"\\nfor \"smaller\", and \"hundreds,\" \"thousands\" for\\n\"larger\". This approach encourages the model\\nto learn and differentiate quantitative concepts\\nmore effectively.\\n\\n#### 4 Conclusion\\n\\nIn this work, we study an interesting problem: Can\\n_CLIP count stars? Through extensive empirical_\\nstudies on different modalities, we conclude that\\nthe CLIP models cannot understand the concept of\\nquantity well. In the future, we will delve deeper\\ninto this quantity bias and design novel methods\\nfor efficient bias mitigation.\\n\\n#### Limitations\\n\\nCLIP is one of the most popular foundation models used in generation tasks (e.g., the development\\nof Stable Diffusion), which motivates us to study\\na variety of CLIP models with different vision\\nand text backbones in this short paper. To ensure\\na controlled examination of the variable in question—specifically, the number of objects in both\\nvisual and textual modalities—we employed manually constructed datasets to evaluate the quantity\\nbias of CLIP in this preliminary research. However,\\nreal-world data presents more diversity and complexity, which were not fully captured in the simulations of this study. Future research should include\\nmore extensive results from real-world datasets and\\n\\n\\nevaluate a broader range of vision-language models\\nto provide a more comprehensive assessment.\\n\\n#### References\\n\\nAilin Deng, Zhirui Chen, and Bryan Hooi. 2024. Seeing\\nis believing: Mitigating hallucination in large visionlanguage models via clip-'),\n",
       " Document(metadata={'filename': 'Can_CLIP_Count_Stars_An_Empirical_Study_on_Quantity_Bias_in_CLIP_2409.15035v1'}, page_content='### Can CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP\\n\\n#### Zeliang Zhang, Zhuo Liu, Mingqian Feng, Chenliang Xu Department of Computer Science, University of Rochester\\n\\n\\n{zeliang.zhang, zhuo.liu, susan.liang,chenliang.xu}@rochester.edu\\n\\n#### Abstract\\n\\n\\nCLIP has demonstrated great versatility in\\nadapting to various downstream tasks, such as\\nimage editing and generation, visual question\\nanswering, and video understanding. However,\\nCLIP-based applications often suffer from misunderstandings regarding user intent, leading to\\ndiscrepancies between the required number of\\nobjects and the actual outputs in image generation tasks. In this work, we empirically investigate the quantity bias in CLIP. By carefully\\ndesigning different experimental settings and\\ndatasets, we comprehensively evaluate CLIP’s\\nunderstanding of quantity from text, image, and\\ncross-modal perspectives. Our experimental results reveal a quantity bias in CLIP embeddings,\\nimpacting the reliability of downstream tasks.\\n\\n#### 1 Introduction\\n\\n\\nThe Contrastive Language-Image Pre-Training\\n(CLIP) model (Radford et al., 2021), trained\\non large-scale image-text pairs, has shown significant success in various downstream visionlanguage tasks, including editing (Guerrero-Viu\\net al., 2024; Michel et al., 2024), generation (Ganz\\nand Elad, 2024; Liu et al., 2024), and quality evaluation (Hong et al., 2024; Deng et al., 2024). It\\nis crucial to maintain a reliable CLIP model at the\\ncore to ensure the development of trustworthy applications built upon it.\\nHowever, several factors potentially hinder the\\ninterpretability and trustworthiness of CLIP, including the black-box nature of the learning process,\\nuneven distributions of the training data, and the\\ndifficulty in accurately learning specific data distributions. Such issues may lead to unintended systematic errors like spurious correlations (Sagawa\\net al., 2020) and subgroup biases (Zhang et al.,\\n2024). These drawbacks not only degrade CLIP’s\\nperformance in learning reliable latent representations for image and text translation, but also pose\\na risk of propagating unexpected biases to models\\n\\n\\nFigure 1: Existing models')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(\"hello world\", filter={\"filename\": \"Can_CLIP_Count_Stars_An_Empirical_Study_on_Quantity_Bias_in_CLIP_2409.15035v1\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
