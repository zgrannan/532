{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os \n",
    "import sys \n",
    "from pprint import pprint\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.pdf import read_pdf\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"ollama/llama3.2\"\n",
    "base_url = \"http://localhost:11434\"\n",
    "llm = LLM(base_url=base_url, model=model, temperature=0, api_key=\"test\")\n",
    "pdf_text = read_pdf(\"../data/Sohn_Visual_Prompt_Tuning_for_Generative_Transfer_Learning_CVPR_2023_paper.pdf\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the document based on the token limit\n",
    "def chunk_document(document, chunk_size=3000):\n",
    "    words = document.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        if len(current_chunk) >= chunk_size:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))  # Add remaining chunk\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55644"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_document(pdf_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leverage Pydantic Models for entity extraction to get output in structured format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamedEntities(BaseModel):\n",
    "    people: List[str] = Field(..., description = \"List of people\")\n",
    "    locations: List[str] = Field(..., description = \"List of locations\")\n",
    "    organizations: List[str] = Field(..., description = \"List of organizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pydantic.main.BaseModel.model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx' = None, exclude: 'IncEx' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NamedEntities.model_dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_agent = Agent(\n",
    "    llm = llm,\n",
    "    role = \"Chief Researcher\",\n",
    "    goal = \"Reads documents thoroughly and identifies important entities and concepts\",\n",
    "    backstory=\"As the chief researcher at an AI firm, you specialize in identifying key concepts\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Task(\n",
    "    output_json=NamedEntities,\n",
    "    description=\"Extract entities entities from the following piece of text \\n{text}\",\n",
    "    agent=extraction_agent,\n",
    "    expected_output=\"List of entities in JSON for people, locations, organizations\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 17:31:25,616 - 126714500720448 - __init__.py-__init__:538 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "crew = Crew(\n",
    "    tasks=[task],\n",
    "    agents=[extraction_agent],\n",
    "    verbose=True,\n",
    "    full_output=True,\n",
    "    process=Process.sequential,\n",
    "    output_log_file=\"logs.txt\"\n",
    "  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mChief Researcher\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mExtract entities entities from the following piece of text \n",
      "# Visual Prompt Tuning for Generative Transfer Learning\n",
      "\n",
      "\n",
      "Kihyuk Sohn, Huiwen Chang, Jos´e Lezama, Luisa Polania,\n",
      "\n",
      "Han Zhang, Yuan Hao, Irfan Essa, Lu Jiang\n",
      "\n",
      "Google Research\n",
      "\n",
      "Figure 1. Image synthesis by knowledge transfer. Unlike previous works using GANs as base model and test transfer on relatively narrow\n",
      "visual domains, we transfer knowledge of generative vision transformers [7, 15] to a wide range of visual domains, including natural\n",
      "(e.g., scene, flower), specialized (e.g., satellite, medical), and structured (e.g., road scenes, infograph, sketch) with a few training images.\n",
      "Notably, the prompt tuning significantly improves the prior best FID on two benchmarks ImageNet (85.9!16.3) and Places (71.3!24.2).\n",
      "\n",
      "\n",
      "**Abstract**\n",
      "\n",
      "_Learning generative image models from various domains_\n",
      "\n",
      "_efficiently needs transferring knowledge from an image syn-_\n",
      "_thesis model trained on a large dataset. We present a recipe_\n",
      "_for learning vision transformers by generative knowledge_\n",
      "_transfer. We base our framework on generative vision trans-_\n",
      "_formers representing an image as a sequence of visual to-_\n",
      "_kens with the autoregressive or non-autoregressive trans-_\n",
      "_formers. To adapt to a new domain, we employ prompt tun-_\n",
      "_ing, which prepends learnable tokens called prompts to the_\n",
      "_image token sequence and introduces a new prompt design_\n",
      "_for our task. We study on a variety of visual domains with_\n",
      "_varying amounts of training images. We show the effective-_\n",
      "_ness of knowledge transfer and a significantly better image_\n",
      "_generation quality.[1]_\n",
      "\n",
      "**1. Introduction**\n",
      "\n",
      "Image synthesis has witnessed tremendous progress re\n",
      "cently with the advancement of deep generative models [2,\n",
      "\n",
      "1https://github.com/google-research/generative_\n",
      "\n",
      "transfer\n",
      "\n",
      "\n",
      "12, 20, 67, 69]. An ideal image synthesis system generates\n",
      "diverse, plausible, and novel scenes capturing the appearance of objects and depicting their interactions. The success\n",
      "of image synthesis does heavily rely on the availability of a\n",
      "large amount of diverse training data [73].\n",
      "\n",
      "Transfer learning, a cornerstone invention in deep learn\n",
      "ing, has proven indispensable in an array of computer vision\n",
      "tasks, including classification [35], object detection [18,19],\n",
      "image segmentation [23, 24], etc. However, transfer learning is not widely used for image synthesis. While recent\n",
      "efforts have shown success in transferring knowledge from\n",
      "pre-trained Generative Adversarial Network (GAN) models [46, 60, 71, 76], their demonstrations are limited to narrow visual domains, e.g., faces or cars [46, 76], as in Fig. 1,\n",
      "or requiring a non-trivial amount of training data [60, 71] to\n",
      "transfer to out-of-distribution domains.\n",
      "\n",
      "In this work, we approach transfer learning for image\n",
      "\n",
      "synthesis using generative vision transformers, an emerging class of image synthesis models, such as DALL·E [53],\n",
      "Taming Transformer [15], MaskGIT [7], CogView [13],\n",
      "N UWA [[¨] 75], Parti [79], among others, which excel in im\n",
      "\n",
      "-----\n",
      "\n",
      "age synthesis tasks. We closely follow the recipe of transfer learning for image classification [35], in which a source\n",
      "model is first trained on a large dataset (e.g., ImageNet) and\n",
      "then transferred to a diverse collection of downstream tasks.\n",
      "Except, in our setting, the input and output are reversed and\n",
      "the model generates images from a class label.\n",
      "\n",
      "We present a transfer learning framework using prompt\n",
      "\n",
      "_tuning [38,40]. While the technique has been used for trans-_\n",
      "fer learning of discriminative models for vision tasks [1,29],\n",
      "we appear to be the first to adopt prompt tuning for trans_fer learning of image synthesis. To this end, we propose a_\n",
      "parameter-efficient design of a prompt token generator that\n",
      "admits condition variables (e.g., class), a key for controllable image synthesis neglected in prompt tuning for discriminative transfer [29, 38]. We also introduce a marquee\n",
      "header prompt that engineers learned prompts to enhance\n",
      "generation diversity while retaining the generation quality.\n",
      "\n",
      "We conduct a large-scale study to understand the me\n",
      "chanics of transfer learning for generative vision transformers. Two types of generative transformers – AutoRegressive\n",
      "_(AR) and Non-AutoRegressive (NAR) – are examined. AR_\n",
      "transformers (e.g., DALL·E [53], Taming Transformer [15],\n",
      "Parti [79]) generate image tokens sequentially with an\n",
      "autoregressive language model. NAR transformers (e.g.,\n",
      "\n",
      "MaskGIT [7], MUSE [6]) or diffusion models (e.g., Imagen [58], Latent Diffusion [57]) decompose image synthesis as a series of refinement or denoising steps. In this work,\n",
      "we study transfer learning of class-conditional AR [15] and\n",
      "NAR [7] transformer models trained on ImageNet to comply with existing transfer learning settings [60, 71]. In addition to investigating proposed prompt tuning, we also conduct an analysis of two other transfer learning methods, i.e.\n",
      "full fine-tuning and adapter tuning, in the context of generative transfer learning using vision transformers. We compare their strengths and weaknesses in Sec. 4.1.\n",
      "\n",
      "Our study shows that generative vision transformers with\n",
      "\n",
      "prompt tuning outperform state-of-the-art methods using\n",
      "GANs [60, 71] by a vast margin, which is verified on 19\n",
      "tasks of diverse visual distributions and drastically different\n",
      "amounts of training data in VTAB [81]. Fig. 1 compares\n",
      "domains, showing the great expansion of downstream domains to what is achieved by previous works. On the onmanifold domains on which previous studies have focused,\n",
      "our method slashes the prior state-of-the-art in FID from\n",
      "71 to 24 on Places [85] and 86 to 16 on Animal Face [61]\n",
      "datasets. Moreover, our method shows highly-competitive\n",
      "data efficiency, generating diverse images following the target distribution when trained from a few images per class.\n",
      "\n",
      "In summary, our contributions are as follows:\n",
      "\n",
      "_• We present a generative visual transfer learning frame-_\n",
      "\n",
      "work for vision transformers with prompt tuning [38],\n",
      "proposing a new prompt token generator design.\n",
      "\n",
      "_• We conduct a large-scale empirical study for genera-_\n",
      "\n",
      "\n",
      "tive transfer learning to validate our proposed prompt\n",
      "tuning and relevant transfer learning methods (e.g., full\n",
      "fine-tuning, adapter tuning) on several visual domains\n",
      "(e.g., VTAB) and scenarios (e.g., few-shot). We show\n",
      "state-of-the-art image synthesis performance.\n",
      "\n",
      "_• To our knowledge, we are first to propose the use of_\n",
      "\n",
      "prompt tuning for transfer learning of generative transformers. Importantly, we provide the quantitative evidence on the necessity of generative knowledge transfer on VTAB [81], the common and challenging transfer learning benchmark.\n",
      "\n",
      "**2. Preliminary**\n",
      "\n",
      "**2.1. Generative Vision Transformers**\n",
      "\n",
      "This paper uses generative vision transformers to denote\n",
      "\n",
      "vision transformers for image synthesis. Broadly, there are\n",
      "two types of generative transformers, AutoRegressive (AR)\n",
      "and Non-AutoRegressive (NAR) transformers, both consisting of two stages – image quantization and decoding. The\n",
      "two models share the same first stage: image quantization\n",
      "by a Vector-Quantized (VQ) auto-encoder [15, 54, 67, 78].\n",
      "The VQ encoder converts image patches into indices (or tokens) in a codebook. The 2D image is then flattened into\n",
      "a 1D sequence to which a special token indicating its class\n",
      "label is prepended.\n",
      "\n",
      "Pretrain on ImageNet Flowers, Retinopathy, Kitti, …\n",
      "\n",
      "transfer\n",
      "\n",
      "Mutable\n",
      "\n",
      "AR / NAR AR / NAR\n",
      "\n",
      "transformer Frozen transformer\n",
      "\n",
      "Prompt token\n",
      "\n",
      "Visual token\n",
      "\n",
      "Autoregressive Decoding\n",
      "\n",
      "t=0 t=1 t=2 t=100 t=160 t=256\n",
      "\n",
      "Non-autoregressive (parallel) Decoding\n",
      "\n",
      "t=0 t=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8\n",
      "\n",
      "\n",
      "Figure 2. Our method transfers knowledge from generative vision\n",
      "transformers (e.g., autoregressive [15] or non-autoregressive [7])\n",
      "trained on a large dataset to various visual domains by prepending\n",
      "learnable prompt tokens (green) to visual tokens (blue).\n",
      "\n",
      "AR and NAR transformers differ in the second stage. AR\n",
      "\n",
      "transformers [8, 13, 15, 53, 75, 79], such as DALL·E [53],\n",
      "Taming Transformer [15], learn an AR decoder on the flattened token sequence to generate image tokens sequentially\n",
      "from previously generated tokens. As in Fig. 2, the generation follows a raster scan ordering, generating tokens from\n",
      "left to right, line-by-line. Finally, the generated tokens are\n",
      "mapped to the pixel space using the VQ decoder.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "|Transformer|Col2|Col3|Col4|Col5|Col6|\n",
      "|---|---|---|---|---|---|\n",
      "|Transformer||||||\n",
      "|||||||\n",
      "\n",
      "|𝝨 F B x S x P x F 1 x 1 x 1 x F B x 1 x P x F 1 x S x P x F MLP MLP MLP C P F class / instance position factor|Col2|\n",
      "|---|---|\n",
      "\n",
      "\n",
      "S\n",
      "\n",
      "B x S x P MLPT D\n",
      "\n",
      "\n",
      "decode\n",
      "\n",
      "\n",
      "(a) Baseline prompt token generators of length\n",
      "_S conditioned on class._\n",
      "\n",
      "\n",
      "(b) The proposed parameter efficient prompt token generator via factorization of class / instance\n",
      "and position. ⊕ is an element-wise sum, ⊙ is an element-wise product, ⌃F is a sum over F\n",
      "dimension. S: sequence length, B: batch size, P : feature dimension, D: token dimension.\n",
      "\n",
      "\n",
      "(c) Number of parameters with respect to the\n",
      "sequence length and different number of factors F .\n",
      "\n",
      "\n",
      "Figure 3. Prompt token generators and their use in transformer. (a) a straightforward extension of baseline prompt token generators [29,38,\n",
      "40] with a class condition. When using an MLP with a single dense layer of P units, the number of trainable parameters is P _·(C·S+D)._\n",
      "(b) The proposed parameter efficient prompt token generators that factorizes data dependent conditions (e.g., class, instance) and token\n",
      "position. Under a similar design choice as baseline models, the number of trainable parameters is P _·(F_ _·(C+S)+D), which could be_\n",
      "significantly fewer when F _⌧_ min(C, S). (c) Number of parameters for prompt token generators with respect to the sequence length (S),\n",
      "while setting P = 768, D = 768, and C = 100 with different number of factors F .\n",
      "\n",
      "\n",
      "NAR or diffusion models, including DALL·E 2 [52],\n",
      "\n",
      "MaskGIT [7], Latent Diffusion [57], or Imagen [58], decompose image synthesis as a series of refinement or denoising steps. For prompt tuning, we need a NAR model\n",
      "with the transformer backbone [7, 17, 21, 36, 37, 39, 83], and\n",
      "use a leading NAR image transformer called MaskGIT [7].\n",
      "\n",
      "NAR transformers are trained on the masked modeling\n",
      "\n",
      "proxy task [11]. For inference, the model adopts a nonautoregressive decoding method to synthesize an image in a\n",
      "few steps [7, 21, 36, 39]. As in Fig. 2, the NAR transformer\n",
      "starts from a blank canvas with all tokens masked, and generates an image in 8 steps or so. In each step, it predicts all\n",
      "tokens in parallel and retains the ones with the highest prediction scores. The remaining tokens are masked out and\n",
      "predicted in the next iteration. NAR transformers [7, 39]\n",
      "have shown faster inference than AR transformers.\n",
      "\n",
      "**2.2. Prompt Tuning**\n",
      "\n",
      "Prompt tuning [38, 40] is introduced recently in natural\n",
      "\n",
      "language processing as a way of efficiently adapting pretrained large language models to downstream tasks. Here,\n",
      "prompt is a sequence of additional tokens prepended to a\n",
      "token sequence. In prompt engineering [3], their values are\n",
      "often chosen by heuristic. On the other hand, in prompt\n",
      "tuning [38, 40], tokens are parameterized by learnable parameters and their parameters are updated via gradient descent to adapt transformers to the downstream tasks. Due to\n",
      "its simplicity and as transformers’ central role in language\n",
      "foundation models, prompt tuning has been applied to some\n",
      "vision tasks for knowledge transfer, e.g., image classification [1, 29], detection and segmentation [45], but not yet for\n",
      "image synthesis.\n",
      "\n",
      "**3. Visual Prompt for Generative Transfer**\n",
      "\n",
      "Fig. 2 overviews the proposed generative transfer learn\n",
      "ing framework. We aim at transferring a generative prior,\n",
      "\n",
      "\n",
      "parameterized by generative vision transformers, while utilizing the same VQ encoder and decoder trained from the\n",
      "large source dataset. We use prompt tuning to adapt to\n",
      "\n",
      "the target distributions while leaving the transformer parameters frozen. We discuss how to learn visual prompts\n",
      "(Sec. 3.1), a new prompt generator for conditional image\n",
      "synthesis (Sec. 3.2), and a prompt design for generating visually diverse images (Sec. 3.3).\n",
      "\n",
      "**3.1. Learning Visual Prompt**\n",
      "\n",
      "A sequence of prompt tokens is prepended to the visual\n",
      "\n",
      "tokens to guide the pretrained transformer models to the target distribution. Prompt tuning, learning the parameters of\n",
      "the token generator, is optimized by gradient descent with\n",
      "respective loss functions, while fixing the parameters of the\n",
      "pretrained transformers. To be specific, let = _zi_ _i=1_\n",
      "_Z_ _{_ _}[H][⇥][W]_\n",
      "\n",
      "be a sequence of visual tokens (i.e., an output of VQ encoder followed by the vectorization) and _φ =_ _ps;φ_ _s=1_\n",
      "_P_ _{_ _}[S]_\n",
      "\n",
      "be a sequence of prompt tokens. For the AR transformer,\n",
      "the loss is given as follows:\n",
      "\n",
      "AR = Ex _P_ log P✓( _φ)_ (1)\n",
      "_L_ _⇠_ _X_ _−_ _Z|P_\n",
      "\n",
      "_P✓(_ _φ) =_ _H⇥⇥_ _W_ _P✓(zi_ _z<i,_ _φ⇤)_ (2)\n",
      "_Z|P_ _i=1_ _|_ _P_\n",
      "\n",
      "Y\n",
      "\n",
      "For the NAR transformer, we follow that of MaskGIT [7]:\n",
      "\n",
      "_LNAR = Ex⇠PX,M_ _⇠PM_ _−_ log P✓(ZM _|ZM_ _, Pφ)_ (3)\n",
      "\n",
      "_P✓(ZM_ _|ZM_ _, Pφ) =⇥_ _i_ _M_ _[P][✓][(][z][i][|Z][M]_ _[,][ P][φ]⇤[)]_ (4)\n",
      "\n",
      "_2_\n",
      "\n",
      "Y\n",
      "\n",
      "\n",
      "where M ⇢{1, ..., H⇥W _} is a set of visual token indices_\n",
      "sampled from a masking schedule distribution P, M is its\n",
      "_M_\n",
      "complement, and _M =_ _zi_ _i_ _M_ . Prompt tuning proceeds\n",
      "_Z_ _{_ _}_ _2_\n",
      "by minimizing the respective loss with respect to the prompt\n",
      "parameters φ while fixing the transformer parameters ✓:\n",
      "\n",
      "\n",
      "_φ[⇤]_ = arg min\n",
      "\n",
      "\n",
      "AR/NAR (5)\n",
      "_L_\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "(a) Image synthesis using instance-conditioned prompts.\n",
      "\n",
      "Dog\n",
      "\n",
      "Dog\n",
      "\n",
      "\n",
      "(b) Image synthesis using a marquee header prompt between instance (blue) and class (red) conditioned prompts.\n",
      "\n",
      "(c) Image synthesis using a marquee header prompt between instance-conditioned prompts (blue and red).\n",
      "\n",
      "Figure 4. Iterative decoding of NAR transformers. (4a) instance prompts generate images of high-fidelity but with low diversity. Marquee\n",
      "header prompts enhance generation diversity by interpolating (4b) from instance to class prompts or (4c) between instance prompts.\n",
      "\n",
      "\n",
      "While we focus on the prompt tuning due to the virtue of\n",
      "effectiveness and compute-efficiency for large source transformers, we note that the proposed learning framework is\n",
      "amenable with other methods, such as adapter [28] or finetuning [35], with learnable prompts. See a detailed comparison in Appendix B.4.\n",
      "\n",
      "After prompt tuning, we generate visual tokens for image\n",
      "\n",
      "synthesis by iterative decoding. For AR transformer,\n",
      "\n",
      "1: for i 1 to H ⇥ _W do_\n",
      "2: _zˆi_ _P✓(zi_ _zˆ<i,_ _φ)_\n",
      "_⇠_ _|_ _P_\n",
      "\n",
      "3: end for\n",
      "\n",
      "\n",
      "dimension P . For example, when using a prompt of length\n",
      "_S=128, hidden P_ =768 and embedding dimension D=768,\n",
      "the token generator would introduce 10.4M parameters for\n",
      "_C=100 class conditions, as in Fig. 3c. The bottleneck oc-_\n",
      "curs at the 3d weight tensor of size C⇥S⇥P .\n",
      "\n",
      "To make it parameter efficient, we propose a factorized\n",
      "\n",
      "token generator (Fig. 3b). We encode class and sequence\n",
      "position index via MLPC and MLPP with F factors, respectively. The MLP outputs are element-wise summed, multiplied by a 1d factor vector from MLPF, and reduced along\n",
      "the factor dimension. The output is then fed to MLPT to\n",
      "produce a prompt of length S. As in Fig. 3c, the number of\n",
      "parameters of the proposed architecture is greatly reduced,\n",
      "requiring only 0.76M parameters, down from 10.4M, for\n",
      "a prompt of length 128 when F = 1.[2] We empirically find\n",
      "that F = 1 is sufficient for NAR transformers. For AR transformers, extra capacity is needed by setting F = 16.\n",
      "\n",
      "Moreover, we build a new type of prompt tokens condi\n",
      "tioned on individual data instances, inspired by the instanceconditioned GAN [5]. We assign each data a unique index\n",
      "and map it into a distinct embedding via MLPC. When both\n",
      "class label and instance index are used, instance index is\n",
      "simply treated as an extra class, indexed from C. To train\n",
      "the model, we sample between class label and instance index. As we explain below in Sec. 3.3, instance-conditioned\n",
      "prompts add more fine-grained control on generation.\n",
      "\n",
      "**3.3. Engineering Learned Prompts**\n",
      "\n",
      "Given the wealth of learned prompts conditioned on the\n",
      "\n",
      "class and instance proposed in Sec. 3.2, we propose a new\n",
      "\n",
      "2The proposed factorization can be extended to incorporate the “depth”\n",
      "\n",
      "position of deep visual prompt [29] to reduce the number of parameters.\n",
      "\n",
      "\n",
      "For the NAR model, parallel decoding [7] is used:\n",
      "**Require: M = {}, T**, {n1, ..., nT }, _t=1_ _[n][t][ =][ H][ ⇥]_ _[W]_\n",
      "\n",
      "1: for t 1 to T do\n",
      "2: _zˆi_ _P✓(zi_ _M_ _,_ _φ),_ _i_ _M[P][T]_\n",
      "_⇠_ _|Z_ _P_ _8_ _2_\n",
      "\n",
      "3: _M_ _M [ {arg topki 2 M_ _P✓(ˆzi|ZM_ _, Pφ), k = nt_ _}_\n",
      "\n",
      "4: end for [b]\n",
      "\n",
      "# $\n",
      "\n",
      "where _n1, ..., nT_ is a masking schedule that decides the[b]\n",
      "_{_ _}_\n",
      "number of tokens to decode at each step. We refer to [7] for\n",
      "details on decoding for NAR transformers. Illustrations of\n",
      "decoding steps for both models are in Fig. 2.\n",
      "\n",
      "**3.2. Prompt Token Generator Design**\n",
      "\n",
      "For transfer learning of discriminative tasks, prompts are\n",
      "\n",
      "designed without condition variables [29]. For generative\n",
      "tasks, it is beneficial to have condition variables (e.g., class,\n",
      "attribute) for better control in generation. We achieve this\n",
      "with a simple design of treating class conditions as another\n",
      "prompt, as in Fig. 3a.\n",
      "\n",
      "One critical issue is that the number of learnable param\n",
      "eters increases as the product of three factors: the number\n",
      "of classes C, the prompt sequence length S and the feature\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "|Model (# tr params)|Mean|Mean (10K)|C101|Flowers|Pet|DTD|Kitti|SUN|EuroSAT|Resisc|\n",
      "|---|---|---|---|---|---|---|---|---|---|---|\n",
      "|MineGAN [71] (88M) cGANTransfer [60] (105M)|151.5 85.1|114.0 63.8|102.4 89.6|132.1 61.6|130.1 48.6|87.4 70.3|117.9 48.9|77.5 31.1|111.5 45.6|81.0 50.3|\n",
      "|Prompt (S = 1) (0.67M) Prompt (S = 16) (0.68M) Non-Autoregressive Prompt (S = 128) (0.76M) Scratch (172M)|53.7 39.9 36.4 42.7|19.7 18.6 18.6 60.0|13.5 12.7 12.9 72.7|13.8 13.2 13.4 57.2|11.9 11.1 10.9 70.3|25.8 26.0 25.9 66.1|32.3 30.0 29.9 33.8|7.3 7.4 7.7 9.2|45.9 35.8 38.4 39.5|28.5 24.9 24.8 32.0|\n",
      "|Prompt (S = 1) (0.86M) Prompt (S = 16) (0.88M) Autoregressive Prompt (S = 256) (1.06M) Prompt (S = 256, F = 16) (5.16M) Scratch (306M)|73.2 47.4 39.0 36.9 39.6|44.1 34.5 32.3 26.6 61.8|45.4 41.4 39.6 27.2 76.0|28.9 19.6 17.3 14.1 56.1|42.2 36.6 34.9 27.2 52.5|37.1 33.4 32.5 30.0 92.7|66.8 41.4 37.1 34.6 31.6|18.8 16.4 15.0 12.8 13.5|37.3 32.6 29.6 26.4 19.4|35.1 28.8 26.7 22.2 29.5|\n",
      "\n",
      "\n",
      "Table 1. FIDs (lower the better) on VTAB tasks. The number of trainable parameters (second column) are computed assuming 100 classes.\n",
      "The mean FID over 19 VTAB tasks (third column), over small-scale datasets (10K, fourth column) and those with a small to mid-scale\n",
      "training data are reported. Complete results are in Appendix C.1.3. The best and the second best results are highlighted in each column.\n",
      "\n",
      "\n",
      "prompt engineering strategy, a “Marquee Header” prompt,\n",
      "tailored to the non-autoregressive transformer decoding, for\n",
      "enhancing generation diversity.\n",
      "\n",
      "We interpolate the learned prompt representations (e.g.,\n",
      "\n",
      "outputs of MLPC). To account for the iterative decoding,\n",
      "the interpolation between prompts is carried out over multiple decoding steps. This is shown in Fig. 4b, where we start\n",
      "the decoding process using instance-conditioned prompts\n",
      "(blue header) but gradually transition to a class-conditioned\n",
      "prompt (red header) over decoding steps. Unlike the generation in Fig. 4a where the instance-conditioned prompts\n",
      "are used all along, the marquee header prompt generates diverse images while maintaining the generation quality and\n",
      "following characteristics of reference instances (e.g., pose,\n",
      "color pattern, hairiness). Fig. 4c shows a consistent trend\n",
      "when applying the prompt between two image instances.\n",
      "\n",
      "The marquee header prompt is formulated as follows:\n",
      "\n",
      "PMT(t) = (1 − _wt)PMT1 + wtPMT2_ (6)\n",
      "\n",
      "2\n",
      "\n",
      "_t_ 1\n",
      "\n",
      "_wt_ = min _−_ _, 1_ (7)\n",
      "\n",
      "_Tcuto↵_ 1\n",
      "\n",
      "n [✓] _−_ ◆ o\n",
      "\n",
      "where t = 1, ..., T is a decoding step, Tcuto↵ _T is a cutoff_\n",
      "__\n",
      "step, and PMTi is a prompt representation (e.g., an output\n",
      "of MLPC). The schedule in Eq. (7) makes a smooth transition of prompts from PMT1 to PMT2. We keep Eq. (7)’s\n",
      "formulation as simple as possible and note that there could\n",
      "be various other prompt formulation\u001b[00m\n",
      "\u001b[91m Error parsing LLM output, agent will retry: I did it wrong. Invalid Format: I missed the 'Action:' after 'Thought:'. I will do right next, and don't use a tool I have already used.\n",
      "\n",
      "If you don't need to use any more tools, you must give your best complete final answer, make sure it satisfy the expect criteria, use the EXACT format below:\n",
      "\n",
      "Thought: I now can give a great answer\n",
      "Final Answer: my best complete final answer to the task.\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[91m Error parsing LLM output, agent will retry: I did it wrong. Invalid Format: I missed the 'Action:' after 'Thought:'. I will do right next, and don't use a tool I have already used.\n",
      "\n",
      "If you don't need to use any more tools, you must give your best complete final answer, make sure it satisfy the expect criteria, use the EXACT format below:\n",
      "\n",
      "Thought: I now can give a great answer\n",
      "Final Answer: my best complete final answer to the task.\n",
      "\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mChief Researcher\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "[\n",
      "  {\n",
      "    \"name\": \"People\",\n",
      "    \"entities\": [\n",
      "      {\n",
      "        \"text\": \"The authors\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Locations\",\n",
      "    \"entities\": []\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Organizations\",\n",
      "    \"entities\": [\n",
      "      {\n",
      "        \"text\": \"VTAB\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[91m Failed to convert text into JSON, error: 3 validation errors for NamedEntities\n",
      "people\n",
      "  Field required [type=missing, input_value={'properties': {'people':...ganizations': ['VTAB']}}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.8/v/missing\n",
      "locations\n",
      "  Field required [type=missing, input_value={'properties': {'people':...ganizations': ['VTAB']}}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.8/v/missing\n",
      "organizations\n",
      "  Field required [type=missing, input_value={'properties': {'people':...ganizations': ['VTAB']}}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.8/v/missing. Using raw output instead.\u001b[00m\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for TaskOutput\njson_dict\n  Input should be a valid dictionary [type=dict_type, input_value=[{'name': 'People', 'enti...s': [{'text': 'VTAB'}]}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.8/v/dict_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mcrew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkickoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf_text\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m    \n",
      "File \u001b[0;32m~/anaconda3/envs/gen-ai/lib/python3.12/site-packages/crewai/crew.py:490\u001b[0m, in \u001b[0;36mCrew.kickoff\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m metrics: List[UsageMetrics] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39msequential:\n\u001b[0;32m--> 490\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sequential_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39mhierarchical:\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_hierarchical_process()\n",
      "File \u001b[0;32m~/anaconda3/envs/gen-ai/lib/python3.12/site-packages/crewai/crew.py:594\u001b[0m, in \u001b[0;36mCrew._run_sequential_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CrewOutput:\n\u001b[1;32m    593\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gen-ai/lib/python3.12/site-packages/crewai/crew.py:692\u001b[0m, in \u001b[0;36mCrew._execute_tasks\u001b[0;34m(self, tasks, start_index, was_replayed)\u001b[0m\n\u001b[1;32m    689\u001b[0m     futures\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    691\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_context(task, task_outputs)\n\u001b[0;32m--> 692\u001b[0m task_output \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_to_use\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m task_outputs \u001b[38;5;241m=\u001b[39m [task_output]\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_task_result(task, task_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/gen-ai/lib/python3.12/site-packages/crewai/task.py:191\u001b[0m, in \u001b[0;36mTask.execute_sync\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_sync\u001b[39m(\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    186\u001b[0m     agent: Optional[BaseAgent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    187\u001b[0m     context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    188\u001b[0m     tools: Optional[List[Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    189\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskOutput:\n\u001b[1;32m    190\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gen-ai/lib/python3.12/site-packages/crewai/task.py:255\u001b[0m, in \u001b[0;36mTask._execute_core\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    247\u001b[0m result \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mexecute_task(\n\u001b[1;32m    248\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    249\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    250\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    251\u001b[0m )\n\u001b[1;32m    253\u001b[0m pydantic_output, json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_output(result)\n\u001b[0;32m--> 255\u001b[0m task_output \u001b[38;5;241m=\u001b[39m \u001b[43mTaskOutput\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpected_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpected_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpydantic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpydantic_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrole\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_output_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m task_output\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_end_execution_time(start_time)\n",
      "File \u001b[0;32m~/anaconda3/envs/gen-ai/lib/python3.12/site-packages/pydantic/main.py:193\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    192\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for TaskOutput\njson_dict\n  Input should be a valid dictionary [type=dict_type, input_value=[{'name': 'People', 'enti...s': [{'text': 'VTAB'}]}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.8/v/dict_type"
     ]
    }
   ],
   "source": [
    "res = crew.kickoff(inputs = {\"text\": pdf_text[0:20000]})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_pydantic_core_schema__',\n",
       " '__get_pydantic_json_schema__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__pydantic_complete__',\n",
       " '__pydantic_core_schema__',\n",
       " '__pydantic_custom_init__',\n",
       " '__pydantic_decorators__',\n",
       " '__pydantic_extra__',\n",
       " '__pydantic_fields_set__',\n",
       " '__pydantic_generic_metadata__',\n",
       " '__pydantic_init_subclass__',\n",
       " '__pydantic_parent_namespace__',\n",
       " '__pydantic_post_init__',\n",
       " '__pydantic_private__',\n",
       " '__pydantic_root_model__',\n",
       " '__pydantic_serializer__',\n",
       " '__pydantic_validator__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_check_frozen',\n",
       " '_copy_and_set_values',\n",
       " '_get_value',\n",
       " '_iter',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'dict',\n",
       " 'from_orm',\n",
       " 'json',\n",
       " 'json_dict',\n",
       " 'model_computed_fields',\n",
       " 'model_config',\n",
       " 'model_construct',\n",
       " 'model_copy',\n",
       " 'model_dump',\n",
       " 'model_dump_json',\n",
       " 'model_extra',\n",
       " 'model_fields',\n",
       " 'model_fields_set',\n",
       " 'model_json_schema',\n",
       " 'model_parametrized_name',\n",
       " 'model_post_init',\n",
       " 'model_rebuild',\n",
       " 'model_validate',\n",
       " 'model_validate_json',\n",
       " 'model_validate_strings',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'pydantic',\n",
       " 'raw',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'tasks_output',\n",
       " 'to_dict',\n",
       " 'token_usage',\n",
       " 'update_forward_refs',\n",
       " 'validate']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"people\": [\"The authors\"], \"locations\": [], \"organizations\": []}'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'People', 'entities': [{'text': 'The authors'}]},\n",
       " {'name': 'Locations', 'entities': []},\n",
       " {'name': 'Organizations', 'entities': [{'text': 'VTAB'}]}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(res.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Visual Prompt Tuning for Generative Transfer Learning\\n\\n\\nKihyuk Sohn, Huiwen Chang, Jos´e Lezama, Luisa Polania,\\n\\nHan Zhang, Yuan Hao, Irfan Essa, Lu Jiang\\n\\nGoogle Research\\n\\nFigure 1. Image synthesis by knowledge transfer. Unlike previous works using GANs as base model and test transfer on relatively narrow\\nvisual domains, we transfer knowledge of generative vision transformers [7, 15] to a wide range of visual domains, including natural\\n(e.g., scene, flower), specialized (e.g., satellite, medical), and structured (e.g., road scenes, infograph, sketch) with a few training images.\\nNotably, the prompt tuning significantly improves the prior best FID on two benchmarks ImageNet (85.9!16.3) and Places (71.3!24.2).\\n\\n\\n**Abstract**\\n\\n_Learning generative image models from various domains_\\n\\n_efficiently needs transferring knowledge from an image syn-_\\n_thesis model trained on a large dataset. We present a recipe_\\n_for learning vision transformers by generative knowledge_\\n_transfer. We base our framework on generative vision trans-_\\n_formers representing an image as a sequence of visual to-_\\n_kens with the autoregressive or non-autoregressive trans-_\\n_formers. To adapt to a new domain, we employ prompt tun-_\\n_ing, which prepends learnable tokens called prompts to the_\\n_image token sequence and introduces a new prompt design_\\n_for our task. We study on a variety of visual domains with_\\n_varying amounts of training images. We show the effective-_\\n_ness of knowledge transfer and a significantly better image_\\n_generation quality.[1]_\\n\\n**1. Introduction**\\n\\nImage synthesis has witnessed tremendous progress re\\ncently with the advancement of deep generative models [2,\\n\\n1https://github.com/google-research/generative_\\n\\ntransfer\\n\\n\\n12, 20, 67, 69]. An ideal image synthesis system generates\\ndiverse, plausible, and novel scenes capturing the appearance of objects and depicting their interactions. The success\\nof image synthesis does heavily rely on the availability of a\\nlarge amount of diverse training data [73].\\n\\nTransfer learning, a cornerstone invention in deep learn\\ning, has proven indispensable in an array of computer vision\\ntasks, including classification [35], object detection [18,19],\\nimage segmentation [23, 24], etc. However, transfer learning is not widely used for image synthesis. While recent\\nefforts have shown success in transferring knowledge from\\npre-trained Generative Adversarial Network (GAN) models [46, 60, 71, 76], their demonstrations are limited to narrow visual domains, e.g., faces or cars [46, 76], as in Fig. 1,\\nor requiring a non-trivial amount of training data [60, 71] to\\ntransfer to out-of-distribution domains.\\n\\nIn this work, we approach transfer learning for image\\n\\nsynthesis using generative vision transformers, an emerging class of image synthesis models, such as DALL·E [53],\\nTaming Transformer [15], MaskGIT [7], CogView [13],\\nN UWA [[¨] 75], Parti [79], among others, which excel in im\\n\\n-----\\n\\nage synthesis tasks. We closely follow the recipe of transfer learning for image classification [35], in which a source\\nmodel is first trained on a large dataset (e.g., ImageNet) and\\nthen transferred to a diverse collection of downstream tasks.\\nExcept, in our setting, the input and output are reversed and\\nthe model generates images from a class label.\\n\\nWe present a transfer learning framework using prompt\\n\\n_tuning [38,40]. While the technique has been used for trans-_\\nfer learning of discriminative models for vision tasks [1,29],\\nwe appear to be the first to adopt prompt tuning for trans_fer learning of image synthesis. To this end, we propose a_\\nparameter-efficient design of a prompt token generator that\\nadmits condition variables (e.g., class), a key for controllable image synthesis neglected in prompt tuning for discriminative transfer [29, 38]. We also introduce a marquee\\nheader prompt that engineers learned prompts to enhance\\ngeneration diversity while retaining the generation quality.\\n\\nWe conduct a large-scale study to understand the me\\nchanics of transfer learning for generative vision transformers. Two types of generative transformers – AutoRegressive\\n_(AR) and Non-AutoRegressive (NAR) – are examined. AR_\\ntransformers (e.g., DALL·E [53], Taming Transformer [15],\\nParti [79]) generate image tokens sequentially with an\\nautoregressive language model. NAR transformers (e.g.,\\n\\nMaskGIT [7], MUSE [6]) or diffusion models (e.g., Imagen [58], Latent Diffusion [57]) decompose image synthesis as a series of refinement or denoising steps. In this work,\\nwe study transfer learning of class-conditional AR [15] and\\nNAR [7] transformer models trained on ImageNet to comply with existing transfer learning settings [60, 71]. In addition to investigating proposed prompt tuning, we also conduct an analysis of two other transfer learning methods, i.e.\\nfull fine-tuning and adapter tuning, in the context of generative transfer learning using vision transformers. We compare their strengths and weaknesses in Sec. 4.1.\\n\\nOur study shows that generative vision transformers with\\n\\nprompt tuning outperform state-of-the-art methods using\\nGANs [60, 71] by a vast margin, which is verified on 19\\ntasks of diverse visual distributions and drastically different\\namounts of training data in VTAB [81]. Fig. 1 compares\\ndomains, showing the great expansion of downstream domains to what is achieved by previous works. On the onmanifold domains on which previous studies have focused,\\nour method slashes the prior state-of-the-art in FID from\\n71 to 24 on Places [85] and 86 to 16 on Animal Face [61]\\ndatasets. Moreover, our method shows highly-competitive\\ndata efficiency, generating diverse images following the target distribution when trained from a few images per class.\\n\\nIn summary, our contributions are as follows:\\n\\n_• We present a generative visual transfer learning frame-_\\n\\nwork for vision transformers with prompt tuning [38],\\nproposing a new prompt token generator design.\\n\\n_• We conduct a large-scale empirical study for genera-_\\n\\n\\ntive transfer learning to validate our proposed prompt\\ntuning and relevant transfer learning methods (e.g., full\\nfine-tuning, adapter tuning) on several visual domains\\n(e.g., VTAB) and scenarios (e.g., few-shot). We show\\nstate-of-the-art image synthesis performance.\\n\\n_• To our knowledge, we are first to propose the use of_\\n\\nprompt tuning for transfer learning of generative transformers. Importantly, we provide the quantitative evidence on the necessity of generative knowledge transfer on VTAB [81], the common and challenging transfer learning benchmark.\\n\\n**2. Preliminary**\\n\\n**2.1. Generative Vision Transformers**\\n\\nThis paper uses generative vision transformers to denote\\n\\nvision transformers for image synthesis. Broadly, there are\\ntwo types of generative transformers, AutoRegressive (AR)\\nand Non-AutoRegressive (NAR) transformers, both consisting of two stages – image quantization and decoding. The\\ntwo models share the same first stage: image quantization\\nby a Vector-Quantized (VQ) auto-encoder [15, 54, 67, 78].\\nThe VQ encoder converts image patches into indices (or tokens) in a codebook. The 2D image is then flattened into\\na 1D sequence to which a special token indicating its class\\nlabel is prepended.\\n\\nPretrain on ImageNet Flowers, Retinopathy, Kitti, …\\n\\ntransfer\\n\\nMutable\\n\\nAR / NAR AR / NAR\\n\\ntransformer Frozen transformer\\n\\nPrompt token\\n\\nVisual token\\n\\nAutoregressive Decoding\\n\\nt=0 t=1 t=2 t=100 t=160 t=256\\n\\nNon-autoregressive (parallel) Decoding\\n\\nt=0 t=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8\\n\\n\\nFigure 2. Our method transfers knowledge from generative vision\\ntransformers (e.g., autoregressive [15] or non-autoregressive [7])\\ntrained on a large dataset to various visual domains by prepending\\nlearnable prompt tokens (green) to visual tokens (blue).\\n\\nAR and NAR transformers differ in the second stage. AR\\n\\ntransformers [8, 13, 15, 53, 75, 79], such as DALL·E [53],\\nTaming Transformer [15], learn an AR decoder on the flattened token sequence to generate image tokens sequentially\\nfrom previously generated tokens. As in Fig. 2, the generation follows a raster scan ordering, generating tokens from\\nleft to right, line-by-line. Finally, the generated tokens are\\nmapped to the pixel space using the VQ decoder.\\n\\n\\n-----\\n\\n|Transformer|Col2|Col3|Col4|Col5|Col6|\\n|---|---|---|---|---|---|\\n|Transformer||||||\\n|||||||\\n\\n|𝝨 F B x S x P x F 1 x 1 x 1 x F B x 1 x P x F 1 x S x P x F MLP MLP MLP C P F class / instance position factor|Col2|\\n|---|---|\\n\\n\\nS\\n\\nB x S x P MLPT D\\n\\n\\ndecode\\n\\n\\n(a) Baseline prompt token generators of length\\n_S conditioned on class._\\n\\n\\n(b) The proposed parameter efficient prompt token generator via factorization of class / instance\\nand position. ⊕ is an element-wise sum, ⊙ is an element-wise product, ⌃F is a sum over F\\ndimension. S: sequence length, B: batch size, P : feature dimension, D: token dimension.\\n\\n\\n(c) Number of parameters with respect to the\\nsequence length and different number of factors F .\\n\\n\\nFigure 3. Prompt token generators and their use in transformer. (a) a straightforward extension of baseline prompt token generators [29,38,\\n40] with a class condition. When using an MLP with a single dense layer of P units, the number of trainable parameters is P _·(C·S+D)._\\n(b) The proposed parameter efficient prompt token generators that factorizes data dependent conditions (e.g., class, instance) and token\\nposition. Under a similar design choice as baseline models, the number of trainable parameters is P _·(F_ _·(C+S)+D), which could be_\\nsignificantly fewer when F _⌧_ min(C, S). (c) Number of parameters for prompt token generators with respect to the sequence length (S),\\nwhile setting P = 768, D = 768, and C = 100 with different number of factors F .\\n\\n\\nNAR or diffusion models, including DALL·E 2 [52],\\n\\nMaskGIT [7], Latent Diffusion [57], or Imagen [58], decompose image synthesis as a series of refinement or denoising steps. For prompt tuning, we need a NAR model\\nwith the transformer backbone [7, 17, 21, 36, 37, 39, 83], and\\nuse a leading NAR image transformer called MaskGIT [7].\\n\\nNAR transformers are trained on the masked modeling\\n\\nproxy task [11]. For inference, the model adopts a nonautoregressive decoding method to synthesize an image in a\\nfew steps [7, 21, 36, 39]. As in Fig. 2, the NAR transformer\\nstarts from a blank canvas with all tokens masked, and generates an image in 8 steps or so. In each step, it predicts all\\ntokens in parallel and retains the ones with the highest prediction scores. The remaining tokens are masked out and\\npredicted in the next iteration. NAR transformers [7, 39]\\nhave shown faster inference than AR transformers.\\n\\n**2.2. Prompt Tuning**\\n\\nPrompt tuning [38, 40] is introduced recently in natural\\n\\nlanguage processing as a way of efficiently adapting pretrained large language models to downstream tasks. Here,\\nprompt is a sequence of additional tokens prepended to a\\ntoken sequence. In prompt engineering [3], their values are\\noften chosen by heuristic. On the other hand, in prompt\\ntuning [38, 40], tokens are parameterized by learnable parameters and their parameters are updated via gradient descent to adapt transformers to the downstream tasks. Due to\\nits simplicity and as transformers’ central role in language\\nfoundation models, prompt tuning has been applied to some\\nvision tasks for knowledge transfer, e.g., image classification [1, 29], detection and segmentation [45], but not yet for\\nimage synthesis.\\n\\n**3. Visual Prompt for Generative Transfer**\\n\\nFig. 2 overviews the proposed generative transfer learn\\ning framework. We aim at transferring a generative prior,\\n\\n\\nparameterized by generative vision transformers, while utilizing the same VQ encoder and decoder trained from the\\nlarge source dataset. We use prompt tuning to adapt to\\n\\nthe target distributions while leaving the transformer parameters frozen. We discuss how to learn visual prompts\\n(Sec. 3.1), a new prompt generator for conditional image\\nsynthesis (Sec. 3.2), and a prompt design for generating visually diverse images (Sec. 3.3).\\n\\n**3.1. Learning Visual Prompt**\\n\\nA sequence of prompt tokens is prepended to the visual\\n\\ntokens to guide the pretrained transformer models to the target distribution. Prompt tuning, learning the parameters of\\nthe token generator, is optimized by gradient descent with\\nrespective loss functions, while fixing the parameters of the\\npretrained transformers. To be specific, let = _zi_ _i=1_\\n_Z_ _{_ _}[H][⇥][W]_\\n\\nbe a sequence of visual tokens (i.e., an output of VQ encoder followed by the vectorization) and _φ =_ _ps;φ_ _s=1_\\n_P_ _{_ _}[S]_\\n\\nbe a sequence of prompt tokens. For the AR transformer,\\nthe loss is given as follows:\\n\\nAR = Ex _P_ log P✓( _φ)_ (1)\\n_L_ _⇠_ _X_ _−_ _Z|P_\\n\\n_P✓(_ _φ) =_ _H⇥⇥_ _W_ _P✓(zi_ _z<i,_ _φ⇤)_ (2)\\n_Z|P_ _i=1_ _|_ _P_\\n\\nY\\n\\nFor the NAR transformer, we follow that of MaskGIT [7]:\\n\\n_LNAR = Ex⇠PX,M_ _⇠PM_ _−_ log P✓(ZM _|ZM_ _, Pφ)_ (3)\\n\\n_P✓(ZM_ _|ZM_ _, Pφ) =⇥_ _i_ _M_ _[P][✓][(][z][i][|Z][M]_ _[,][ P][φ]⇤[)]_ (4)\\n\\n_2_\\n\\nY\\n\\n\\nwhere M ⇢{1, ..., H⇥W _} is a set of visual token indices_\\nsampled from a masking schedule distribution P, M is its\\n_M_\\ncomplement, and _M =_ _zi_ _i_ _M_ . Prompt tuning proceeds\\n_Z_ _{_ _}_ _2_\\nby minimizing the respective loss with respect to the prompt\\nparameters φ while fixing the transformer parameters ✓:\\n\\n\\n_φ[⇤]_ = arg min\\n\\n\\nAR/NAR (5)\\n_L_\\n\\n\\n-----\\n\\n(a) Image synthesis using instance-conditioned prompts.\\n\\nDog\\n\\nDog\\n\\n\\n(b) Image synthesis using a marquee header prompt between instance (blue) and class (red) conditioned prompts.\\n\\n(c) Image synthesis using a marquee header prompt between instance-conditioned prompts (blue and red).\\n\\nFigure 4. Iterative decoding of NAR transformers. (4a) instance prompts generate images of high-fidelity but with low diversity. Marquee\\nheader prompts enhance generation diversity by interpolating (4b) from instance to class prompts or (4c) between instance prompts.\\n\\n\\nWhile we focus on the prompt tuning due to the virtue of\\neffectiveness and compute-efficiency for large source transformers, we note that the proposed learning framework is\\namenable with other methods, such as adapter [28] or finetuning [35], with learnable prompts. See a detailed comparison in Appendix B.4.\\n\\nAfter prompt tuning, we generate visual tokens for image\\n\\nsynthesis by iterative decoding. For AR transformer,\\n\\n1: for i 1 to H ⇥ _W do_\\n2: _zˆi_ _P✓(zi_ _zˆ<i,_ _φ)_\\n_⇠_ _|_ _P_\\n\\n3: end for\\n\\n\\ndimension P . For example, when using a prompt of length\\n_S=128, hidden P_ =768 and embedding dimension D=768,\\nthe token generator would introduce 10.4M parameters for\\n_C=100 class conditions, as in Fig. 3c. The bottleneck oc-_\\ncurs at the 3d weight tensor of size C⇥S⇥P .\\n\\nTo make it parameter efficient, we propose a factorized\\n\\ntoken generator (Fig. 3b). We encode class and sequence\\nposition index via MLPC and MLPP with F factors, respectively. The MLP outputs are element-wise summed, multiplied by a 1d factor vector from MLPF, and reduced along\\nthe factor dimension. The output is then fed to MLPT to\\nproduce a prompt of length S. As in Fig. 3c, the number of\\nparameters of the proposed architecture is greatly reduced,\\nrequiring only 0.76M parameters, down from 10.4M, for\\na prompt of length 128 when F = 1.[2] We empirically find\\nthat F = 1 is sufficient for NAR transformers. For AR transformers, extra capacity is needed by setting F = 16.\\n\\nMoreover, we build a new type of prompt tokens condi\\ntioned on individual data instances, inspired by the instanceconditioned GAN [5]. We assign each data a unique index\\nand map it into a distinct embedding via MLPC. When both\\nclass label and instance index are used, instance index is\\nsimply treated as an extra class, indexed from C. To train\\nthe model, we sample between class label and instance index. As we explain below in Sec. 3.3, instance-conditioned\\nprompts add more fine-grained control on generation.\\n\\n**3.3. Engineering Learned Prompts**\\n\\nGiven the wealth of learned prompts conditioned on the\\n\\nclass and instance proposed in Sec. 3.2, we propose a new\\n\\n2The proposed factorization can be extended to incorporate the “depth”\\n\\nposition of deep visual prompt [29] to reduce the number of parameters.\\n\\n\\nFor the NAR model, parallel decoding [7] is used:\\n**Require: M = {}, T**, {n1, ..., nT }, _t=1_ _[n][t][ =][ H][ ⇥]_ _[W]_\\n\\n1: for t 1 to T do\\n2: _zˆi_ _P✓(zi_ _M_ _,_ _φ),_ _i_ _M[P][T]_\\n_⇠_ _|Z_ _P_ _8_ _2_\\n\\n3: _M_ _M [ {arg topki 2 M_ _P✓(ˆzi|ZM_ _, Pφ), k = nt_ _}_\\n\\n4: end for [b]\\n\\n# $\\n\\nwhere _n1, ..., nT_ is a masking schedule that decides the[b]\\n_{_ _}_\\nnumber of tokens to decode at each step. We refer to [7] for\\ndetails on decoding for NAR transformers. Illustrations of\\ndecoding steps for both models are in Fig. 2.\\n\\n**3.2. Prompt Token Generator Design**\\n\\nFor transfer learning of discriminative tasks, prompts are\\n\\ndesigned without condition variables [29]. For generative\\ntasks, it is beneficial to have condition variables (e.g., class,\\nattribute) for better control in generation. We achieve this\\nwith a simple design of treating class conditions as another\\nprompt, as in Fig. 3a.\\n\\nOne critical issue is that the number of learnable param\\neters increases as the product of three factors: the number\\nof classes C, the prompt sequence length S and the feature\\n\\n\\n-----\\n\\n|Model (# tr params)|Mean|Mean (\\uf8ff10K)|C101|Flowers|Pet|DTD|Kitti|SUN|EuroSAT|Resisc|\\n|---|---|---|---|---|---|---|---|---|---|---|\\n|MineGAN [71] (88M) cGANTransfer [60] (105M)|151.5 85.1|114.0 63.8|102.4 89.6|132.1 61.6|130.1 48.6|87.4 70.3|117.9 48.9|77.5 31.1|111.5 45.6|81.0 50.3|\\n|Prompt (S = 1) (0.67M) Prompt (S = 16) (0.68M) Non-Autoregressive Prompt (S = 128) (0.76M) Scratch (172M)|53.7 39.9 36.4 42.7|19.7 18.6 18.6 60.0|13.5 12.7 12.9 72.7|13.8 13.2 13.4 57.2|11.9 11.1 10.9 70.3|25.8 26.0 25.9 66.1|32.3 30.0 29.9 33.8|7.3 7.4 7.7 9.2|45.9 35.8 38.4 39.5|28.5 24.9 24.8 32.0|\\n|Prompt (S = 1) (0.86M) Prompt (S = 16) (0.88M) Autoregressive Prompt (S = 256) (1.06M) Prompt (S = 256, F = 16) (5.16M) Scratch (306M)|73.2 47.4 39.0 36.9 39.6|44.1 34.5 32.3 26.6 61.8|45.4 41.4 39.6 27.2 76.0|28.9 19.6 17.3 14.1 56.1|42.2 36.6 34.9 27.2 52.5|37.1 33.4 32.5 30.0 92.7|66.8 41.4 37.1 34.6 31.6|18.8 16.4 15.0 12.8 13.5|37.3 32.6 29.6 26.4 19.4|35.1 28.8 26.7 22.2 29.5|\\n\\n\\nTable 1. FIDs (lower the better) on VTAB tasks. The number of trainable parameters (second column) are computed assuming 100 classes.\\nThe mean FID over 19 VTAB tasks (third column), over small-scale datasets (\\uf8ff10K, fourth column) and those with a small to mid-scale\\ntraining data are reported. Complete results are in Appendix C.1.3. The best and the second best results are highlighted in each column.\\n\\n\\nprompt engineering strategy, a “Marquee Header” prompt,\\ntailored to the non-autoregressive transformer decoding, for\\nenhancing generation diversity.\\n\\nWe interpolate the learned prompt representations (e.g.,\\n\\noutputs of MLPC). To account for the iterative decoding,\\nthe interpolation between prompts is carried out over multiple decoding steps. This is shown in Fig. 4b, where we start\\nthe decoding process using instance-conditioned prompts\\n(blue header) but gradually transition to a class-conditioned\\nprompt (red header) over decoding steps. Unlike the generation in Fig. 4a where the instance-conditioned prompts\\nare used all along, the marquee header prompt generates diverse images while maintaining the generation quality and\\nfollowing characteristics of reference instances (e.g., pose,\\ncolor pattern, hairiness). Fig. 4c shows a consistent trend\\nwhen applying the prompt between two image instances.\\n\\nThe marquee header prompt is formulated as follows:\\n\\nPMT(t) = (1 − _wt)PMT1 + wtPMT2_ (6)\\n\\n2\\n\\n_t_ 1\\n\\n_wt_ = min _−_ _, 1_ (7)\\n\\n_Tcuto↵_ 1\\n\\nn [✓] _−_ ◆ o\\n\\nwhere t = 1, ..., T is a decoding step, Tcuto↵ _T is a cutoff_\\n_\\uf8ff_\\nstep, and PMTi is a prompt representation (e.g., an output\\nof MLPC). The schedule in Eq. (7) makes a smooth transition of prompts from PMT1 to PMT2. We keep Eq. (7)’s\\nformulation as simple as possible and note that there could\\nbe various other prompt formulation'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_text[0:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
