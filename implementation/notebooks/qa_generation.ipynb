{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os \n",
    "import sys \n",
    "from pprint import pprint\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.pdf import read_pdf\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "from pipeline.entity_extraction import ENTITY_EXTRACTION_SYSTEM_PROMPT, EntityExtractionModel\n",
    "from pipeline.helpers import get_json_response, get_messages_response\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"ollama/llama3.2\"\n",
    "ollama_base_url = \"http://localhost:11434/v1\"\n",
    "lm_studio_base_url = \"http://localhost:1234/v1\"\n",
    "pdf_text = read_pdf(\"../data/Taming Transformers for High Resolution Image Synthesis.pdf\") \n",
    "\n",
    "lm_studio_client = OpenAI(base_url=lm_studio_base_url, api_key=\"lm_studio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "entities = get_json_response(\n",
    "    client=lm_studio_client,\n",
    "    # model=\"llama-3.2-3b-instruct-q8_0\"\n",
    "    model=\"meta-llama-3.1-8b-instruct-q6_k\",\n",
    "    messages=[\n",
    "        \n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": ENTITY_EXTRACTION_SYSTEM_PROMPT.format(text=pdf_text[0:20000])\n",
    "        },\n",
    "\n",
    "    ],\n",
    "    response_format=EntityExtractionModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entities.entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.question_answer import QUESTION_EXTRACTION_SYSTEM_PROMPT, QuestionAnswerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa_prompt = QUESTION_EXTRACTION_SYSTEM_PROMPT.format(text=pdf_text[0:20000], \n",
    "                                               entities=\",\".join(entities.entities[0:10]),\n",
    "                                               source=\"Taming Transformers for High Resolution Image Synthesis\",\n",
    "                                               source_type=\"paper\" # this can get automated\n",
    "                                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "questions = get_json_response(\n",
    "    client=lm_studio_client,\n",
    "    # model=\"llama-3.2-3b-instruct-q8_0\"\n",
    "    model=\"meta-llama-3.1-8b-instruct-q6_k\",\n",
    "    messages=[\n",
    "        \n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\":qa_prompt\n",
    "        },\n",
    "\n",
    "    ],\n",
    "    response_format=QuestionAnswerModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What is the purpose of using a convolutional approach in conjunction with transformers to model high-resolution images, as described in the paper 'Taming Transformers for High-Resolution Image Synthesis'?\",\n",
       " \"Why does the use of transformers in image synthesis pose fundamental problems for scaling them to high-resolution images, according to the paper 'Taming Transformers for High-Resolution Image Synthesis'?\",\n",
       " \"Summarize the method of learning an effective codebook of image constituents using a convolutional VQGAN and an autoregressive transformer architecture as presented in the paper 'Taming Transformers for High-Resolution Image Synthesis.'\",\n",
       " \"Where are the learnable tokens, or prompts, added in the process of adapting vision transformers to a new domain, as explained in the paper 'Visual prompt tuning'?\",\n",
       " \"What is the key insight that enables combining convolutional and transformer architectures to model the compositional nature of visual data, according to the paper 'Taming Transformers for High-Resolution Image Synthesis'?\",\n",
       " \"Why does learning an effective codebook of image constituents require pushing the limits of compression and using a perceptual loss with a patch-based discriminator, as described in the paper 'Taming Transformers for High-Resolution Image Synthesis'?\",\n",
       " \"Summarize the results of comparing transformer and PixelSNAIL architectures across different datasets and model sizes, as reported in Table 1 of the paper 'Taming Transformers for High-Resolution Image Synthesis.'\",\n",
       " \"What is the main advantage of using a two-stage approach with a VQGAN to learn an encoding of data and then learning a probabilistic model of this encoding, according to the paper 'Taming Transformers for High-Resolution Image Synthesis'?\",\n",
       " \"Why does the use of transformers in image synthesis require a powerful first stage that captures as much context as possible in the learned representation, as described in the paper 'Taming Transformers for High-Resolution Image Synthesis'?\",\n",
       " \"What is the role of the attention mechanism in limiting the sequence length and conditioning information in transformer-based high-resolution image synthesis, according to the paper 'Taming Transformers for High-Resolution Image Synthesis'?\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_pairs(questions: List[str]) -> List[dict]:\n",
    "    qa_pairs = []\n",
    "    for question in questions:\n",
    "        answer = get_messages_response(\n",
    "                                    client=lm_studio_client,\n",
    "                                    model=\"meta-llama-3.1-8b-instruct-q6_k\",\n",
    "                                    messages=[\n",
    "                                        \n",
    "                                        {\n",
    "                                            \"role\": \"system\",\n",
    "                                            \"content\":qa_prompt\n",
    "                                        },\n",
    "\n",
    "                                        {\n",
    "                                            \"role\": \"user\",\n",
    "                                            \"content\": question\n",
    "                                        },\n",
    "\n",
    "                                    ],\n",
    "                                )\n",
    "\n",
    "        qa_pairs.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "qa_pairs = generate_qa_pairs(questions.questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's why learning an effective codebook of image constituents requires pushing the limits of compression and using a perceptual loss with a patch-based discriminator:\n",
      "\n",
      "1.  **High-resolution images require complex representations**: High-resolution images contain intricate details, textures, and patterns that are difficult to capture with simple or low-dimensional representations. To effectively represent these images, the codebook needs to be rich and diverse, which requires pushing the limits of compression.\n",
      "2.  **Perceptual loss is necessary for preserving image quality**: The authors use a perceptual loss function, which measures the difference between the original image and its reconstructed version in terms of human perception. This loss function helps preserve the image's quality and details, even when compressing it to a lower dimensionality.\n",
      "3.  **Patch-based discriminator improves compression efficiency**: A patch-based discriminator is used to differentiate between real and reconstructed images at different spatial locations (patches). This approach allows for more efficient compression by focusing on the most important regions of the image, rather than trying to capture every detail uniformly.\n",
      "\n",
      "By combining these techniques, the authors can learn an effective codebook that efficiently represents high-resolution images while preserving their quality and details. The use of a perceptual loss with a patch-based discriminator enables the model to push the limits of compression, resulting in a more compact and expressive representation of the image constituents.\n",
      "\n",
      "Here's a possible question based on this explanation:\n",
      "\n",
      "*   \"What is the primary reason for using a perceptual loss function with a patch-based discriminator when learning an effective codebook of image constituents, as described in the paper 'Taming Transformers for High-Resolution Image Synthesis'?\"\n",
      "\n",
      "And here are some additional questions that explore different aspects of the topic:\n",
      "\n",
      "*   \"Why is it necessary to push the limits of compression when learning an effective codebook of image constituents?\"\n",
      "*   \"How does the use of a perceptual loss function help preserve image quality during compression?\"\n",
      "*   \"What advantages do patch-based discriminators offer in terms of compression efficiency?\"\n"
     ]
    }
   ],
   "source": [
    "print(qa_pairs[5].get('answer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create HF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/datasets/owenren/532_finetune_qa_datasets', endpoint='https://huggingface.co', repo_type='dataset', repo_id='owenren/532_finetune_qa_datasets')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Initialize API client\n",
    "api = HfApi()\n",
    "repo_id = \"owenren/532_finetune_qa_datasets\"\n",
    "\n",
    "api.create_repo(repo_id=repo_id, repo_type=\"dataset\", exist_ok=True, private=True, token=os.getenv('HUGGINGFACE_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qa_pair in qa_pairs:\n",
    "    qa_pair['source'] = \"Taming Transformers for High Resolution Image Synthesis.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs_dict = {\n",
    "    \"question\": [item[\"question\"] for item in qa_pairs],\n",
    "    \"answer\": [item[\"answer\"] for item in qa_pairs],\n",
    "    \"source\": [item[\"source\"] for item in qa_pairs]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict(qa_pairs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 528.18ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/owenren/532_finetune_qa_datasets/commit/4b464d52ebc49b502d194506b50db8dc4970fe89', commit_message='Upload dataset', commit_description='', oid='4b464d52ebc49b502d194506b50db8dc4970fe89', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"owenren/532_finetune_qa_datasets\", \n",
    "                        config_name=\"test_dataset_2024OCT20\",  # Give it a unique name\n",
    "                        token=os.getenv('HUGGINGFACE_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "answer = get_messages_response(\n",
    "    client=lm_studio_client,\n",
    "    # model=\"llama-3.2-3b-instruct-q8_0\"\n",
    "    model=\"meta-llama-3.1-8b-instruct-q6_k\",\n",
    "    messages=[\n",
    "        \n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\":qa_prompt\n",
    "        },\n",
    "\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": questions.questions[0]\n",
    "        },\n",
    "\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the paper \"Taming Transformers for High-Resolution Image Synthesis\", the authors propose combining the effectiveness of convolutional architectures with the expressivity of transformers to model and synthesize high-resolution images. The purpose of using a convolutional approach in conjunction with transformers is to:\n",
      "\n",
      "1. **Efficiently learn local structure**: Convolutional neural networks (CNNs) are well-suited for learning local structures and regularities in images, which is essential for modeling high-resolution images.\n",
      "2. **Capture context-rich visual parts**: The authors use a convolutional VQGAN (Vector Quantized Generative Adversarial Network) to learn a codebook of context-rich visual parts, which can be used as input to the transformer architecture.\n",
      "3. **Reduce computational costs**: By using a convolutional approach to learn local structure and capture context-rich visual parts, the authors aim to reduce the computational costs associated with modeling high-resolution images using transformers alone.\n",
      "\n",
      "The combination of convolutional and transformer architectures allows for:\n",
      "\n",
      "1. **Efficient modeling of global interrelations**: The transformer architecture can model long-range interactions between visual parts, while the convolutional approach provides a more efficient way to learn local structure.\n",
      "2. **Improved image synthesis quality**: By combining the strengths of both architectures, the authors demonstrate improved image synthesis quality and efficiency compared to using transformers alone.\n",
      "\n",
      "In summary, the purpose of using a convolutional approach in conjunction with transformers is to efficiently model high-resolution images by leveraging the strengths of both architectures: convolutional networks for learning local structure and transformers for modeling long-range interactions.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
