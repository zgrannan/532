{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os \n",
    "import sys \n",
    "from pprint import pprint\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.pdf import read_pdf\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like crewAI leverages Litellm for model inference. LM Studio isn't one of the listed providers. Seems like custom models off HF may be a challenge to use. Lets leverage Ollama models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"ollama/llama3.2\"\n",
    "base_url = \"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(base_url=base_url, model=model, temperature=0, api_key=\"test\")\n",
    "pdf_text = read_pdf(\"../data/Sohn_Visual_Prompt_Tuning_for_Generative_Transfer_Learning_CVPR_2023_paper.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summary(BaseModel):\n",
    "    summary: str=Field(..., description=\"Cohesive summary of the text.\")\n",
    "    key_points: List[str] = Field(..., description=\"Key points, figures, facts from the input text.\")\n",
    "    source: str = Field(..., description=\"Source of the summary. It can be a URL, a name, or a description.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_agent = Agent(\n",
    "    role=\"summarizer\",\n",
    "    goal=\"Generating a cohesive summary of the given text and extracting key points from it.\",\n",
    "    backstory=\"\"\"You are working on a project that will summarize given text\"\"\",\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumamry_task = Task(\n",
    "    agent=summarizer_agent,\n",
    "    output_json = Summary,\n",
    "    description=\"Conduct a thorough summarization of the given text. \\n {text} \\n Include key points, figures, and facts from the text.\",\n",
    "    expected_output = \"A cohesive summary of the given text and extracting key points from it.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-19 17:04:18,426 - 133946742433600 - __init__.py-__init__:538 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "crew = Crew(\n",
    "    tasks=[sumamry_task],\n",
    "    agents=[summarizer_agent],\n",
    "    verbose=True,\n",
    "  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92msummarizer\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mConduct a thorough summarization of the given text. \n",
      " # Visual Prompt Tuning for Generative Transfer Learning\n",
      "\n",
      "\n",
      "Kihyuk Sohn, Huiwen Chang, Jos¬¥e Lezama, Luisa Polania,\n",
      "\n",
      "Han Zhang, Yuan Hao, Irfan Essa, Lu Jiang\n",
      "\n",
      "Google Research\n",
      "\n",
      "Figure 1. Image synthesis by knowledge transfer. Unlike previous works using GANs as base model and test transfer on relatively narrow\n",
      "visual domains, we transfer knowledge of generative vision transformers [7, 15] to a wide range of visual domains, including natural\n",
      "(e.g., scene, flower), specialized (e.g., satellite, medical), and structured (e.g., road scenes, infograph, sketch) with a few training images.\n",
      "Notably, the prompt tuning significantly improves the prior best FID on two benchmarks ImageNet (85.9!16.3) and Places (71.3!24.2).\n",
      "\n",
      "\n",
      "**Abstract**\n",
      "\n",
      "_Learning generative image models from various domains_\n",
      "\n",
      "_efficiently needs transferring knowledge from an image syn-_\n",
      "_thesis model trained on a large dataset. We present a recipe_\n",
      "_for learning vision transformers by generative knowledge_\n",
      "_transfer. We base our framework on generative vision trans-_\n",
      "_formers representing an image as a sequence of visual to-_\n",
      "_kens with the autoregressive or non-autoregressive trans-_\n",
      "_formers. To adapt to a new domain, we employ prompt tun-_\n",
      "_ing, which prepends learnable tokens called prompts to the_\n",
      "_image token sequence and introduces a new prompt design_\n",
      "_for our task. We study on a variety of visual domains with_\n",
      "_varying amounts of training images. We show the effective-_\n",
      "_ness of knowledge transfer and a significantly better image_\n",
      "_generation quality.[1]_\n",
      "\n",
      "**1. Introduction**\n",
      "\n",
      "Image synthesis has witnessed tremendous progress re\n",
      "cently with the advancement of deep generative models [2,\n",
      "\n",
      "1https://github.com/google-research/generative_\n",
      "\n",
      "transfer\n",
      "\n",
      "\n",
      "12, 20, 67, 69]. An ideal image synthesis system generates\n",
      "diverse, plausible, and novel scenes capturing the appearance of objects and depicting their interactions. The success\n",
      "of image synthesis does heavily rely on the availability of a\n",
      "large amount of diverse training data [73].\n",
      "\n",
      "Transfer learning, a cornerstone invention in deep learn\n",
      "ing, has proven indispensable in an array of computer vision\n",
      "tasks, including classification [35], object detection [18,19],\n",
      "image segmentation [23, 24], etc. However, transfer learning is not widely used for image synthesis. While recent\n",
      "efforts have shown success in transferring knowledge from\n",
      "pre-trained Generative Adversarial Network (GAN) models [46, 60, 71, 76], their demonstrations are limited to narrow visual domains, e.g., faces or cars [46, 76], as in Fig. 1,\n",
      "or requiring a non-trivial amount of training data [60, 71] to\n",
      "transfer to out-of-distribution domains.\n",
      "\n",
      "In this work, we approach transfer learning for image\n",
      "\n",
      "synthesis using generative vision transformers, an emerging class of image synthesis models, such as DALL¬∑E [53],\n",
      "Taming Transformer [15], MaskGIT [7], CogView [13],\n",
      "N UWA [[¬®] 75], Parti [79], among others, which excel in im\n",
      "\n",
      "-----\n",
      "\n",
      "age synthesis tasks. We closely follow the recipe of transfer learning for image classification [35], in which a source\n",
      "model is first trained on a large dataset (e.g., ImageNet) and\n",
      "then transferred to a diverse collection of downstream tasks.\n",
      "Except, in our setting, the input and output are reversed and\n",
      "the model generates images from a class label.\n",
      "\n",
      "We present a transfer learning framework using prompt\n",
      "\n",
      "_tuning [38,40]. While the technique has been used for trans-_\n",
      "fer learning of discriminative models for vision tasks [1,29],\n",
      "we appear to be the first to adopt prompt tuning for trans_fer learning of image synthesis. To this end, we propose a_\n",
      "parameter-efficient design of a prompt token generator that\n",
      "admits condition variables (e.g., class), a key for controllable image synthesis neglected in prompt tuning for discriminative transfer [29, 38]. We also introduce a marquee\n",
      "header prompt that engineers learned prompts to enhance\n",
      "generation diversity while retaining the generation quality.\n",
      "\n",
      "We conduct a large-scale study to understand the me\n",
      "chanics of transfer learning for generative vision transformers. Two types of generative transformers ‚Äì AutoRegressive\n",
      "_(AR) and Non-AutoRegressive (NAR) ‚Äì are examined. AR_\n",
      "transformers (e.g., DALL¬∑E [53], Taming Transformer [15],\n",
      "Parti [79]) generate image tokens sequentially with an\n",
      "autoregressive language model. NAR transformers (e.g.,\n",
      "\n",
      "MaskGIT [7], MUSE [6]) or diffusion models (e.g., Imagen [58], Latent Diffusion [57]) decompose image synthesis as a series of refinement or denoising steps. In this work,\n",
      "we study transfer learning of class-conditional AR [15] and\n",
      "NAR [7] transformer models trained on ImageNet to comply with existing transfer learning settings [60, 71]. In addition to investigating proposed prompt tuning, we also conduct an analysis of two other transfer learning methods, i.e.\n",
      "full fine-tuning and adapter tuning, in the context of generative transfer learning using vision transformers. We compare their strengths and weaknesses in Sec. 4.1.\n",
      "\n",
      "Our study shows that generative vision transformers with\n",
      "\n",
      "prompt tuning outperform state-of-the-art methods using\n",
      "GANs [60, 71] by a vast margin, which is verified on 19\n",
      "tasks of diverse visual distributions and drastically different\n",
      "amounts of training data in VTAB [81]. Fig. 1 compares\n",
      "domains, showing the great expansion of downstream domains to what is achieved by previous works. On the onmanifold domains on which previous studies have focused,\n",
      "our method slashes the prior state-of-the-art in FID from\n",
      "71 to 24 on Places [85] and 86 to 16 on Animal Face [61]\n",
      "datasets. Moreover, our method shows highly-competitive\n",
      "data efficiency, generating diverse images following the target distribution when trained from a few images per class.\n",
      "\n",
      "In summary, our contributions are as follows:\n",
      "\n",
      "_‚Ä¢ We present a generative visual transfer learning frame-_\n",
      "\n",
      "work for vision transformers with prompt tuning [38],\n",
      "proposing a new prompt token generator design.\n",
      "\n",
      "_‚Ä¢ We conduct a large-scale empirical study for genera-_\n",
      "\n",
      "\n",
      "tive transfer learning to validate our proposed prompt\n",
      "tuning and relevant transfer learning methods (e.g., full\n",
      "fine-tuning, adapter tuning) on several visual domains\n",
      "(e.g., VTAB) and scenarios (e.g., few-shot). We show\n",
      "state-of-the-art image synthesis performance.\n",
      "\n",
      "_‚Ä¢ To our knowledge, we are first to propose the use of_\n",
      "\n",
      "prompt tuning for transfer learning of generative transformers. Importantly, we provide the quantitative evidence on the necessity of generative knowledge transfer on VTAB [81], the common and challenging transfer learning benchmark.\n",
      "\n",
      "**2. Preliminary**\n",
      "\n",
      "**2.1. Generative Vision Transformers**\n",
      "\n",
      "This paper uses generative vision transformers to denote\n",
      "\n",
      "vision transformers for image synthesis. Broadly, there are\n",
      "two types of generative transformers, AutoRegressive (AR)\n",
      "and Non-AutoRegressive (NAR) transformers, both consisting of two stages ‚Äì image quantization and decoding. The\n",
      "two models share the same first stage: image quantization\n",
      "by a Vector-Quantized (VQ) auto-encoder [15, 54, 67, 78].\n",
      "The VQ encoder converts image patches into indices (or tokens) in a codebook. The 2D image is then flattened into\n",
      "a 1D sequence to which a special token indicating its class\n",
      "label is prepended.\n",
      "\n",
      "Pretrain on ImageNet Flowers, Retinopathy, Kitti, ‚Ä¶\n",
      "\n",
      "transfer\n",
      "\n",
      "Mutable\n",
      "\n",
      "AR / NAR AR / NAR\n",
      "\n",
      "transformer Frozen transformer\n",
      "\n",
      "Prompt token\n",
      "\n",
      "Visual token\n",
      "\n",
      "Autoregressive Decoding\n",
      "\n",
      "t=0 t=1 t=2 t=100 t=160 t=256\n",
      "\n",
      "Non-autoregressive (parallel) Decoding\n",
      "\n",
      "t=0 t=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8\n",
      "\n",
      "\n",
      "Figure 2. Our method transfers knowledge from generative vision\n",
      "transformers (e.g., autoregressive [15] or non-autoregressive [7])\n",
      "trained on a large dataset to various visual domains by prepending\n",
      "learnable prompt tokens (green) to visual tokens (blue).\n",
      "\n",
      "AR and NAR transformers differ in the second stage. AR\n",
      "\n",
      "transformers [8, 13, 15, 53, 75, 79], such as DALL¬∑E [53],\n",
      "Taming Transformer [15], learn an AR decoder on the flattened token sequence to generate image tokens sequentially\n",
      "from previously generated tokens. As in Fig. 2, the generation follows a raster scan ordering, generating tokens from\n",
      "left to right, line-by-line. Finally, the generated tokens are\n",
      "mapped to the pixel space using the VQ decoder.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "|Transformer|Col2|Col3|Col4|Col5|Col6|\n",
      "|---|---|---|---|---|---|\n",
      "|Transformer||||||\n",
      "|||||||\n",
      "\n",
      "|ùù® F B x S x P x F 1 x 1 x 1 x F B x 1 x P x F 1 x S x P x F MLP MLP MLP C P F class / instance position factor|Col2|\n",
      "|---|---|\n",
      "\n",
      "\n",
      "S\n",
      "\n",
      "B x S x P MLPT D\n",
      "\n",
      "\n",
      "decode\n",
      "\n",
      "\n",
      "(a) Baseline prompt token generators of length\n",
      "_S conditioned on class._\n",
      "\n",
      "\n",
      "(b) The proposed parameter efficient prompt token generator via factorization of class / instance\n",
      "and position. ‚äï is an element-wise sum, ‚äô is an element-wise product, ‚åÉF is a sum over F\n",
      "dimension. S: sequence length, B: batch size, P : feature dimension, D: token dimension.\n",
      "\n",
      "\n",
      "(c) Number of parameters with respect to the\n",
      "sequence length and different number of factors F .\n",
      "\n",
      "\n",
      "Figure 3. Prompt token generators and their use in transformer. (a) a straightforward extension of baseline prompt token generators [29,38,\n",
      "40] with a class condition. When using an MLP with a single dense layer of P units, the number of trainable parameters is P _¬∑(C¬∑S+D)._\n",
      "(b) The proposed parameter efficient prompt token generators that factorizes data dependent conditions (e.g., class, instance) and token\n",
      "position. Under a similar design choice as baseline models, the number of trainable parameters is P _¬∑(F_ _¬∑(C+S)+D), which could be_\n",
      "significantly fewer when F _‚åß_ min(C, S). (c) Number of parameters for prompt token generators with respect to the sequence length (S),\n",
      "while setting P = 768, D = 768, and C = 100 with different number of factors F .\n",
      "\n",
      "\n",
      "NAR or diffusion models, including DALL¬∑E 2 [52],\n",
      "\n",
      "MaskGIT [7], Latent Diffusion [57], or Imagen [58], decompose image synthesis as a series of refinement or denoising steps. For prompt tuning, we need a NAR model\n",
      "with the transformer backbone [7, 17, 21, 36, 37, 39, 83], and\n",
      "use a leading NAR image transformer called MaskGIT [7].\n",
      "\n",
      "NAR transformers are trained on the masked modeling\n",
      "\n",
      "proxy task [11]. For inference, the model adopts a nonautoregressive decoding method to synthesize an image in a\n",
      "few steps [7, 21, 36, 39]. As in Fig. 2, the NAR transformer\n",
      "starts from a blank canvas with all tokens masked, and generates an image in 8 steps or so. In each step, it predicts all\n",
      "tokens in parallel and retains the ones with the highest prediction scores. The remaining tokens are masked out and\n",
      "predicted in the next iteration. NAR transformers [7, 39]\n",
      "have shown faster inference than AR transformers.\n",
      "\n",
      "**2.2. Prompt Tuning**\n",
      "\n",
      "Prompt tuning [38, 40] is introduced recently in natural\n",
      "\n",
      "language processing as a way of efficiently adapting pretrained large language models to downstream tasks. Here,\n",
      "prompt is a sequence of additional tokens prepended to a\n",
      "token sequence. In prompt engineering [3], their values are\n",
      "often chosen by heuristic. On the other hand, in prompt\n",
      "tuning [38, 40], tokens are parameterized by learnable parameters and their parameters are updated via gradient descent to adapt transformers to the downstream tasks. Due to\n",
      "its simplicity and as transformers‚Äô central role in language\n",
      "foundation models, prompt tuning has been applied to some\n",
      "vision tasks for knowledge transfer, e.g., image classification [1, 29], detection and segmentation [45], but not yet for\n",
      "image synthesis.\n",
      "\n",
      "**3. Visual Prompt for Generative Transfer**\n",
      "\n",
      "Fig. 2 overviews the proposed generative transfer learn\n",
      "ing framework. We aim at transferring a generative prior,\n",
      "\n",
      "\n",
      "parameterized by generative vision transformers, while utilizing the same VQ encoder and decoder trained from the\n",
      "large source dataset. We use prompt tuning to adapt to\n",
      "\n",
      "the target distributions while leaving the transformer parameters frozen. We discuss how to learn visual prompts\n",
      "(Sec. 3.1), a new prompt generator for conditional image\n",
      "synthesis (Sec. 3.2), and a prompt design for generating visually diverse images (Sec. 3.3).\n",
      "\n",
      "**3.1. Learning Visual Prompt**\n",
      "\n",
      "A sequence of prompt tokens is prepended to the visual\n",
      "\n",
      "tokens to guide the pretrained transformer models to the target distribution. Prompt tuning, learning the parameters of\n",
      "the token generator, is optimized by gradient descent with\n",
      "respective loss functions, while fixing the parameters of the\n",
      "pretrained transformers. To be specific, let = _zi_ _i=1_\n",
      "_Z_ _{_ _}[H][‚á•][W]_\n",
      "\n",
      "be a sequence of visual tokens (i.e., an output of VQ encoder followed by the vectorization) and _œÜ =_ _ps;œÜ_ _s=1_\n",
      "_P_ _{_ _}[S]_\n",
      "\n",
      "be a sequence of prompt tokens. For the AR transformer,\n",
      "the loss is given as follows:\n",
      "\n",
      "AR = Ex _P_ log P‚úì( _œÜ)_ (1)\n",
      "_L_ _‚á†_ _X_ _‚àí_ _Z|P_\n",
      "\n",
      "_P‚úì(_ _œÜ) =_ _H‚á•‚á•_ _W_ _P‚úì(zi_ _z<i,_ _œÜ‚á§)_ (2)\n",
      "_Z|P_ _i=1_ _|_ _P_\n",
      "\n",
      "Y\n",
      "\n",
      "For the NAR transformer, we follow that of MaskGIT [7]:\n",
      "\n",
      "_LNAR = Ex‚á†PX,M_ _‚á†PM_ _‚àí_ log P‚úì(ZM _|ZM_ _, PœÜ)_ (3)\n",
      "\n",
      "_P‚úì(ZM_ _|ZM_ _, PœÜ) =‚á•_ _i_ _M_ _[P][‚úì][(][z][i][|Z][M]_ _[,][ P][œÜ]‚á§[)]_ (4)\n",
      "\n",
      "_2_\n",
      "\n",
      "Y\n",
      "\n",
      "\n",
      "where M ‚á¢{1, ..., H‚á•W _} is a set of visual token indices_\n",
      "sampled from a masking schedule distribution P, M is its\n",
      "_M_\n",
      "complement, and _M =_ _zi_ _i_ _M_ . Prompt tuning proceeds\n",
      "_Z_ _{_ _}_ _2_\n",
      "by minimizing the respective loss with respect to the prompt\n",
      "parameters œÜ while fixing the transformer parameters ‚úì:\n",
      "\n",
      "\n",
      "_œÜ[‚á§]_ = arg min\n",
      "\n",
      "\n",
      "AR/NAR (5)\n",
      "_L_\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "(a) Image synthesis using instance-conditioned prompts.\n",
      "\n",
      "Dog\n",
      "\n",
      "Dog\n",
      "\n",
      "\n",
      "(b) Image synthesis using a marquee header prompt between instance (blue) and class (red) conditioned prompts.\n",
      "\n",
      "(c) Image synthesis using a marquee header prompt between instance-conditioned prompts (blue and red).\n",
      "\n",
      "Figure 4. Iterative decoding of NAR transformers. (4a) instance prompts generate images of high-fidelity but with low diversity. Marquee\n",
      "header prompts enhance generation diversity by interpolating (4b) from instance to class prompts or (4c) between instance prompts.\n",
      "\n",
      "\n",
      "While we focus on the prompt tuning due to the virtue of\n",
      "effectiveness and compute-efficiency for large source transformers, we note that the proposed learning framework is\n",
      "amenable with other methods, such as adapter [28] or finetuning [35], with learnable prompts. See a detailed comparison in Appendix B.4.\n",
      "\n",
      "After prompt tuning, we generate visual tokens for image\n",
      "\n",
      "synthesis by iterative decoding. For AR transformer,\n",
      "\n",
      "1: for i 1 to H ‚á• _W do_\n",
      "2: _zÀÜi_ _P‚úì(zi_ _zÀÜ<i,_ _œÜ)_\n",
      "_‚á†_ _|_ _P_\n",
      "\n",
      "3: end for\n",
      "\n",
      "\n",
      "dimension P . For example, when using a prompt of length\n",
      "_S=128, hidden P_ =768 and embedding dimension D=768,\n",
      "the token generator would introduce 10.4M parameters for\n",
      "_C=100 class conditions, as in Fig. 3c. The bottleneck oc-_\n",
      "curs at the 3d weight tensor of size C‚á•S‚á•P .\n",
      "\n",
      "To make it parameter efficient, we propose a factorized\n",
      "\n",
      "token generator (Fig. 3b). We encode class and sequence\n",
      "position index via MLPC and MLPP with F factors, respectively. The MLP outputs are element-wise summed, multiplied by a 1d factor vector from MLPF, and reduced along\n",
      "the factor dimension. The output is then fed to MLPT to\n",
      "produce a prompt of length S. As in Fig. 3c, the number of\n",
      "parameters of the proposed architecture is greatly reduced,\n",
      "requiring only 0.76M parameters, down from 10.4M, for\n",
      "a prompt of length 128 when F = 1.[2] We empirically find\n",
      "that F = 1 is sufficient for NAR transformers. For AR transformers, extra capacity is needed by setting F = 16.\n",
      "\n",
      "Moreover, we build a new type of prompt tokens condi\n",
      "tioned on individual data instances, inspired by the instanceconditioned GAN [5]. We assign each data a unique index\n",
      "and map it into a distinct embedding via MLPC. When both\n",
      "class label and instance index are used, instance index is\n",
      "simply treated as an extra class, indexed from C. To train\n",
      "the model, we sample between class label and instance index. As we explain below in Sec. 3.3, instance-conditioned\n",
      "prompts add more fine-grained control on generation.\n",
      "\n",
      "**3.3. Engineering Learned Prompts**\n",
      "\n",
      "Given the wealth of learned prompts conditioned on the\n",
      "\n",
      "class and instance proposed in Sec. 3.2, we propose a new\n",
      "\n",
      "2The proposed factorization can be extended to incorporate the ‚Äúdepth‚Äù\n",
      "\n",
      "position of deep visual prompt [29] to reduce the number of parameters.\n",
      "\n",
      "\n",
      "For the NAR model, parallel decoding [7] is used:\n",
      "**Require: M = {}, T**, {n1, ..., nT }, _t=1_ _[n][t][ =][ H][ ‚á•]_ _[W]_\n",
      "\n",
      "1: for t 1 to T do\n",
      "2: _zÀÜi_ _P‚úì(zi_ _M_ _,_ _œÜ),_ _i_ _M[P][T]_\n",
      "_‚á†_ _|Z_ _P_ _8_ _2_\n",
      "\n",
      "3: _M_ _M [ {arg topki 2 M_ _P‚úì(ÀÜzi|ZM_ _, PœÜ), k = nt_ _}_\n",
      "\n",
      "4: end for [b]\n",
      "\n",
      "# $\n",
      "\n",
      "where _n1, ..., nT_ is a masking schedule that decides the[b]\n",
      "_{_ _}_\n",
      "number of tokens to decode at each step. We refer to [7] for\n",
      "details on decoding for NAR transformers. Illustrations of\n",
      "decoding steps for both models are in Fig. 2.\n",
      "\n",
      "**3.2. Prompt Token Generator Design**\n",
      "\n",
      "For transfer learning of discriminative tasks, prompts are\n",
      "\n",
      "designed without condition variables [29]. For generative\n",
      "tasks, it is beneficial to have condition variables (e.g., class,\n",
      "attribute) for better control in generation. We achieve this\n",
      "with a simple design of treating class conditions as another\n",
      "prompt, as in Fig. 3a.\n",
      "\n",
      "One critical issue is that the number of learnable param\n",
      "eters increases as the product of three factors: the number\n",
      "of classes C, the prompt sequence length S and the feature\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "|Model (# tr params)|Mean|Mean (Ô£ø10K)|C101|Flowers|Pet|DTD|Kitti|SUN|EuroSAT|Resisc|\n",
      "|---|---|---|---|---|---|---|---|---|---|---|\n",
      "|MineGAN [71] (88M) cGANTransfer [60] (105M)|151.5 85.1|114.0 63.8|102.4 89.6|132.1 61.6|130.1 48.6|87.4 70.3|117.9 48.9|77.5 31.1|111.5 45.6|81.0 50.3|\n",
      "|Prompt (S = 1) (0.67M) Prompt (S = 16) (0.68M) Non-Autoregressive Prompt (S = 128) (0.76M) Scratch (172M)|53.7 39.9 36.4 42.7|19.7 18.6 18.6 60.0|13.5 12.7 12.9 72.7|13.8 13.2 13.4 57.2|11.9 11.1 10.9 70.3|25.8 26.0 25.9 66.1|32.3 30.0 29.9 33.8|7.3 7.4 7.7 9.2|45.9 35.8 38.4 39.5|28.5 24.9 24.8 32.0|\n",
      "|Prompt (S = 1) (0.86M) Prompt (S = 16) (0.88M) Autoregressive Prompt (S = 256) (1.06M) Prompt (S = 256, F = 16) (5.16M) Scratch (306M)|73.2 47.4 39.0 36.9 39.6|44.1 34.5 32.3 26.6 61.8|45.4 41.4 39.6 27.2 76.0|28.9 19.6 17.3 14.1 56.1|42.2 36.6 34.9 27.2 52.5|37.1 33.4 32.5 30.0 92.7|66.8 41.4 37.1 34.6 31.6|18.8 16.4 15.0 12.8 13.5|37.3 32.6 29.6 26.4 19.4|35.1 28.8 26.7 22.2 29.5|\n",
      "\n",
      "\n",
      "Table 1. FIDs (lower the better) on VTAB tasks. The number of trainable parameters (second column) are computed assuming 100 classes.\n",
      "The mean FID over 19 VTAB tasks (third column), over small-scale datasets (Ô£ø10K, fourth column) and those with a small to mid-scale\n",
      "training data are reported. Complete results are in Appendix C.1.3. The best and the second best results are highlighted in each column.\n",
      "\n",
      "\n",
      "prompt engineering strategy, a ‚ÄúMarquee Header‚Äù prompt,\n",
      "tailored to the non-autoregressive transformer decoding, for\n",
      "enhancing generation diversity.\n",
      "\n",
      "We interpolate the learned prompt representations (e.g.,\n",
      "\n",
      "outputs of MLPC). To account for the iterative decoding,\n",
      "the interpolation between prompts is carried out over multiple decoding steps. This is shown in Fig. 4b, where we start\n",
      "the decoding process using instance-conditioned prompts\n",
      "(blue header) but gradually transition to a class-conditioned\n",
      "prompt (red header) over decoding steps. Unlike the generation in Fig. 4a where the instance-conditioned prompts\n",
      "are used all along, the marquee header prompt generates diverse images while maintaining the generation quality and\n",
      "following characteristics of reference instances (e.g., pose,\n",
      "color pattern, hairiness). Fig. 4c shows a consistent trend\n",
      "when applying the prompt between two image instances.\n",
      "\n",
      "The marquee header prompt is formulated as follows:\n",
      "\n",
      "PMT(t) = (1 ‚àí _wt)PMT1 + wtPMT2_ (6)\n",
      "\n",
      "2\n",
      "\n",
      "_t_ 1\n",
      "\n",
      "_wt_ = min _‚àí_ _, 1_ (7)\n",
      "\n",
      "_Tcuto‚Üµ_ 1\n",
      "\n",
      "n [‚úì] _‚àí_ ‚óÜ o\n",
      "\n",
      "where t = 1, ..., T is a decoding step, Tcuto‚Üµ _T is a cutoff_\n",
      "_Ô£ø_\n",
      "step, and PMTi is a prompt representation (e.g., an output\n",
      "of MLPC). The schedule in Eq. (7) makes a smooth transition of prompts from PMT1 to PMT2. We keep Eq. (7)‚Äôs\n",
      "formulation as simple as possible and note that there could\n",
      "be various other prompt formulations, which we leave their\n",
      "investigations as our future work.\n",
      "\n",
      "**4. Experiments**\n",
      "\n",
      "We conduct extensive experiments of generative transfer\n",
      "\n",
      "learning by prompt tuning. Sec. 4.1 evaluates the efficacy\n",
      "on diverse visual domains on the VTAB benchmark [81].\n",
      "Sec. 4.2 assess the task of few-shot transfer learning on six\n",
      "common benchmarks. Sec. 4.3 presents more discussions.\n",
      "\n",
      "**4.1. Generative Transfer on VTAB**\n",
      "\n",
      "**Dataset. We employ the visual task adaptation benchmark**\n",
      "(VTAB) [81] ‚Äì a suite of 19 visual recognition tasks based\n",
      "\n",
      "\n",
      "on 16 datasets. VTAB covers diverse image domains (e.g.,\n",
      "natural, structured, and specialized such as medical or satellite imagery) and tasks (e.g., object and scene recognition,\n",
      "distance classification, and counting). while VTAB serves\n",
      "as a standard yet challenging benchmark for transferring\n",
      "representation, this work provides the first study of generative transfer learning on the VTAB benchmark.\n",
      "**Setting. We train class-conditional image generation mod-**\n",
      "els on the VTAB (full) tasks, where the class-conditional\n",
      "prompts are trained on the ‚Äútrain‚Äù split, using the same hyperparameters across tasks. We investigate the generative\n",
      "transfer of AR [15] and NAR transformers [7] trained on\n",
      "256‚á•256 images of the ImageNet dataset as source models. Both models contain 24 transformer layers, comprised\n",
      "of 306M and 172M model parameters, respectively. See\n",
      "more implementation details in Appendix C.1.2.\n",
      "**Baselines. We compare our method against state-of-the-**\n",
      "art GAN-based transfer learning methods, including MineGAN [71] and cGANTransfer [60]. Both models use BigGAN [2] trained on ImageNet as the source. BigGan‚Äôs FID\n",
      "on the ImageNet validation is 7.4 which is better than our\n",
      "pretrained AR transformer (18.7) and almost on par with\n",
      "that of NAR transformer (6.2).\n",
      "\n",
      "In addition, we compare generative transformers trained\n",
      "\n",
      "from scratch on VTAB with a comparable number of training epochs. We provide an analysis under different compute\n",
      "budgets in Appendix B.4.\n",
      "**Evaluation. We use Frechet Inception Distance (FID) [27].**\n",
      "FID is computed using 20k generated images and 20k real\n",
      "images randomly sampled from a respective dataset.\n",
      "**Results. We report mean FIDs over 3 runs in Tab. 1. As**\n",
      "shown in Tab. 1, prompt tuning is effective for both AR and\n",
      "NAR generative transformers, especially when the number\n",
      "of training images is small (e.g., Ô£ø 10k). We find that\n",
      "\n",
      "the NAR model transfers better than the AR model. Nevertheless, both models with class-conditional prompt tuning show significant gains in performance over GAN-based\n",
      "baselines. These comparisons validate the superiority of\n",
      "prompt tuning over the prior state-of-the-arts. The result\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "(a) SUN397 (NAR) (b) DTD (NAR) (c) Resisc (NAR) (d) SUN397 (AR) (e) DTD (AR) (f) Resisc (AR)\n",
      "\n",
      "Figure 5. Class conditional generation using (a ‚Äì c) NAR (S=128) and (d ‚Äì f) AR (S=256, F =16) transformers with prompt tuning.\n",
      "\n",
      "Method # params 10 epoch 200 epoch 800/1600 epoch which have more tunable parameters. Nevertheless, our re\n",
      "Prompt (S = 128) 0.76M 27.6 18.5 17.7 sults consistently show the necessity of generative knowl\n",
      "Prompt + Adapter 5.43M 20.1 15.7 15.1 edge transfer when learning from limited training data.\n",
      "\n",
      "**4.2. Few-shot Generative Transfer**\n",
      "\n",
      "|Method|# params|10 epoch|200 epoch|800/1600 epoch|\n",
      "|---|---|---|---|---|\n",
      "|Prompt (S = 128) Prompt + Adapter Prompt + Fine-tune Scratch|0.76M 5.43M 172M 172M|27.6 20.1 19.5 ‚Äì|18.5 15.7 15.0 60.0|17.7 15.1 14.2 22.7|\n",
      "\n",
      "\n",
      "Table 2. FID vs the number of train epochs for different learning methods of NAR transformers on VTAB small-scale datasets\n",
      "(Ô£ø10k). Each number of trainable parameters is provided in the\n",
      "second column. Complete results are in Appendix B.1.\n",
      "\n",
      "also provides the first quantitative evidence on the necessity\n",
      "of generative knowledge transfer on the VTAB benchmark.\n",
      "\n",
      "In Fig. 5, we show generated images using 128 prompt\n",
      "\n",
      "tokens for NAR transformers and 256 prompt tokens (with\n",
      "_F = 16) for AR transformers on a few VTAB tasks. Due to_\n",
      "limited space, we report complete results on all 19 tasks in\n",
      "Appendix C.1.3 and generated images in Appendix C.1.4.\n",
      "\n",
      "Tab. 1 also shows that prompt tuning of generative trans\n",
      "formers benefits greatly from a long prompt, reducing mean\n",
      "FID from 53.7 to 36.4 by increasing the length from 1 to\n",
      "128. This is achieved by only adding extra 0.1M parameters\n",
      "(0.76M overall), thanks to our parameter-efficient design\n",
      "of the prompt token generator. Besides, AR transformers\n",
      "generally require prompts with more learnable parameters,\n",
      "which needs increasing the number of factors. The performance is still on par with that achievable with the baseline\n",
      "prompt, while using significantly less number of parameters\n",
      "(5.6M instead of 20.5M), as shown in Appendix B.3. The\n",
      "above results verify the design of our parameter-efficient\n",
      "prompt token generator.\n",
      "**Transfer learning settings. We compare prompt tuning**\n",
      "with other transfer learning settings including 1) full finetuning, 2) adapter tuning [28], and 3) learning from scratch\n",
      "on the target domain. To adapt these methods for generative\n",
      "transfer learning, we integrate prompt tuning with them to\n",
      "introduce class conditioning for image synthesis.\n",
      "\n",
      "The results are given in Tab. 2, with detailed results avail\n",
      "able in Appendix B.4. Our findings indicate that prompt\n",
      "tuning is the most efficient approach, making it likely the\n",
      "only feasible option for transferring from large transformers. However, prompt tuning may not be the most expressive method for transfer learning, as its generation quality\n",
      "is often outperformed by adapter tuning or full fine-tuning,\n",
      "\n",
      "\n",
      "After validation on VTAB, we examine few-shot trans\n",
      "fer learning, where the number of training images is further reduced. We focus on studying the transfer of the NAR\n",
      "transformer, i.e., MaskGIT [7], and provide more comparisons to existing few-shot image generation models, either\n",
      "with [60, 71] or without [63, 84] knowledge transfer.\n",
      "**Dataset.** We study few-shot generative transfer learn\n",
      "ing on three broadly-used benchmarks: Places [85], ImageNet [10], and Animal Face [61]. Following [60, 71], for\n",
      "Places and ImageNet, we select 5 classes[3] and use 500 images per class for training. For Animal Face, we consider\n",
      "two scenarios ‚Äì following [60], we use 100 images per class\n",
      "for training from 20 classes (denoted as ‚ÄúAnimal Face‚Äù in\n",
      "Tab. 3); alternatively, following [63, 84], we use all images\n",
      "of dog (389) and cat (160) classes (denoted as ‚Äúdog face‚Äù\n",
      "and ‚Äúcat face‚Äù in Tab. 3) for training.\n",
      "\n",
      "Moreover, we test on three challenging off-manifold do\n",
      "mains, i.e. DomainNet Infograph, Clipart (345 classes) [49],\n",
      "and ImageNet sketch (1000 classes) [70] where only two\n",
      "training images per class are used for transfer.\n",
      "**Setting. We study the class-and-instance conditional gen-**\n",
      "erative transfer as in Sec. 3.2 that is particularly suitable for\n",
      "few-shot transfer scenarios\n",
      "**Baselines. In addition to the transfer learning baselines,**\n",
      "_i.e., MineGAN [71] and cGANTransfer [60], we compare to_\n",
      "competitive models specially design for few-shot learning,\n",
      "_e.g., DiffAug [84] and LeCam GAN [63]._\n",
      "**Evaluation. We report FIDs using 10k generated images,**\n",
      "except for experiments on dog and cat faces, where we generate 5k images following [84]. For Places, ImageNet, and\n",
      "Animal Face, we use the entire training data (i.e., 2500 for\n",
      "Places and ImageNet, 2000 for Animal Face, 389 and 160\n",
      "for dog and cat faces, respectively) for the reference distribution. We sample 10k images for the reference distribution\n",
      "to compute FID for DomainNet and ImageNet sketch.\n",
      "\n",
      "3Cock, Tape player, Broccoli, Fire engine, Harvester for ImageNet, and\n",
      "\n",
      "Alley, Arch, Art gallery, Auditorium, Ballroom for Places.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "Numbers with ‚Ä†, ‚Ä°, ‚á§ are from [71], [60], [63], respectively.\n",
      "\n",
      "(a) DomainNet Clipart (2 shot; FID=22.4) (b) DomainNet Infograph (2 shot; FID=20.6) (c) ImageNet Sketch (2 shot; FID=14.4)\n",
      "\n",
      "Figure 6. Class conditional generation of few-shot transfer models. Images in red boxes are two training images of each class.\n",
      "\n",
      "Dataset ImageNet Places Animal Face Dog Face Cat Face\n",
      "\n",
      "(shot) (500) (500) (100) (389) (160)\n",
      "\n",
      "MineGAN [71] 61.8[‚Ä†] 82.3[‚Ä†] ‚Äì 93.0[‚á§] 54.5[‚á§]\n",
      "\n",
      "cGANTransfer [60] ‚Äì 71.1[‚Ä°] 85.9[‚Ä°] ‚Äì ‚Äì\n",
      "\n",
      "DiffAug [84] ‚Äì ‚Äì ‚Äì 58.5[‚á§] 42.4[‚á§]\n",
      "\n",
      "LeCam GAN [63] ‚Äì ‚Äì ‚Äì 54.9[‚á§] 34.2[‚á§]\n",
      "\n",
      "Ours (class) **16.9** 24.2 16.3 65.4 40.2\n",
      "\n",
      "Ours (instance) 19.6 **19.5** **13.3** **26.0** **31.2**\n",
      "\n",
      "Table 3. FIDs of image generation models on few-shot benchmark.\n",
      "\n",
      "**Results. In Tab. 3, we report FIDs of our method using**\n",
      "prompts of S = 128. When conditioned on the class, our\n",
      "method improves FIDs upon existing generative transfer\n",
      "learning methods. When comparing with few-shot generation methods on dog and cat face datasets, our method with\n",
      "a class condition slightly under-performs, likely due to that\n",
      "dataset having one class. When conditioned on instances,\n",
      "our models outperform highly-competitive few-shot generation models such as DiffAug, cGANTransfe, and LeCam\n",
      "GAN. We provide visualizations in Appendix C.2.1.\n",
      "\n",
      "We visualize generated images conditioned on the class\n",
      "\n",
      "by our models in Fig. 6, which shows the two images used\n",
      "in transfer training for each class in red boxes. We observe\n",
      "reasonable generalization, achieved by two training images,\n",
      "to target domains that are visually distinct from the source\n",
      "ImageNet dataset.\n",
      "**Data Efficiency. We conduct experiments to investigate**\n",
      "data efficiency. We train models on 5, 10, 50, and 100 training images per class for ImageNet, Places, and Animal Face\n",
      "datasets. The same number of images is used for the reference set to make FIDs comparable across settings.\n",
      "\n",
      "Results are in Fig. 7. Our method shows superior data\n",
      "\n",
      "efficiency, achieving substantially lower FIDs with only 5\n",
      "training images per class, to MineGAN [71] or cGANTransfer [60] based on GANs trained with 20 or 100 times more\n",
      "images per class. We find that using long prompts is not favorable when the number of training images is too small as\n",
      "models start to overfit to the small train set. We discuss how\n",
      "the prompt length affects the adaptation-diversity trade-off\n",
      "in Appendix B.2. The above results substantiate the efficacy\n",
      "of our method on the few-shot image synthesis task.\n",
      "**Enhancing Generation Diversity via Prompt Engineer-**\n",
      "**ing. As in Sec. 3.3 and Figs. 4b and 4c, our model offers a**\n",
      "way to enhance generation diversity by composing prompts.\n",
      "\n",
      "\n",
      "(a) ImageNet (b) Places (c) Animal Face\n",
      "\n",
      "Figure 7. FIDs for models trained with varying numbers of images\n",
      "per class for class-conditional few-shot generative transfer.\n",
      "\n",
      "|Col1|Col2|# params|Small M|edium|Large|Natural|Stru|ct. Spec.|\n",
      "|---|---|---|---|---|---|---|---|---|\n",
      "|S=16|baseline F=1 F=4 F=16|1.81M 0.68M 0.95M 2.02M|18.6 18.6 18.6 18.5|34.6 36.1 35.5 35.0|89.1 89.5 88.4 86.8|23.8 25.2 24.4 24.3|50 51 51 50|.9 41.7 .9 41.5 .5 41.4 .8 40.4|\n",
      "|S=128|baseline F=1 F=4 F=16|10.4M 0.76M 1.30M 3.39M|18.2 18.5 18.1 17.9|30.8 30.6 31.5 30.8|86.4 88.9 88.0 86.5|22.0 22.5 23.3 22.6|46 47 48 47|.9 39.9 .1 40.5 .2 38.0 .4 37.7|\n",
      "\n",
      "\n",
      "\n",
      "Table 4. Ablation on prompt token generators for NAR transformers on VTAB. We report FIDs averaged by different categorizations of tasks.\n",
      "\n",
      "We report quantitative metrics to support our claim.\n",
      "\n",
      "We conduct experiments on the dog and cat faces dataset\n",
      "\n",
      "using marquee header prompts with different Tcuto‚Üµ values.\n",
      "For the fidelity metric, we compute the FID. To measure the\n",
      "diversity, we follow [46] and report the intra-cluster pairwise LPIPS distance, where we generate 5k samples and\n",
      "map them to one of the training images.[4]\n",
      "\n",
      "Results are in Fig. 8. Ideally, we expect a model with\n",
      "\n",
      "low FID and high intra-cluster LPIPS scores (yellow star at\n",
      "top-left corner). When generating samples using the classcondition prompt (red square), we generate diverse images,\n",
      "but with poorer fidelity. When conditioned on data in\n",
      "stances (green dot), the FID is improved but at the cost of reduced diversity. Instance to class Marquee header prompts\n",
      "(blue) control the generation diversity and fidelity. Moreover, instance to instance Marquee header prompts, which\n",
      "interpolate the prompts between two instances, shows an\n",
      "improved trade-off between fidelity and diversity.\n",
      "\n",
      "4We use a pixel-wise L2 distance for computation efficiency instead of\n",
      "\n",
      "LPIPS distance in [46].\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "Figure 9. Instance-conditioned generation with (top) S = 1 and\n",
      "(bottom) S = 128. Images in red are the conditioned instances.\n",
      "\n",
      "classification [1,29]), this paper proposes an effective visual\n",
      "prompt tuning approach for image synthesis.\n",
      "\n",
      "**Generative models have been extensively studied for im-**\n",
      "age synthesis, including variational autoencoder [34,64,66],\n",
      "diffusion [12, 57] and autoregressive [48, 65, 69] models. A\n",
      "large volume of progress has been made around the generative adversarial network (GAN) [20] thanks to its ability at synthesizing high-fidelity images [2, 31, 32, 59]. As\n",
      "such, generative knowledge transfer has been studied to\n",
      "transfer knowledge of pretrained GAN models. TransferGAN [72], following a usual practice of fine-tuning on the\n",
      "target dataset, has demonstrated that transferring knowledge\n",
      "from pretraining improves the performance when training\n",
      "with limited data. Freezing a few layers of the discriminator [44] further improves, while stabilizing the training\n",
      "process. MineGAN [71] introduces a miner, which projects\n",
      "random noise into the embedding space of the pretrained\n",
      "generator, and trains it with discriminator while fixing generator parameters. cGANTransfer [60] makes explicit transfer of knowledge on classes of the source dataset to new\n",
      "classes. Albeit showing improvement, these methods still\n",
      "require careful training (e.g., early stopping) and have evaluated on a few datasets. In our work, we extensively test\n",
      "methods on a wide variety of visual domains (e.g., VTAB)\n",
      "and show improvement by a large margin over existing\n",
      "GAN-based generative transfer methods.\n",
      "\n",
      "**6. Conclusion**\n",
      "\n",
      "We present a method for learning image generation mod\n",
      "els from diverse data distributions and varying amount of\n",
      "training data via knowledge transfer from the source model\n",
      "trained on a large dataset. A simple modification on prompt\n",
      "token designs allows to learn a parameter and compute efficient class and instance conditional image generation models of autoregressive and non-autoregressive vision transformers. We provide comprehensive experimental results\n",
      "of image synthesis across diverse visual domains, tasks, and\n",
      "the number of training images. In addition, we show how\n",
      "to apply learned prompts for novel image synthesis in the\n",
      "form of marquee header prompts using just a few images.\n",
      "\n",
      "**Acknowledgment. We thank Brian Lester for helpful dis-**\n",
      "cussion on prompt tuning, Boqing Gong and David Salesin\n",
      "for their feedback on the manuscript.\n",
      "\n",
      "\n",
      "(a) Dog Faces (b) Cat Faces\n",
      "\n",
      "Figure 8. Marquee header prompt shows clear tradeoff between\n",
      "fidelity (FID) and diversity (LPIPS) when interpolating from instance to class (blue). It shows a better tradeoff when interpolating\n",
      "between instances (orange), achieving low FID and high LPIPS.\n",
      "\n",
      "**4.3. Analysis and Discussion**\n",
      "\n",
      "**Parameter-efficiency. Tab. 4 provides results of using dif-**\n",
      "ferent prompt token generators, where the baseline indicates\n",
      "the non-factorized prompt tuning method. As shown, FIDs\n",
      "of prompt tuning with the proposed factorization reasonably\n",
      "match those of the baseline. while achieving comparable or\n",
      "better FIDs than the baseline using 70% fewer parameters.\n",
      "**Adaptation-Diversity Trade-Off. We study the instance-**\n",
      "conditioned prompts with various lengths. Fig. 9 shows\n",
      "\n",
      "the generated images with S = 1 (top) and S = 128 (bottom) where more results can be found in Appendix. With\n",
      "a longer prompt, the synthesized images follow, more faithfully, the conditioned image, but seem less diverse. With\n",
      "a short prompt, on the other hand, the model still captures\n",
      "more dominant characters of the conditioned image (e.g.,\n",
      "color, class), but lacking fine details. The results suggest\n",
      "that the adaptation and diversity could be controlled with\n",
      "the prompt length.\n",
      "\n",
      "**5. Related Work**\n",
      "\n",
      "**Transfer learning [47,62,74,87] improves the performance**\n",
      "of downstream tasks using knowledge from the source domain. It is particularly effective when the amount of training data is limited for downstream tasks. Knowledge transfer of deep neural networks has been realized in various\n",
      "forms, such as linear probing [9, 26], side-tuning [82], biastuning [4, 80], fine-tuning [35, 51], or adapter [28, 55, 56].\n",
      "Recently, prompt tuning [38, 40‚Äì42] has emerged as a powerful tool for transfer learning of transformer-based large\n",
      "language models in NLP. It has also been applied to visionlanguage models [16, 30, 50, 77, 86] that are limited to the\n",
      "input of text encoders. Since the introduction of Vision\n",
      "\n",
      "Transformer [14], prompt tuning has been studied for vision tasks where the pre-trained model is an image encoder [1, 29]. While previous works have shown the effectiveness of prompt tuning for discriminative tasks (e.g.,\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "**References**\n",
      "\n",
      "[1] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and\n",
      "\n",
      "Phillip Isola. Exploring visual prompts for adapting largescale models. arXiv preprint arXiv:2203.17274, 2022. 2, 3,\n",
      "8\n",
      "\n",
      "[2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\n",
      "\n",
      "scale gan training for high fidelity natural image synthesis.\n",
      "In International Conference on Learning Representations,\n",
      "2018. 1, 5, 8\n",
      "\n",
      "[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub\n",
      "biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural In_formation Processing Systems, 33:1877‚Äì1901, 2020. 3_\n",
      "\n",
      "[4] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl:\n",
      "\n",
      "Reduce memory, not parameters for efficient on-device\n",
      "learning. Advances in Neural Information Processing Sys_tems, 33:11285‚Äì11297, 2020. 8_\n",
      "\n",
      "[5] Arantxa Casanova, Marlene Careil, Jakob Verbeek, Michal\n",
      "\n",
      "Drozdzal, and Adriana Romero Soriano. Instance\n",
      "conditioned GAN. Advances in Neural Information Process_ing Systems, 34:27517‚Äì27529, 2021. 4_\n",
      "\n",
      "[6] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\n",
      "\n",
      "Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy,\n",
      "William T Freeman, Michael Rubinstein, Yuanzhen Li, and\n",
      "Dilip Krishnan. Muse: Text-to-image generation via masked\n",
      "generative transformers. arXiv preprint arXiv:2301.00704,\n",
      "2023. 2\n",
      "\n",
      "[7] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T\n",
      "\n",
      "Freeman. MaskGIT: Masked generative image transformer.\n",
      "In Proceedings of the IEEE/CVF Conference on Computer\n",
      "_Vision and Pattern Recognition, pages 11315‚Äì11325, 2022._\n",
      "1, 2, 3, 4, 5, 6, 15\n",
      "\n",
      "[8] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee\n",
      "woo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International Conference on Ma_chine Learning, pages 1691‚Äì1703. PMLR, 2020. 2_\n",
      "\n",
      "[9] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank\n",
      "\n",
      "Wang, and Jia-Bin Huang. A closer look at few-shot classification. In International Conference on Learning Represen_tations, 2018. 8_\n",
      "\n",
      "[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\n",
      "\n",
      "and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision\n",
      "_and Pattern Recognition, pages 248‚Äì255. Ieee, 2009. 6_\n",
      "\n",
      "[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\n",
      "\n",
      "Toutanova. Bert: Pre-training of deep bidirectional\n",
      "\n",
      "transformers for language understanding. _arXiv preprint_\n",
      "\n",
      "_arXiv:1810.04805, 2018. 3_\n",
      "\n",
      "[12] Prafulla Dhariwal and Alexander Nichol. Diffusion models\n",
      "\n",
      "beat GANs on image synthesis. Advances in Neural Infor_mation Processing Systems, 34, 2021. 1, 8_\n",
      "\n",
      "[13] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,\n",
      "\n",
      "Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,\n",
      "Hongxia Yang, et al. CogView: Mastering text-to-image\n",
      "\n",
      "generation via transformers. Advances in Neural Informa_tion Processing Systems, 34:19822‚Äì19835, 2021. 1, 2_\n",
      "\n",
      "\n",
      "\n",
      "[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\n",
      "\n",
      "Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\n",
      "Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint_\n",
      "\n",
      "_arXiv:2010.11929, 2020. 8_\n",
      "\n",
      "[15] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\n",
      "\n",
      "transformers for high-resolution image synthesis. In Pro\n",
      "_ceedings of the IEEE/CVF Conference on Computer Vision_\n",
      "_and Pattern Recognition, pages 12873‚Äì12883, 2021. 1, 2, 5_\n",
      "\n",
      "[16] Chunjiang Ge, Rui Huang, Mixue Xie, Zihang Lai, Shiji\n",
      "\n",
      "Song, Shuang Li, and Gao Huang. Domain adaptation via\n",
      "prompt learning. arXiv preprint arXiv:2202.06687, 2022. 8\n",
      "\n",
      "[17] Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke\n",
      "\n",
      "Zettlemoyer. Mask-predict: Parallel decoding of conditional\n",
      "masked language models. In EMNLP-IJCNLP, 2019. 3\n",
      "\n",
      "[18] Ross Girshick. Fast R-CNN. In Proceedings of the IEEE\n",
      "\n",
      "_International Conference on Computer Vision, pages 1440‚Äì_\n",
      "1448, 2015. 1\n",
      "\n",
      "[19] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\n",
      "\n",
      "Malik. Rich feature hierarchies for accurate object detection\n",
      "and semantic segmentation. In Proceedings of the IEEE Con_ference on Computer Vision and Pattern Recognition, pages_\n",
      "580‚Äì587, 2014. 1\n",
      "\n",
      "[20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\n",
      "\n",
      "Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\n",
      "Yoshua Bengio. Generative adversarial nets. Advances in\n",
      "_Neural Information Processing Systems, 27, 2014. 1, 8_\n",
      "\n",
      "[21] Jiatao Gu and Xiang Kong. Fully non-autoregressive neural\n",
      "\n",
      "machine translation: Tricks of the trade. In Findings of ACL_IJCNLP, 2021. 3_\n",
      "\n",
      "[22] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg\n",
      "Kirkpatrick, and Graham Neubig. Towards a unified view\n",
      "of parameter-efficient transfer learning. _arXiv preprint_\n",
      "\n",
      "_arXiv:2110.04366, 2021. 15_\n",
      "\n",
      "[23] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\n",
      "\n",
      "Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Con_ference on Computer Vision and Pattern Recognition, pages_\n",
      "9729‚Äì9738, 2020. 1\n",
      "\n",
      "[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n",
      "\n",
      "Deep residual learning for image recognition. In Proceed_ings of the IEEE Conference on Computer Vision and Pattern_\n",
      "_Recognition, pages 770‚Äì778, 2016. 1, 13_\n",
      "\n",
      "[25] Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Rit\n",
      "ter, Bertrand Rondepierre, Andreas Steiner, and Marc van\n",
      "Zee. Flax: A neural network library and ecosystem for JAX,\n",
      "2020. 13, 16\n",
      "\n",
      "[26] Olivier Henaff. Data-efficient image recognition with con\n",
      "trastive predictive coding. In International Conference on\n",
      "_Machine Learning, pages 4182‚Äì4192. PMLR, 2020. 8_\n",
      "\n",
      "[27] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\n",
      "\n",
      "Bernhard Nessler, and Sepp Hochreiter. GANs trained by\n",
      "a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems,\n",
      "30, 2017. 5\n",
      "\n",
      "[28] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna\n",
      "\n",
      "Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "Attariyan, and Sylvain Gelly. Parameter-efficient transfer\n",
      "\n",
      "learning for NLP. In International Conference on Machine\n",
      "_Learning, pages 2790‚Äì2799. PMLR, 2019. 4, 6, 8, 15, 16_\n",
      "\n",
      "[29] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,\n",
      "\n",
      "Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. arXiv preprint arXiv:2203.12119, 2022.\n",
      "2, 3, 4, 8\n",
      "\n",
      "[30] Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi\n",
      "\n",
      "Xie. Prompting visual-language models for efficient video\n",
      "understanding. arXiv preprint arXiv:2112.04478, 2021. 8\n",
      "\n",
      "[31] Tero Karras, Samuli Laine, and Timo Aila. A style-based\n",
      "\n",
      "generator architecture for generative adversarial networks.\n",
      "In Proceedings of the IEEE/CVF Conference on Computer\n",
      "_Vision and Pattern Recognition, pages 4401‚Äì4410, 2019. 8_\n",
      "\n",
      "[32] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\n",
      "\n",
      "Jaakko Lehtinen, and Timo Aila. Analyzing and improv\n",
      "ing the image quality of stylegan. In Proceedings of\n",
      "\n",
      "_the IEEE/CVF conference on computer vision and pattern_\n",
      "_recognition, pages 8110‚Äì8119, 2020. 8_\n",
      "\n",
      "[33] Diederik P Kingma and Jimmy Ba. Adam: A method for\n",
      "\n",
      "stochastic optimization. _arXiv preprint arXiv:1412.6980,_\n",
      "\n",
      "2014. 16\n",
      "\n",
      "[34] Diederik P Kingma and Max Welling. Auto-encoding varia\n",
      "tional bayes. arXiv preprint arXiv:1312.6114, 2013. 8\n",
      "\n",
      "[35] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan\n",
      "\n",
      "Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\n",
      "Big transfer (bit): General visual representation learning. In\n",
      "_European conference on computer vision, pages 491‚Äì507._\n",
      "Springer, 2020. 1, 2, 4, 8, 16\n",
      "\n",
      "[36] Xiang Kong, Lu Jiang, Huiwen Chang, Han Zhang, Yuan\n",
      "\n",
      "Hao, Haifeng Gong, and Irfan Essa. Blt: Bidirectional layout transformer for controllable layout generation. _arXiv_\n",
      "\n",
      "_preprint arXiv:2112.05112, 2021. 3_\n",
      "\n",
      "[37] Xiang Kong, Zhisong Zhang, and Eduard Hovy. Incorpo\n",
      "rating a local translation mechanism into non-autoregressive\n",
      "translation. arXiv preprint arXiv:2011.06132, 2020. 3\n",
      "\n",
      "[38] Brian Lester, Rami Al-Rfou, and Noah Constant. The power\n",
      "\n",
      "of scale for parameter-efficient prompt tuning. In Proceed_ings of the 2021 Conference on Empirical Methods in Nat-_\n",
      "_ural Language Processing, pages 3045‚Äì3059, 2021. 2, 3,_\n",
      "8\n",
      "\n",
      "[39] Jose Lezama, Huiwen Chang, Lu Jiang, and Irfan Essa. Im\n",
      "proved masked image generation with token-critic. In ECCV,\n",
      "2022. 3\n",
      "\n",
      "[40] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimiz\n",
      "ing continuous prompts for generation. _arXiv preprint_\n",
      "\n",
      "_arXiv:2101.00190, 2021. 2, 3, 8_\n",
      "\n",
      "[41] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi\n",
      "roaki Hayashi, and Graham Neubig. Pre-train, prompt, and\n",
      "predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586,\n",
      "2021. 8\n",
      "\n",
      "[42] Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin\n",
      "\n",
      "Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.\n",
      "_arXiv preprint arXiv:2110.07602, 2021. 8_\n",
      "\n",
      "\n",
      "\n",
      "[43] Ilya Loshchilov and Frank Hutter. Sgdr: Stochas\n",
      "tic gradient descent with warm restarts. _arXiv preprint_\n",
      "\n",
      "_arXiv:1608.03983, 2016. 16_\n",
      "\n",
      "[44] Sangwoo Mo, Minsu Cho, and Jinwoo Shin. Freeze the dis\n",
      "criminator: a simple baseline for fine-tuning gans. _arXiv_\n",
      "\n",
      "_preprint arXiv:2002.10964, 2020. 8_\n",
      "\n",
      "[45] Xing Nie, Bolin Ni, Jianlong Chang, Gaomeng Meng, Chun\n",
      "lei Huo, Zhaoxiang Zhang, Shiming Xiang, Qi Tian, and\n",
      "Chunhong Pan. Pro-tuning: Unified prompt tuning for vision tasks. arXiv preprint arXiv:2207.14381, 2022. 3\n",
      "\n",
      "[46] Utkarsh Ojha, Yijun Li, Jingwan Lu, Alexei A Efros,\n",
      "\n",
      "Yong Jae Lee, Eli Shechtman, and Richard Zhang. Few-shot\n",
      "image generation via cross-domain correspondence. In Pro_ceedings of the IEEE/CVF Conference on Computer Vision_\n",
      "_and Pattern Recognition, pages 10743‚Äì10752, 2021. 1, 7_\n",
      "\n",
      "[47] Sinno Jialin Pan and Qiang Yang. A survey on transfer learn\n",
      "ing. IEEE Transactions on knowledge and data engineering,\n",
      "22(10):1345‚Äì1359, 2009. 8\n",
      "\n",
      "[48] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\n",
      "\n",
      "Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International conference on machine\n",
      "\n",
      "_learning, pages 4055‚Äì4064. PMLR, 2018. 8_\n",
      "\n",
      "[49] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate\n",
      "\n",
      "Saenko, and Bo Wang. Moment matching for multi-source\n",
      "domain adaptation. In Proceedings of the IEEE/CVF inter_national conference on computer vision, pages 1406‚Äì1415,_\n",
      "2019. 6\n",
      "\n",
      "[50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\n",
      "\n",
      "Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\n",
      "Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning,\n",
      "pages 8748‚Äì8763, 2021. 8\n",
      "\n",
      "[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\n",
      "\n",
      "Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\n",
      "Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint\n",
      "_arXiv:1910.10683, 2019. 8_\n",
      "\n",
      "[52] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\n",
      "\n",
      "and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125,\n",
      "2022. 3\n",
      "\n",
      "[53] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\n",
      "\n",
      "Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\n",
      "Zero-shot text-to-image generation. In Marina Meila and\n",
      "\n",
      "Tong Zhang, editors, ICML, 2021. 1, 2\n",
      "\n",
      "[54] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gener\n",
      "ating diverse high-fidelity images with vq-vae-2. Advances\n",
      "_in neural information processing systems, 32, 2019. 2_\n",
      "\n",
      "[55] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.\n",
      "\n",
      "Learning multiple visual domains with residual adapters. Ad_vances in neural information processing systems, 30, 2017._\n",
      "8\n",
      "\n",
      "[56] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.\n",
      "\n",
      "Efficient parametrization of multi-domain deep neural networks. In Proceedings of the IEEE Conference on Computer\n",
      "_Vision and Pattern Recognition, pages 8119‚Äì8127, 2018. 8_\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "[57] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\n",
      "\n",
      "Patrick Esser, and Bj¬®orn Ommer. High-resolution image\n",
      "\n",
      "synthesis with latent diffusion models. In Proceedings of\n",
      "\n",
      "_the IEEE/CVF Conference on Computer Vision and Pattern_\n",
      "_Recognition, pages 10684‚Äì10695, 2022. 2, 3, 8_\n",
      "\n",
      "[58] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\n",
      "\n",
      "Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed\n",
      "Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi,\n",
      "\n",
      "Rapha Gontijo Lopes, et al. Photorealistic text-to-image\n",
      "\n",
      "diffusion models with deep language understanding. arXiv\n",
      "_preprint arXiv:2205.11487, 2022. 2, 3_\n",
      "\n",
      "[59] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan\n",
      "xl: Scaling stylegan to large diverse datasets. arXiv preprint\n",
      "_arXiv:2202.00273, 1, 2022. 8_\n",
      "\n",
      "[60] Mohamad Shahbazi, Zhiwu Huang, Danda Pani Paudel,\n",
      "\n",
      "Ajad Chhatkuli, and Luc Van Gool. Efficient conditional gan\n",
      "transfer with knowledge propagation across classes. In Pro_ceedings of the IEEE/CVF Conference on Computer Vision_\n",
      "_and Pattern Recognition, pages 12167‚Äì12176, 2021. 1, 2, 5,_\n",
      "6, 7, 8, 16\n",
      "\n",
      "[61] Zhangzhang Si and Song-Chun Zhu. Learning hybrid image\n",
      "\n",
      "templates (hit) by information projection. IEEE Transactions\n",
      "_on pattern analysis and machine intelligence, 34(7):1354‚Äì_\n",
      "1367, 2011. 2, 6\n",
      "\n",
      "[62] Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang,\n",
      "\n",
      "Chao Yang, and Chunfang Liu. A survey on deep transfer\n",
      "learning. In International conference on artificial neural net_works, pages 270‚Äì279. Springer, 2018. 8_\n",
      "\n",
      "[63] Hung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, and\n",
      "\n",
      "Weilong Yang. Regularizing generative adversarial networks\n",
      "under limited data. In Proceedings of the IEEE/CVF Con_ference on Computer Vision and Pattern Recognition, pages_\n",
      "7921‚Äì7931, 2021. 6, 7\n",
      "\n",
      "[64] Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical vari\n",
      "ational autoencoder. Advances in Neural Information Pro_cessing Systems, 33:19667‚Äì19679, 2020. 8_\n",
      "\n",
      "[65] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt,\n",
      "\n",
      "Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. Advances in neural information\n",
      "_processing systems, 29, 2016. 8_\n",
      "\n",
      "[66] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\n",
      "\n",
      "representation learning. Advances in neural information pro_cessing systems, 30, 2017. 8_\n",
      "\n",
      "[67] A¬®aron van den Oord, Oriol Vinyals, and Koray\n",
      "\n",
      "Kavukcuoglu. Neural discrete representation learning.\n",
      "\n",
      "In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,\n",
      "Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and\n",
      "Roman Garnett, editors, NeurIPS, 2017. 1, 2\n",
      "\n",
      "[68] Laurens Van der Maaten and Geoffrey Hinton. Visualiz\n",
      "ing data using t-sne. Journal of machine learning research,\n",
      "9(11), 2008. 13\n",
      "\n",
      "[69] Aaron Van Oord, Nal Kalchbrenner, and Koray\n",
      "\n",
      "Kavukcuoglu. Pixel recurrent neural networks. In\n",
      "\n",
      "_International_ _conference_ _on_ _machine_ _learning,_ pages\n",
      "\n",
      "1747‚Äì1756. PMLR, 2016. 1, 8\n",
      "\n",
      "[70] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P\n",
      "\n",
      "Xing. Learning robust global representations by penalizing\n",
      "\n",
      "\n",
      "local predictive power. Advances in Neural Information Pro_cessing Systems, 32, 2019. 6_\n",
      "\n",
      "[71] Yaxing Wang, Abel Gonzalez-Garcia, David Berga, Luis\n",
      "\n",
      "Herranz, Fahad Shahbaz Khan, and Joost van de Weijer.\n",
      "Minegan: effective knowledge transfer from gans to target\n",
      "domains with few images. In Proceedings of the IEEE/CVF\n",
      "_Conference on Computer Vision and Pattern Recognition,_\n",
      "pages 9332‚Äì9341, 2020. 1, 2, 5, 6, 7, 8, 16\n",
      "\n",
      "[72] Yaxing Wang, Chenshen Wu, Luis Herranz, Joost van de\n",
      "\n",
      "Weijer, Abel Gonzalez-Garcia, and Bogdan Raducanu.\n",
      "Transferring gans: generating images from limited data. In\n",
      "_Proceedings of the European Conference on Computer Vi-_\n",
      "_sion (ECCV), pages 218‚Äì234, 2018. 8_\n",
      "\n",
      "[73] Ryan Webster, Julien Rabin, Lo¬®ƒ±c Simon, and Fr¬¥ed¬¥eric Jurie.\n",
      "\n",
      "Detecting overfitting of deep generative networks via latent\n",
      "recovery. In CVPR, 2019. 1\n",
      "\n",
      "[74] Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A\n",
      "\n",
      "survey of transfer learning. Journal of Big data, 3(1):1‚Äì40,\n",
      "2016. 8\n",
      "\n",
      "[75] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang,\n",
      "\n",
      "Daxin Jiang, and Nan Duan. N\\‚Äù uwa: Visual synthesis\n",
      "\n",
      "pre-training for neural visual world creation. arXiv preprint\n",
      "_arXiv:2111.12417, 2021. 1, 2_\n",
      "\n",
      "[76] Ceyuan Yang, Yujun Shen, Zhiyi Zhang, Yinghao Xu, Jia\n",
      "peng Zhu, Zhirong Wu, and Bolei Zhou. One-shot generative\n",
      "domain adaptation. arXiv preprint arXiv:2111.09876, 2021.\n",
      "1\n",
      "\n",
      "[77] Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat\n",
      "Seng Chua, and Maosong Sun. Cpt: Colorful prompt tuning for pre-trained vision-language models. arXiv preprint\n",
      "_arXiv:2109.11797, 2021. 8_\n",
      "\n",
      "[78] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,\n",
      "\n",
      "James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,\n",
      "and Yonghui Wu. Vector-quantized image modeling with\n",
      "\n",
      "improved VQGAN. arXiv preprint arXiv:2110.04627, 2021.\n",
      "2\n",
      "\n",
      "[79] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun\n",
      "jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregres\n",
      "sive models for content-rich text-to-image generation. arXiv\n",
      "_preprint arXiv:2206.10789, 2022. 1, 2_\n",
      "\n",
      "[80] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit:\n",
      "\n",
      "Simple parameter-efficient fine-tuning for transformer-based\n",
      "masked language-models. arXiv preprint arXiv:2106.10199,\n",
      "2021. 8\n",
      "\n",
      "[81] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov,\n",
      "\n",
      "Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A large-scale study of representation learning\n",
      "with the visual task adaptation benchmark. arXiv preprint\n",
      "_arXiv:1910.04867, 2019. 2, 5_\n",
      "\n",
      "[82] Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas\n",
      "\n",
      "Guibas, and Jitendra Malik. Side-tuning: a baseline for network adaptation via additive side networks. In European\n",
      "\n",
      "_Conference on Computer Vision, pages 698‚Äì714. Springer,_\n",
      "2020. 8\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "[83] Zhu Zhang, Jianxin Ma, Chang Zhou, Rui Men, Zhikang Li,\n",
      "\n",
      "Ming Ding, Jie Tang, Jingren Zhou, and Hongxia Yang. M6ufc: Unifying multi-modal controls for conditional image\n",
      "synthesis. arXiv preprint arXiv:2105.14211, 2021. 3\n",
      "\n",
      "[84] Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song\n",
      "\n",
      "Han. Differentiable augmentation for data-efficient gan\n",
      "\n",
      "training. Advances in Neural Information Processing Sys_tems, 33:7559‚Äì7570, 2020. 6, 7_\n",
      "\n",
      "[85] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Tor\n",
      "ralba, and Aude Oliva. Learning deep features for scene\n",
      "\n",
      "recognition using places database. Advances in neural in_formation processing systems, 27, 2014. 2, 6_\n",
      "\n",
      "[86] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\n",
      "\n",
      "Liu. Learning to prompt for vision-language models. arXiv\n",
      "_preprint arXiv:2109.01134, 2021. 8_\n",
      "\n",
      "[87] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi,\n",
      "\n",
      "Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A\n",
      "comprehensive survey on transfer learning. Proceedings of\n",
      "_the IEEE, 109(1):43‚Äì76, 2020. 8_\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      " \n",
      " Include key points, figures, and facts from the text.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92msummarizer\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "The provided text is a compilation of preprint arXiv articles that explore various aspects of computer vision and language models. These articles cover topics such as:\n",
      "\n",
      "* Prompt tuning for pre-trained vision-language models (Yuan Yao et al.)\n",
      "* Vector-quantized image modeling with improved VQGAN (Jiahui Yu et al.)\n",
      "* Scaling autoregressive models for content-rich text-to-image generation (Jiahui Yu et al.)\n",
      "* Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language models (Elad Ben Zaken et al.)\n",
      "* A large-scale study of representation learning with the visual task adaptation benchmark (Xiaohua Zhai et al.)\n",
      "* Side-tuning: a baseline for network adaptation via additive side networks (Jeffrey O Zhang et al.)\n",
      "* M6ufc: Unifying multi-modal controls for conditional image synthesis (Zhu Zhang et al.)\n",
      "* Differentiable augmentation for data-efficient GAN training (Shengyu Zhao et al.)\n",
      "* Learning deep features for scene recognition using Places database (Bolei Zhou et al.)\n",
      "* Learning to prompt for vision-language models (Kaiyang Zhou et al.)\n",
      "* A comprehensive survey on transfer learning (Fuzhen Zhuang et al.)\n",
      "\n",
      "These articles demonstrate the advancements and challenges in computer vision and language models, highlighting the importance of efficient fine-tuning, representation learning, and multi-modal controls.\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owen/anaconda3/envs/gen-ai/lib/python3.12/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m Failed to convert text into JSON, error: 1 validation error for Summary\n",
      "key_points\n",
      "  Input should be a valid array [type=list_type, input_value='[\"Prompt tuning for pre-... on transfer learning\"]', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.8/v/list_type. Using raw output instead.\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "crew_output = crew.kickoff(inputs={\"text\": pdf_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Language Models suck at summarizing entire text at once even though it fits into context window\n",
    "\n",
    "\n",
    "* Probably need to chunk the document and build summaries\n",
    "https://python.langchain.com/docs/tutorials/summarization/#map-reduce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_llm = OllamaLLM(model=\"llama3.2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "map_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")]\n",
    ")\n",
    "\n",
    "map_chain = map_prompt | lc_llm | StrOutputParser()\n",
    "\n",
    "reduce_template = \"\"\"\n",
    "The following is a set of summaries:\n",
    "{docs}\n",
    "Take these and distill it into a final, consolidated summary\n",
    "of the main themes.\n",
    "\"\"\"\n",
    "\n",
    "reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])\n",
    "\n",
    "reduce_chain = reduce_prompt | lc_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 9 documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=2000, chunk_overlap=100\n",
    ")\n",
    "split_docs = text_splitter.split_documents([Document(page_content=pdf_text)])\n",
    "print(f\"Generated {len(split_docs)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Literal, TypedDict\n",
    "\n",
    "from langchain.chains.combine_documents.reduce import (\n",
    "    acollapse_docs,\n",
    "    split_list_of_docs,\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "token_max = 1000\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "def num_tokens_from_string(string: str, encoding_name=\"cl100k_base\" ) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def length_function(documents: List[Document]) -> int:\n",
    "    \"\"\"Get number of tokens for input contents.\"\"\"\n",
    "    return sum(num_tokens_from_string(doc.page_content) for doc in documents)\n",
    "\n",
    "\n",
    "# This will be the overall state of the main graph.\n",
    "# It will contain the input document contents, corresponding\n",
    "# summaries, and a final summary.\n",
    "class OverallState(TypedDict):\n",
    "    # Notice here we use the operator.add\n",
    "    # This is because we want combine all the summaries we generate\n",
    "    # from individual nodes back into one list - this is essentially\n",
    "    # the \"reduce\" part\n",
    "    contents: List[str]\n",
    "    summaries: Annotated[list, operator.add]\n",
    "    collapsed_summaries: List[Document]\n",
    "    final_summary: str\n",
    "\n",
    "\n",
    "# This will be the state of the node that we will \"map\" all\n",
    "# documents to in order to generate summaries\n",
    "class SummaryState(TypedDict):\n",
    "    content: str\n",
    "\n",
    "\n",
    "# Here we generate a summary, given a document\n",
    "async def generate_summary(state: SummaryState):\n",
    "    response = await map_chain.ainvoke(state[\"content\"])\n",
    "    return {\"summaries\": [response]}\n",
    "\n",
    "\n",
    "# Here we define the logic to map out over the documents\n",
    "# We will use this an edge in the graph\n",
    "def map_summaries(state: OverallState):\n",
    "    # We will return a list of `Send` objects\n",
    "    # Each `Send` object consists of the name of a node in the graph\n",
    "    # as well as the state to send to that node\n",
    "    return [\n",
    "        Send(\"generate_summary\", {\"content\": content}) for content in state[\"contents\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "def collect_summaries(state: OverallState):\n",
    "    return {\n",
    "        \"collapsed_summaries\": [Document(summary) for summary in state[\"summaries\"]]\n",
    "    }\n",
    "\n",
    "\n",
    "# Add node to collapse summaries\n",
    "async def collapse_summaries(state: OverallState):\n",
    "    doc_lists = split_list_of_docs(\n",
    "        state[\"collapsed_summaries\"], length_function, token_max\n",
    "    )\n",
    "    results = []\n",
    "    for doc_list in doc_lists:\n",
    "        results.append(await acollapse_docs(doc_list, reduce_chain.ainvoke))\n",
    "\n",
    "    return {\"collapsed_summaries\": results}\n",
    "\n",
    "\n",
    "# This represents a conditional edge in the graph that determines\n",
    "# if we should collapse the summaries or not\n",
    "def should_collapse(\n",
    "    state: OverallState,\n",
    ") -> Literal[\"collapse_summaries\", \"generate_final_summary\"]:\n",
    "    num_tokens = length_function(state[\"collapsed_summaries\"])\n",
    "    if num_tokens > token_max:\n",
    "        return \"collapse_summaries\"\n",
    "    else:\n",
    "        return \"generate_final_summary\"\n",
    "\n",
    "\n",
    "# Here we will generate the final summary\n",
    "async def generate_final_summary(state: OverallState):\n",
    "    response = await reduce_chain.ainvoke(state[\"collapsed_summaries\"])\n",
    "    return {\"final_summary\": response}\n",
    "\n",
    "\n",
    "# Construct the graph\n",
    "# Nodes:\n",
    "graph = StateGraph(OverallState)\n",
    "graph.add_node(\"generate_summary\", generate_summary)  # same as before\n",
    "graph.add_node(\"collect_summaries\", collect_summaries)\n",
    "graph.add_node(\"collapse_summaries\", collapse_summaries)\n",
    "graph.add_node(\"generate_final_summary\", generate_final_summary)\n",
    "\n",
    "# Edges:\n",
    "graph.add_conditional_edges(START, map_summaries, [\"generate_summary\"])\n",
    "graph.add_edge(\"generate_summary\", \"collect_summaries\")\n",
    "graph.add_conditional_edges(\"collect_summaries\", should_collapse)\n",
    "graph.add_conditional_edges(\"collapse_summaries\", should_collapse)\n",
    "graph.add_edge(\"generate_final_summary\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['collect_summaries']\n",
      "['collapse_summaries']\n",
      "['generate_final_summary']\n"
     ]
    }
   ],
   "source": [
    "async for step in app.astream(\n",
    "    {\"contents\": [doc.page_content for doc in split_docs]},\n",
    "    {\"recursion_limit\": 10},\n",
    "):\n",
    "    print(list(step.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['generate_final_summary'])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a consolidated summary of the main themes:\n",
      "\n",
      "**Advances in Generative Models for Image Synthesis**\n",
      "\n",
      "* Research papers focus on advancing Generative Adversarial Networks (GANs) and other generative models for image synthesis using autoregressive vision transformers, diffusion models, transformers, and self-supervised learning.\n",
      "* New approaches include visual prompt tuning, which involves prepending learnable tokens to guide the model to generate images in the target distribution.\n",
      "\n",
      "**Importance of Prompt Tuning and Transfer Learning**\n",
      "\n",
      "* The importance of prompt tuning is highlighted, with various techniques and approaches reviewed, including diffusion models, transformers, and self-supervised learning.\n",
      "* Prompt learning can improve performance on image synthesis tasks, while diffusion models have surpassed GANs in image synthesis quality.\n",
      "* Transfer learning and fine-tuning are crucial for adapting models to new tasks and domains.\n",
      "\n",
      "**Methods for Efficient Image Synthesis**\n",
      "\n",
      "* The proposed framework consists of two stages: prompt generation and iterative decoding.\n",
      "* Autoregressive (AR) and non-autoregressive (NAR) decoding methods are used, with the latter being more efficient.\n",
      "\n",
      "**Key Findings and Applications**\n",
      "\n",
      "* Effective visual prompt tuning approaches can improve image synthesis performance.\n",
      "* Autoregressive vision transformers demonstrate promise in few-shot image synthesis using knowledge transfer from source models.\n",
      "* Comprehensive experimental results showcase the potential applications of these advancements in computer vision, image generation, and language understanding.\n",
      "\n",
      "Overall, the research papers presented highlight significant progress in developing more efficient and effective generative models for image synthesis, as well as the importance of prompt tuning and transfer learning in optimizing model performance.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(step.get('generate_final_summary').get('final_summary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows it's better for small language models to see less data to generate better responses than to stuff everything into it. LangGraph can be integrated with CrewAI as well. Or can leverage crewAI Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other option is providing tools to crewAI and having a 'manager' decide what tools to call. May be tricky with small language models. Need to do feedback well\n",
    "* Manager = hierarchical Process = More like planner and executor type\n",
    "* output of one task is automatically passed to the next task\n",
    "* Kickoff Crew for Each -> May be more similar to creating tasks for each component within a piece of text: https://docs.crewai.com/how-to/kickoff-for-each\n",
    "* Langtrace for monitoring - OS https://app.langtrace.ai/onboarding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
