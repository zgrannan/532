{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys \n",
    "sys.path.append('../')\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import requests\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import json \n",
    "\n",
    "import pandas as pd \n",
    "from pipeline.helpers import list_of_dicts_to_dict_of_lists, upload_to_hf \n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data generated from NOV2 and upload cleaned set to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ID=\"CPSC532/arxiv_qa_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st run with 1 pdf\n",
    "dataset_1 = load_dataset(\n",
    "                        path=REPO_ID, \n",
    "                        name=\"2024NOV2_1file_full\",\n",
    "                        token=os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd run with 8 pdf's\n",
    "dataset_2 = load_dataset(\n",
    "                        path=REPO_ID, \n",
    "                        name=\"2024NOV2_8file_full\",\n",
    "                        token=os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1, dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = concatenate_datasets([dataset_1['train'], dataset_2['train']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load JSON cached file of 13 files processed that did not get uploaded to HF at the end of the pipeline run\n",
    "* this data did not include the answers generated from RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pipeline/cache/2024NOV2_13files_full_get_answer_cache.json', 'r') as f:\n",
    "    answer_cache = json.load(f)\n",
    "\n",
    "with open('../pipeline/cache/2024NOV2_13files_full_question_generator_cache.json', 'r') as f:\n",
    "    question_cache = json.load(f)\n",
    "\n",
    "with open('../pipeline/cache/2024NOV2_13files_full_refine_question_cache.json', 'r') as f:\n",
    "    refine_question_cache = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the key is the literal string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(answer_cache.items()))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_get_answer_cache(cache: dict):\n",
    "    # Create new dict with simplified keys\n",
    "    qa_list = []    \n",
    "    for key, value in cache.items():\n",
    "        # Parse the JSON string key\n",
    "        try:\n",
    "            key_dict = json.loads(key)\n",
    "            qa_list.append({\n",
    "                'question': key_dict['question'],\n",
    "                'chunk': key_dict['chunk'],\n",
    "                'answer': value\n",
    "            })\n",
    "        except json.JSONDecodeError:\n",
    "            # Skip malformed keys\n",
    "            print(f\"Skipping key: {key}\")\n",
    "            continue\n",
    "            \n",
    "    return qa_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_answer_cache = clean_get_answer_cache(answer_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_answer_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question_cache seems to have a one to many mapping. The key_dict contains everything value has except it also contains the entities extracted, this isn't necessary for the final HF data, thus we will just use the value in the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_question_generator_cache(cache: dict):\n",
    "    # Create new dict with simplified keys\n",
    "    qa_list = []    \n",
    "    for key, value in cache.items():\n",
    "        qa_list.extend(value)\n",
    "            \n",
    "    return qa_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_question_cache = clean_question_generator_cache(question_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_question_cache) # may contain duplicates in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_question_cache[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_answer_cache[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can merge cleaned_question_cache and cleaned_answer_cache to get the same fields as the dataset, then will upload cleaned data to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(\n",
    "                    pd.DataFrame(cleaned_question_cache),\n",
    "                    pd.DataFrame(cleaned_answer_cache),\n",
    "                    on=['question', 'chunk'],\n",
    "                    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = Dataset.from_dict(df_merged.to_dict(orient='list'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = concatenate_datasets([merged_dataset, cleaned_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combined_dataset.data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[df.answer.str.contains('NO ANSWER FOUND')].shape[0])\n",
    "df = df.loc[~df.answer.str.contains('NO ANSWER FOUND')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['question', 'chunk'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../pipeline/outputs/2024NOV2_combined_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=Dataset.from_dict(df.to_dict(orient='list'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub(\n",
    "    repo_id=\"CPSC532/2024NOV2_arxiv_qa_data_cleaned\",\n",
    "    token=os.getenv(\"HUGGINGFACE_API_KEY\"),\n",
    "    commit_message=\"cleaned data\",\n",
    "    commit_description=\"Dataset contains 681 questons and answers from 19 different arxiv pdf files\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
