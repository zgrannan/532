{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "sys.path.append('../')\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import requests\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import json \n",
    "\n",
    "import pandas as pd \n",
    "from pipeline.helpers import list_of_dicts_to_dict_of_lists, upload_to_hf \n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data generated from NOV2 and upload cleaned set to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ID=\"CPSC532/arxiv_qa_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 113/113 [00:00<00:00, 11645.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 1st run with 1 pdf\n",
    "dataset_1 = load_dataset(\n",
    "                        path=REPO_ID, \n",
    "                        name=\"2024NOV2_1file_full\",\n",
    "                        token=os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 388/388 [00:00<00:00, 14705.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 2nd run with 8 pdf's\n",
    "dataset_2 = load_dataset(\n",
    "                        path=REPO_ID, \n",
    "                        name=\"2024NOV2_8file_full\",\n",
    "                        token=os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['filename', 'source', 'source_type', 'chunk', 'question', 'answer'],\n",
       "         num_rows: 113\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['filename', 'source', 'source_type', 'chunk', 'question', 'answer'],\n",
       "         num_rows: 388\n",
       "     })\n",
       " }))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_1, dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = concatenate_datasets([dataset_1['train'], dataset_2['train']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['filename', 'source', 'source_type', 'chunk', 'question', 'answer'],\n",
       "    num_rows: 501\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load JSON cached file of 13 files processed that did not get uploaded to HF at the end of the pipeline run\n",
    "* this data did not include the answers generated from RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pipeline/cache/2024NOV2_13files_full_get_answer_cache.json', 'r') as f:\n",
    "    answer_cache = json.load(f)\n",
    "\n",
    "with open('../pipeline/cache/2024NOV2_13files_full_question_generator_cache.json', 'r') as f:\n",
    "    question_cache = json.load(f)\n",
    "\n",
    "with open('../pipeline/cache/2024NOV2_13files_full_refine_question_cache.json', 'r') as f:\n",
    "    refine_question_cache = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the key is the literal string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"question\": \"Why does the use of full datasets in training models introduce inconsistencies and potential unfair comparisons, according to the paper \\'Making Text Embedders Few-Shot Learners\\'?\", \"chunk\": \"-Clustering-{S2S/P2P},_\\\\nTwentyNewsgroups-Clustering (Lang, 1995).\\\\n\\\\n    - STS: STS12 (Agirre et al., 2012), STS22 (Chen et al., 2022), STS-Benchmark (Cer et al.,\\\\n2017).\\\\n\\\\n**Training Detail. We fine-tune the Mistral-7B model using a contrastive loss and conduct the pro-**\\\\ncess over a single epoch. For efficient fine-tuning, we employ Low-Rank Adaptation (LoRA) (Hu\\\\net al., 2021), setting the LoRA rank to 64 and the LoRA alpha to 32, with a learning rate of 1e-4.\\\\nFor retrieval tasks, we use in-batch negatives, a strategy not adopted for other tasks. Each dataset\\\\nincorporates 7 hard negatives. The batch size is set to 512 for retrieval tasks and 256 for other types\\\\nof tasks. We maintain consistency by using the same dataset throughout one training step, and the\\\\nmaximum sequence length is set at 512 tokens. To distill the score from reranker in retrieval tasks,\\\\nwe use the bge-reranker model as the teacher. For in-context learning training, we implement a\\\\nrandomized sampling method. For each query, we select between 0 to 5 examples from the in-batch\\\\ntraining data. The maximum allowable lengths for example queries and documents are set to 256\\\\ntokens each, and the combined length for a query with examples is set at 2048 tokens.\\\\n\\\\n**Evaluation. We evaluate the performance of our model under both zero-shot and few-shot condi-**\\\\ntions. In the few-shot scenario, a consistent set of in-context examples is applied to each query. The\\\\nexamples utilized for evaluation are sourced from training datasets. In cases where training datasets\\\\nare unavailable, examples are generated using ChatGPT.\\\\n\\\\n4.2 MAIN RESULTS\\\\n\\\\n**MTEB. Table 1 presents the performance of our model, bge-en-icl, evaluated on the MTEB bench-**\\\\nmark. This evaluation contrasts the results obtained from using the full dataset with those obtained\\\\nfrom using only the public dataset. When leveraging the full dataset, our model demonstrates strong\\\\ncapabilities in both zero-shot and few-shot settings, achieving SOTA results in few-shot scenarios.\\\\nHowever, it is important to note that the use of full datasets may introduce inconsistencies, as different models often rely on varying datasets. Notably, many of these models do not disclose the\\\\nspecific datasets they use, leading to potential unfair comparisons.\\\\n\\\\nFor a fairer comparison and to better understand the impact of in-context learning, we conducts\\\\nan evaluation using only the public dataset. Under these constraints, our model\\\\u2019s performance in\\\\n\\\\n\\\\n-----\\\\n\\\\nTask Retr. Rerank. Clust. PairClass. Class. STS Summ. Avg.\\\\n# of datasets \\\\u2192 15 4 11 3 12 10 1 56\\\\n\\\\nw/ full data\\\\n\\\\nE5-mistral-7b-instruct 56.90 60.21 50.26 88.34 78.47 84.66 31.40 66.63\\\\nGritLM-7B 57.41 60.49 50.61 87.16 79.46 83.35 30.37 66.76\\\\nSFR-Embedding 59.00 60.64 51.67 88.54 78.33 85.05 31.16 67.56\\\\nLinq-Embed-Mistral 60.19 60.29 51.42 88.35 80.20 84.97 30.98 68.17\\\\nvoyage-large-2-instruct 58.28 60.09 53.35 89.24 81.49 84.31 30.84 68.23\\\\nNV-Embed-v1 59.36 60.59 52.80 86.91 87.35 82.84 31.20 69.32\\\\nbge-multilingual-gemma2 59.24 59.72 54.65 85.84 88.08 83.88 31.20 69.88\\\\nstella en 400M v5 58.97 60.16 56.70 87.74 86.67 84.22 31.66 70.11\\\\ngte-Qwen2-7B-instruct 60.25 61.42 56.92 85.79 86.58 83.04 31.35 70.24\\\\nSFR-Embedding-2 R 60.18 60.14 56.17 88.07 89.05 81.26 30.71 70.31\\\\nstella en 1.5B v5 61.01 61.21 57.69 88.07 87.63 84.51 31.49 71.19\\\\n**bge-en-icl (zero-shot)** 61.67 59.66 57.51 86.93 88.62 83.74 30.75 71.24\\\\n**bge-en-icl (few-shot)** 62.16 59.82 57.89 88.14 88.95 84.24 30.77 **71.67**\\\\n\\\\nw/ public data only\\\\n\\\\nE5-mistral-7b-instruct 52.78 60.38 47.78 88.47 76.80 83.77 31.90 64.56\\\\nGritLM-7B 53.10 61.30 48.90 86.90 77.00 82.80 29.40 64.70\\\\nLLM2Vec-Mistral-supervised 55.99 58.42 45.54 87.99 76.63 84.09 29.96 64.80\\\\n**bge-en-icl (zero-shot)** 59.59 56.85 42.61 87.87 75.47 83.30 29.52 64.67\\\\n**bge-en-icl (few-shot)** 60.08 56.67 46.55 88.51 77.31 83.69 30.68 **66.08**\\\\n\\\\nTable 1: Top MTEB leaderboard models as of August 27, 2024.\\\\n\\\\nDomain wiki web news healthcare law finance arxiv msmarco Avg.\\\\n# of datasets \\\\u2192 1 1 1 1 1 1 1 1 8\\\\n\\\\nw/ full data\\\\n\\\\nE5-mistral-7b-instruct 61.67 44.41 48.18 56.32 19.32 54.79 44.78 59.03 48.56\\\\nSFR-Embedding 63.46 51.27 52.21 58.76 23.27 56.94 47.75 58.99 51.58\\\\nNV-Embed-v1 62.84 50.42 51.46 58.53 20.65 49.89 46.10 60.27 50.02\\\\nLinq-Embed-Mistral 61.04 48.41 49.44 60.18 20.34 50.04 47.56 60.50 49.69\\\\ngte-Qwen2-7B-instruct 63.46 51.20 54.07 54.20 22.31 58.20 40.27 58.39 50.26\\\\nstella en 1.5B v5 61.99 50.88 53.87 58.81 23.22 57.26 44.81 61.38 51.53\\\\nbge-en-icl (zero-shot) 64.61 54.40 55.11 57.25 25.10 54.81 48.46 63.71 52.93\\\\nbge-en-icl (few-shot) 64.94 55.11 56.02 58.85 28.29 57.16 50.04 64.50 **54.36**\\\\n\\\\nw/ public data only\\\\n\\\\nbge-en-icl (zero-shot) 64.82 54.96 55.82 57.06 28.87 54.46 49.60 63.25 53.60\\\\nbge-en-icl (few-shot) 66.98 56.38 57.17 59.54 32.03 58.81 51.36 65.05 **55.92**\\\\n\\\\nTable 2: QA (en, nDCG@10) performance on AIR-Bench 24.04.\\\\n\\\\nthe zero-shot scenario is on par with, or slightly below, that of other models such as LLM2Vec\\\\nand GritLM. However, in the few-shot settings, our model show significant enhancements (\\\\u21911.41),\\\\nparticularly in the classification and clustering tasks that were not part of the training data. These\\\\nimprovements underscore the potential advantages of in-context learning, emphasizing its efficacy\\\\nin adapting to tasks beyond the direct scope of initial training parameters. Furthermore, in contrast\\\\nto training exclusively with public datasets, the utilization of full training data effectively familiarizes the model with these datasets. As a result, the model\\\\u2019s ability to generalize effectively is\\\\ncompromised, leading to only a modest improvement in few-shot settings (\\\\u21910.43).\\\\n\\\\n**AIR-Bench. The performance of our model is also evaluated using the AIR-Bench dataset. As**\\\\nillustrated in Tables 2 and 3, the model demonstrates superior performance compared to prior models\\\\nin both zero-shot and few-shot scenarios, excelling across qa and long-doc tasks. Notably, there is\\\\nno overlap between the training dataset and the evaluation data for these tasks, highlighting the\\\\nrobustness of the model in scenarios with limited prior exposure. In the few-shot setting, the model\\\\nexhibits significant improvements over the zero-shot scenario, achieving gains of 1.43 points in the\\\\nqa task and 1.08 points in the long-doc task. This improvement underscores the efficacy of in-context\\\\nlearning in enhancing the model\\\\u2019s generalization capabilities.\\\\n\\\\n|Task # of datasets \\\\u2192|Retr. Rerank. Clust. PairClass. Class. STS Summ. 15 4 11 3 12 10 1|Avg. 56|\\\\n|---|---|---|\\\\n\\\\n|E5-mistral-7b-instruct GritLM-7B SFR-Embedding Linq-Embed-Mistral voyage-large-2-instruct NV-Embed-v1 bge-multilingual-gemma2 stella en 400M v5 gte-Qwen2-7B-instruct SFR-Embedding-2 R stella en 1.5B v5 bge-en-icl (zero-shot) bge-en-icl (few-shot)|56.90 60.21 50.26 88.34 78.47 84.66 31.40 57.41 60.49 50.61 87.16 79.46 83.35 30.37 59.00 60.64 51.67 88.54 78.33 85.05 31.16 60.19 60.29 51.42 88.35 80.20 84.97 30.98 58.28 60.09 53.35 89.24 81.49 84.31 30.84 59.36 60.59 52.80 86.91 87.35 82.84 31.20 59.24 59.72 54.65 85.84 88.08 83.88 31.20 58.97 60.16 56.70 87.74 86.67 84.22 31.66 60.25 61.42 56.92 85.79 86.58 83.04 31.35 60.18 60.14 56.17 88.07 89.05 81.26 30.71 61.01 61.21 57.69 88.07 87.63 84.51 31.49 61.67 59.66 57.51 86.93 88.62 83.74 30.75 62.16 59.82 57.89 88.14 88.95 84.24 30.77|66.63 66.76 67.56 68.17 68.23 69.32 69.88 70.11 70.24 70.31 71.19 71.24 71.67|\\\\n|---|---|---|\\\\n\\\\n|E5-mistral-7b-instruct GritLM-7B LLM2Vec-Mistral-supervised bge-en-icl (zero-shot) bge-en-icl (few-shot)|52.78 60.38 47.78 88.47 76.80 83.77 31.90 53.10 61.30 48.90 86.90 77.00 82.80 29.40 55.99 58.42 45.54 87.99 76.63 84.09 29.96 59.59 56.85 42.61 87.87 75.47 83.30 29.52 60.08 56.67 46.55 88.51 77.31 83.69 30.68|64.56 64.70 64.80 64.67 66.08|\\\\n|---|---|---|\\\\n\\\\n|Domain # of datasets \\\\u2192|wiki web news healthcare law finance arxiv msmarco 1 1 1 1 1 1 1 1|Avg. 8|\\\\n|---|---|---|\\\\n\\\\n|E5-mistral-7b-instruct SFR-Embedding NV-Embed-v1 Linq-Embed-Mistral gte-Qwen2-7B-instruct stella en 1.5B v5 bge-en-icl (zero-shot) bge-en-icl (few-shot)|61.67 44.41 48.18 56.32 19.32 54.79 44.78 59.03 63.46 51.27 52.21 58.76 23.27 56.94 47.75 58.99 62.84 50.42 51.46 58.53 20.65 49.89 46.10 60.27 61.04 48.41 49.44 60.18 20.34 50.04 47.56 60.50 63.46 51.20 54.07 54.20 22.31 58.20 40.27 58.39 61.99 50.88 53.87 58.81 23.22 57.26 44.81 61.38 64.61 54.40 55.11 57.25 25.10 54.81 48.46 63.71 64.94 55.11 56.02 58.85 28.29 57.16 50.04 64.50|48.56 51.58 50.02 49.69 50.26 51.53 52.93 54.36|\\\\n|---|---|---|\\\\n\\\\n|bge-en-icl (zero-shot) bge-en-icl (few-shot)|64.82 54.96 55.82 57.06 28.87 54.46 49.60 63.25 66.98 56.38 57.17 59.54 32.03 58.81 51.36 65.05|53.60 55.92|\\\\n|---|---|---|\\\\n\\\\n\\\\n-----\\\\n\\\\nDomain arxiv book healthcare law Avg.\\\\n# of datasets \\\\u2192 4 2 5 4 15\\\\n\\\\nw/ full data\\\\n\\\\ntext-embedding-3-large 74.53 73.16 65.83 64.47 68.77\\\\nE5-mistral-7b-instruct 72.14 72.44 68.44 62.92 68.49\\\\nSFR-Embedding 72.79 72.41 67.94 64.83 69.00\\\\nNV-Embed-v1 77.65 75.49 72.38 69.55 73.45\\\\nLinq-Embed-Mistral 75.46 73.81 71.58 68.58 72.11\\\\ngte-Qwen2-7B-instruct 63.93 68.51 65.59 65.26 65.45\\\\nstella en 1.5B v5 73.17 74.38 70.02 69.32 71.25\\\\nbge-multilingual-gemma2 71.77 76.46 73.96 70.86 72.88\\\\nbge-en-icl (zero-shot) 78.30 78.21 73.65 67.09 73.75\\\\nbge-en-icl (few-shot) 79.63 79.36 74.80 67.79 **74.83**\\\\n\\\\nw/ public data only\\\\n\\\\nbge-en-icl (zero-shot) 79.73 78.66 72.88 70.59 74.86\\\\nbge-en-icl (few-shot) 79.82 80.37 74.60 71.66 **75.98**\\\\n\\\\nTable 3: Long-Doc (en, Recall@10) performance on AIR-Bench 24.04.\\\\n\\\\nHowever, when the model is trained exclusively using public data, it achieves better results compared to training with the full dataset. This could be attributed to the presence of an excessive\\\\namount of MTEB-related data, such as clustering and classification, within the full dataset. Such\\\\ndata might introduce the risk of overfitting, thereby potentially hampering the model\\\\u2019s generalization\\\\nperformance on the AIR-Bench dataset.\\\\n\\\\n4.3 IN-CONTEXT LEARNING\\\\n\\\\nTask Retr. Rerank. Clust. PairClass. Class. STS Summ. Avg.\\\\n# of datasets \\\\u2192 15 4 11 3 12 10 1 56\\\\n\\\\nw/ full data\\\\n\\\\nw/o in-context learning 59.11 57.02 42.60 87.99 76.27 83.93 30.50 64.83\\\\nw/ fix examples (zero-shot) 48.98 56.48 41.84 85.94 74.38 84.31 29.68 61.50\\\\nw/ fix examples (few-shot) 59.00 56.90 45.75 88.54 75.56 84.67 30.66 65.46\\\\nw/ in-batch examples (zero-shot) 59.59 56.85 42.61 87.87 75.47 83.30 29.52 64.67\\\\nw/ in-batch examples (few-shot) 60.08 56.67 46.55 88.51 77.31 83.69 30.68 **66.08**\\\\n\\\\nTable 4: Evaluation of various ICL strategies on the MTEB Benchmark.\\\\n\\\\nTo evaluate the impact of the ICL strategy, we conduct a series of ablation studies using the MTEB\\\\nbenchmark. In these studies, we compare the performance of models fine-tuned with the ICL strategy against those fine-tuned without it. Specifically, for ICL training, we employ two distinct training approaches: fixed examples and in-batch examples. In the fixed examples approach, each task\\\\nwas trained using three predetermined examples.\\\\n\\\\nIn Table 4, we present various results from our experiment. When the model is trained without ICL\\\\nstrategy, its average performance is 64.83. This performance is comparable to GritLM (Muennighoff\\\\net al., 2024), LLM2Vec (BehnamGhader et al., 2024), etc. When fixed examples are used during\\\\nICL training, there is a significant decline in zero-shot evaluation performance, with a decrease of\\\\n3.33 points. This decline is attributed to the model\\\\u2019s consistent exposure to specific training examples, which can impair its zero-shot capabilities. On the other hand, in few-shot scenarios, the\\\\nmodel demonstrates improved performance, exceeding zero-shot results by 3.96 points and surpassing models trained without ICL by 0.63 points. This also confirms the effectiveness of the ICL\\\\nstrategy.\\\\n\\\\nMeanwhile, the use of in-batch examples, where training may involve zero examples, has preserved\\\\nthe zero-shot capability of the model. There is a modest decrease of 0.16 points compared to the\\\\nmodel trained without ICL. Notably, in few-shot scenarios, the performance of the model employing\\\\nin-batch examples rises to 66.08 (\\\\u21911.25), indicating a robust improvement. Furthermore, when compared with the model utilizing fixed examples, the model trained with in-batch examples displays\\\\n\\\\n|Domain # of datasets \\\\u2192|arxiv book healthcare law 4 2 5 4|Avg. 15|\\\\n|---|---|---|\\\\n\\\\n|text-embedding-3-large E5-mistral-7b-instruct SFR-Embedding NV-Embed-v1 Linq-Embed-Mistral gte-Qwen2-7B-instruct stella en 1.5B v5 bge-multilingual-gemma2 bge-en-icl (zero-shot) bge-en-icl (few-shot)|74.53 73.16 65.83 64.47 72.14 72.44 68.44 62.92 72.79 72.41 67.94 64.83 77.65 75.49 72.38 69.55 75.46 73.81 71.58 68.58 63.93 68.51 65.59 65.26 73.17 74.38 70.02 69.32 71.77 76.46 73.96 70.86 78.30 78.21 73.65 67.09 79.63 79.36 74.80 67.79|68.77 68.49 69.00 73.45 72.11 65.45 71.25 72.88 73.75 74.83|\\\\n|---|---|---|\\\\n\\\\n|bge-en-icl (zero-shot) bge-en-icl (few-shot)|79.73 78.66 72.88 70.59 79.82 80.37 74.60 71.66|74.86 75.98|\\\\n|---|---|---|\\\\n\\\\n|Task # of datasets \\\\u2192|Retr. Rerank. Clust. PairClass. Class. STS Summ. 15 4 11 3 12 10 1|Avg. 56|\\\\n|---|---|---|\\\\n\\\\n|w/o in-context learning w/ fix examples (zero-shot) w/ fix examples (few-shot) w/ in-batch examples (zero-shot) w/ in-batch examples (few-shot)|59.11 57.02 42.60 87.99 76.27 83.93 30.50 48.98 56.48 41.84 85.94 74.38 84.31 29.68 59.00 56.90 45.75 88.54 75.56 84.67 30.66 59.59 56.85 42.61 87.87 75.47 83.30 29.52 60.08 56.67 46.55 88.51 77.31 83.69 30.68|64.83 61.50 65.46 64.67 66.08|\\\\n|---|---|---|\\\\n\\\\n\\\\n-----\\\\n\\\\nsuperior performance in tasks that diverge significantly from the training data, such as classification\\\\nand clustering tasks.\\\\n\\\\n4.4 ATTENTION\\\\n\\\\nTask Retr. Rerank. Clust. PairClass. Class. STS Summ. Avg.\\\\n# of datasets \\\\u2192 15 4 11 3 12 10 1 56\\\\n\\\\ncausal attention & last token pooling\\\\n\\\\nw/o in-context learning 59.11 57.02\"}'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(answer_cache.items()))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_get_answer_cache(cache: dict):\n",
    "    # Create new dict with simplified keys\n",
    "    qa_list = []    \n",
    "    for key, value in cache.items():\n",
    "        # Parse the JSON string key\n",
    "        try:\n",
    "            key_dict = json.loads(key)\n",
    "            qa_list.append({\n",
    "                'question': key_dict['question'],\n",
    "                'chunk': key_dict['chunk'],\n",
    "                'answer': value\n",
    "            })\n",
    "        except json.JSONDecodeError:\n",
    "            # Skip malformed keys\n",
    "            print(f\"Skipping key: {key}\")\n",
    "            continue\n",
    "            \n",
    "    return qa_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_answer_cache = clean_get_answer_cache(answer_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "430"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_answer_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question_cache seems to have a one to many mapping. The key_dict contains everything value has except it also contains the entities extracted, this isn't necessary for the final HF data, thus we will just use the value in the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_question_generator_cache(cache: dict):\n",
    "    # Create new dict with simplified keys\n",
    "    qa_list = []    \n",
    "    for key, value in cache.items():\n",
    "        qa_list.extend(value)\n",
    "            \n",
    "    return qa_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_question_cache = clean_question_generator_cache(question_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_question_cache) # may contain duplicates in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': '../data/Making_Text_Embedders_Few-Shot_Learners_2409.15700v1.pdf',\n",
       " 'source': 'Making_Text_Embedders_Few-Shot_Learners_2409.15700v1',\n",
       " 'source_type': 'paper',\n",
       " 'chunk': '-Clustering-{S2S/P2P},_\\nTwentyNewsgroups-Clustering (Lang, 1995).\\n\\n    - STS: STS12 (Agirre et al., 2012), STS22 (Chen et al., 2022), STS-Benchmark (Cer et al.,\\n2017).\\n\\n**Training Detail. We fine-tune the Mistral-7B model using a contrastive loss and conduct the pro-**\\ncess over a single epoch. For efficient fine-tuning, we employ Low-Rank Adaptation (LoRA) (Hu\\net al., 2021), setting the LoRA rank to 64 and the LoRA alpha to 32, with a learning rate of 1e-4.\\nFor retrieval tasks, we use in-batch negatives, a strategy not adopted for other tasks. Each dataset\\nincorporates 7 hard negatives. The batch size is set to 512 for retrieval tasks and 256 for other types\\nof tasks. We maintain consistency by using the same dataset throughout one training step, and the\\nmaximum sequence length is set at 512 tokens. To distill the score from reranker in retrieval tasks,\\nwe use the bge-reranker model as the teacher. For in-context learning training, we implement a\\nrandomized sampling method. For each query, we select between 0 to 5 examples from the in-batch\\ntraining data. The maximum allowable lengths for example queries and documents are set to 256\\ntokens each, and the combined length for a query with examples is set at 2048 tokens.\\n\\n**Evaluation. We evaluate the performance of our model under both zero-shot and few-shot condi-**\\ntions. In the few-shot scenario, a consistent set of in-context examples is applied to each query. The\\nexamples utilized for evaluation are sourced from training datasets. In cases where training datasets\\nare unavailable, examples are generated using ChatGPT.\\n\\n4.2 MAIN RESULTS\\n\\n**MTEB. Table 1 presents the performance of our model, bge-en-icl, evaluated on the MTEB bench-**\\nmark. This evaluation contrasts the results obtained from using the full dataset with those obtained\\nfrom using only the public dataset. When leveraging the full dataset, our model demonstrates strong\\ncapabilities in both zero-shot and few-shot settings, achieving SOTA results in few-shot scenarios.\\nHowever, it is important to note that the use of full datasets may introduce inconsistencies, as different models often rely on varying datasets. Notably, many of these models do not disclose the\\nspecific datasets they use, leading to potential unfair comparisons.\\n\\nFor a fairer comparison and to better understand the impact of in-context learning, we conducts\\nan evaluation using only the public dataset. Under these constraints, our model’s performance in\\n\\n\\n-----\\n\\nTask Retr. Rerank. Clust. PairClass. Class. STS Summ. Avg.\\n# of datasets → 15 4 11 3 12 10 1 56\\n\\nw/ full data\\n\\nE5-mistral-7b-instruct 56.90 60.21 50.26 88.34 78.47 84.66 31.40 66.63\\nGritLM-7B 57.41 60.49 50.61 87.16 79.46 83.35 30.37 66.76\\nSFR-Embedding 59.00 60.64 51.67 88.54 78.33 85.05 31.16 67.56\\nLinq-Embed-Mistral 60.19 60.29 51.42 88.35 80.20 84.97 30.98 68.17\\nvoyage-large-2-instruct 58.28 60.09 53.35 89.24 81.49 84.31 30.84 68.23\\nNV-Embed-v1 59.36 60.59 52.80 86.91 87.35 82.84 31.20 69.32\\nbge-multilingual-gemma2 59.24 59.72 54.65 85.84 88.08 83.88 31.20 69.88\\nstella en 400M v5 58.97 60.16 56.70 87.74 86.67 84.22 31.66 70.11\\ngte-Qwen2-7B-instruct 60.25 61.42 56.92 85.79 86.58 83.04 31.35 70.24\\nSFR-Embedding-2 R 60.18 60.14 56.17 88.07 89.05 81.26 30.71 70.31\\nstella en 1.5B v5 61.01 61.21 57.69 88.07 87.63 84.51 31.49 71.19\\n**bge-en-icl (zero-shot)** 61.67 59.66 57.51 86.93 88.62 83.74 30.75 71.24\\n**bge-en-icl (few-shot)** 62.16 59.82 57.89 88.14 88.95 84.24 30.77 **71.67**\\n\\nw/ public data only\\n\\nE5-mistral-7b-instruct 52.78 60.38 47.78 88.47 76.80 83.77 31.90 64.56\\nGritLM-7B 53.10 61.30 48.90 86.90 77.00 82.80 29.40 64.70\\nLLM2Vec-Mistral-supervised 55.99 58.42 45.54 87.99 76.63 84.09 29.96 64.80\\n**bge-en-icl (zero-shot)** 59.59 56.85 42.61 87.87 75.47 83.30 29.52 64.67\\n**bge-en-icl (few-shot)** 60.08 56.67 46.55 88.51 77.31 83.69 30.68 **66.08**\\n\\nTable 1: Top MTEB leaderboard models as of August 27, 2024.\\n\\nDomain wiki web news healthcare law finance arxiv msmarco Avg.\\n# of datasets → 1 1 1 1 1 1 1 1 8\\n\\nw/ full data\\n\\nE5-mistral-7b-instruct 61.67 44.41 48.18 56.32 19.32 54.79 44.78 59.03 48.56\\nSFR-Embedding 63.46 51.27 52.21 58.76 23.27 56.94 47.75 58.99 51.58\\nNV-Embed-v1 62.84 50.42 51.46 58.53 20.65 49.89 46.10 60.27 50.02\\nLinq-Embed-Mistral 61.04 48.41 49.44 60.18 20.34 50.04 47.56 60.50 49.69\\ngte-Qwen2-7B-instruct 63.46 51.20 54.07 54.20 22.31 58.20 40.27 58.39 50.26\\nstella en 1.5B v5 61.99 50.88 53.87 58.81 23.22 57.26 44.81 61.38 51.53\\nbge-en-icl (zero-shot) 64.61 54.40 55.11 57.25 25.10 54.81 48.46 63.71 52.93\\nbge-en-icl (few-shot) 64.94 55.11 56.02 58.85 28.29 57.16 50.04 64.50 **54.36**\\n\\nw/ public data only\\n\\nbge-en-icl (zero-shot) 64.82 54.96 55.82 57.06 28.87 54.46 49.60 63.25 53.60\\nbge-en-icl (few-shot) 66.98 56.38 57.17 59.54 32.03 58.81 51.36 65.05 **55.92**\\n\\nTable 2: QA (en, nDCG@10) performance on AIR-Bench 24.04.\\n\\nthe zero-shot scenario is on par with, or slightly below, that of other models such as LLM2Vec\\nand GritLM. However, in the few-shot settings, our model show significant enhancements (↑1.41),\\nparticularly in the classification and clustering tasks that were not part of the training data. These\\nimprovements underscore the potential advantages of in-context learning, emphasizing its efficacy\\nin adapting to tasks beyond the direct scope of initial training parameters. Furthermore, in contrast\\nto training exclusively with public datasets, the utilization of full training data effectively familiarizes the model with these datasets. As a result, the model’s ability to generalize effectively is\\ncompromised, leading to only a modest improvement in few-shot settings (↑0.43).\\n\\n**AIR-Bench. The performance of our model is also evaluated using the AIR-Bench dataset. As**\\nillustrated in Tables 2 and 3, the model demonstrates superior performance compared to prior models\\nin both zero-shot and few-shot scenarios, excelling across qa and long-doc tasks. Notably, there is\\nno overlap between the training dataset and the evaluation data for these tasks, highlighting the\\nrobustness of the model in scenarios with limited prior exposure. In the few-shot setting, the model\\nexhibits significant improvements over the zero-shot scenario, achieving gains of 1.43 points in the\\nqa task and 1.08 points in the long-doc task. This improvement underscores the efficacy of in-context\\nlearning in enhancing the model’s generalization capabilities.\\n\\n|Task # of datasets →|Retr. Rerank. Clust. PairClass. Class. STS Summ. 15 4 11 3 12 10 1|Avg. 56|\\n|---|---|---|\\n\\n|E5-mistral-7b-instruct GritLM-7B SFR-Embedding Linq-Embed-Mistral voyage-large-2-instruct NV-Embed-v1 bge-multilingual-gemma2 stella en 400M v5 gte-Qwen2-7B-instruct SFR-Embedding-2 R stella en 1.5B v5 bge-en-icl (zero-shot) bge-en-icl (few-shot)|56.90 60.21 50.26 88.34 78.47 84.66 31.40 57.41 60.49 50.61 87.16 79.46 83.35 30.37 59.00 60.64 51.67 88.54 78.33 85.05 31.16 60.19 60.29 51.42 88.35 80.20 84.97 30.98 58.28 60.09 53.35 89.24 81.49 84.31 30.84 59.36 60.59 52.80 86.91 87.35 82.84 31.20 59.24 59.72 54.65 85.84 88.08 83.88 31.20 58.97 60.16 56.70 87.74 86.67 84.22 31.66 60.25 61.42 56.92 85.79 86.58 83.04 31.35 60.18 60.14 56.17 88.07 89.05 81.26 30.71 61.01 61.21 57.69 88.07 87.63 84.51 31.49 61.67 59.66 57.51 86.93 88.62 83.74 30.75 62.16 59.82 57.89 88.14 88.95 84.24 30.77|66.63 66.76 67.56 68.17 68.23 69.32 69.88 70.11 70.24 70.31 71.19 71.24 71.67|\\n|---|---|---|\\n\\n|E5-mistral-7b-instruct GritLM-7B LLM2Vec-Mistral-supervised bge-en-icl (zero-shot) bge-en-icl (few-shot)|52.78 60.38 47.78 88.47 76.80 83.77 31.90 53.10 61.30 48.90 86.90 77.00 82.80 29.40 55.99 58.42 45.54 87.99 76.63 84.09 29.96 59.59 56.85 42.61 87.87 75.47 83.30 29.52 60.08 56.67 46.55 88.51 77.31 83.69 30.68|64.56 64.70 64.80 64.67 66.08|\\n|---|---|---|\\n\\n|Domain # of datasets →|wiki web news healthcare law finance arxiv msmarco 1 1 1 1 1 1 1 1|Avg. 8|\\n|---|---|---|\\n\\n|E5-mistral-7b-instruct SFR-Embedding NV-Embed-v1 Linq-Embed-Mistral gte-Qwen2-7B-instruct stella en 1.5B v5 bge-en-icl (zero-shot) bge-en-icl (few-shot)|61.67 44.41 48.18 56.32 19.32 54.79 44.78 59.03 63.46 51.27 52.21 58.76 23.27 56.94 47.75 58.99 62.84 50.42 51.46 58.53 20.65 49.89 46.10 60.27 61.04 48.41 49.44 60.18 20.34 50.04 47.56 60.50 63.46 51.20 54.07 54.20 22.31 58.20 40.27 58.39 61.99 50.88 53.87 58.81 23.22 57.26 44.81 61.38 64.61 54.40 55.11 57.25 25.10 54.81 48.46 63.71 64.94 55.11 56.02 58.85 28.29 57.16 50.04 64.50|48.56 51.58 50.02 49.69 50.26 51.53 52.93 54.36|\\n|---|---|---|\\n\\n|bge-en-icl (zero-shot) bge-en-icl (few-shot)|64.82 54.96 55.82 57.06 28.87 54.46 49.60 63.25 66.98 56.38 57.17 59.54 32.03 58.81 51.36 65.05|53.60 55.92|\\n|---|---|---|\\n\\n\\n-----\\n\\nDomain arxiv book healthcare law Avg.\\n# of datasets → 4 2 5 4 15\\n\\nw/ full data\\n\\ntext-embedding-3-large 74.53 73.16 65.83 64.47 68.77\\nE5-mistral-7b-instruct 72.14 72.44 68.44 62.92 68.49\\nSFR-Embedding 72.79 72.41 67.94 64.83 69.00\\nNV-Embed-v1 77.65 75.49 72.38 69.55 73.45\\nLinq-Embed-Mistral 75.46 73.81 71.58 68.58 72.11\\ngte-Qwen2-7B-instruct 63.93 68.51 65.59 65.26 65.45\\nstella en 1.5B v5 73.17 74.38 70.02 69.32 71.25\\nbge-multilingual-gemma2 71.77 76.46 73.96 70.86 72.88\\nbge-en-icl (zero-shot) 78.30 78.21 73.65 67.09 73.75\\nbge-en-icl (few-shot) 79.63 79.36 74.80 67.79 **74.83**\\n\\nw/ public data only\\n\\nbge-en-icl (zero-shot) 79.73 78.66 72.88 70.59 74.86\\nbge-en-icl (few-shot) 79.82 80.37 74.60 71.66 **75.98**\\n\\nTable 3: Long-Doc (en, Recall@10) performance on AIR-Bench 24.04.\\n\\nHowever, when the model is trained exclusively using public data, it achieves better results compared to training with the full dataset. This could be attributed to the presence of an excessive\\namount of MTEB-related data, such as clustering and classification, within the full dataset. Such\\ndata might introduce the risk of overfitting, thereby potentially hampering the model’s generalization\\nperformance on the AIR-Bench dataset.\\n\\n4.3 IN-CONTEXT LEARNING\\n\\nTask Retr. Rerank. Clust. PairClass. Class. STS Summ. Avg.\\n# of datasets → 15 4 11 3 12 10 1 56\\n\\nw/ full data\\n\\nw/o in-context learning 59.11 57.02 42.60 87.99 76.27 83.93 30.50 64.83\\nw/ fix examples (zero-shot) 48.98 56.48 41.84 85.94 74.38 84.31 29.68 61.50\\nw/ fix examples (few-shot) 59.00 56.90 45.75 88.54 75.56 84.67 30.66 65.46\\nw/ in-batch examples (zero-shot) 59.59 56.85 42.61 87.87 75.47 83.30 29.52 64.67\\nw/ in-batch examples (few-shot) 60.08 56.67 46.55 88.51 77.31 83.69 30.68 **66.08**\\n\\nTable 4: Evaluation of various ICL strategies on the MTEB Benchmark.\\n\\nTo evaluate the impact of the ICL strategy, we conduct a series of ablation studies using the MTEB\\nbenchmark. In these studies, we compare the performance of models fine-tuned with the ICL strategy against those fine-tuned without it. Specifically, for ICL training, we employ two distinct training approaches: fixed examples and in-batch examples. In the fixed examples approach, each task\\nwas trained using three predetermined examples.\\n\\nIn Table 4, we present various results from our experiment. When the model is trained without ICL\\nstrategy, its average performance is 64.83. This performance is comparable to GritLM (Muennighoff\\net al., 2024), LLM2Vec (BehnamGhader et al., 2024), etc. When fixed examples are used during\\nICL training, there is a significant decline in zero-shot evaluation performance, with a decrease of\\n3.33 points. This decline is attributed to the model’s consistent exposure to specific training examples, which can impair its zero-shot capabilities. On the other hand, in few-shot scenarios, the\\nmodel demonstrates improved performance, exceeding zero-shot results by 3.96 points and surpassing models trained without ICL by 0.63 points. This also confirms the effectiveness of the ICL\\nstrategy.\\n\\nMeanwhile, the use of in-batch examples, where training may involve zero examples, has preserved\\nthe zero-shot capability of the model. There is a modest decrease of 0.16 points compared to the\\nmodel trained without ICL. Notably, in few-shot scenarios, the performance of the model employing\\nin-batch examples rises to 66.08 (↑1.25), indicating a robust improvement. Furthermore, when compared with the model utilizing fixed examples, the model trained with in-batch examples displays\\n\\n|Domain # of datasets →|arxiv book healthcare law 4 2 5 4|Avg. 15|\\n|---|---|---|\\n\\n|text-embedding-3-large E5-mistral-7b-instruct SFR-Embedding NV-Embed-v1 Linq-Embed-Mistral gte-Qwen2-7B-instruct stella en 1.5B v5 bge-multilingual-gemma2 bge-en-icl (zero-shot) bge-en-icl (few-shot)|74.53 73.16 65.83 64.47 72.14 72.44 68.44 62.92 72.79 72.41 67.94 64.83 77.65 75.49 72.38 69.55 75.46 73.81 71.58 68.58 63.93 68.51 65.59 65.26 73.17 74.38 70.02 69.32 71.77 76.46 73.96 70.86 78.30 78.21 73.65 67.09 79.63 79.36 74.80 67.79|68.77 68.49 69.00 73.45 72.11 65.45 71.25 72.88 73.75 74.83|\\n|---|---|---|\\n\\n|bge-en-icl (zero-shot) bge-en-icl (few-shot)|79.73 78.66 72.88 70.59 79.82 80.37 74.60 71.66|74.86 75.98|\\n|---|---|---|\\n\\n|Task # of datasets →|Retr. Rerank. Clust. PairClass. Class. STS Summ. 15 4 11 3 12 10 1|Avg. 56|\\n|---|---|---|\\n\\n|w/o in-context learning w/ fix examples (zero-shot) w/ fix examples (few-shot) w/ in-batch examples (zero-shot) w/ in-batch examples (few-shot)|59.11 57.02 42.60 87.99 76.27 83.93 30.50 48.98 56.48 41.84 85.94 74.38 84.31 29.68 59.00 56.90 45.75 88.54 75.56 84.67 30.66 59.59 56.85 42.61 87.87 75.47 83.30 29.52 60.08 56.67 46.55 88.51 77.31 83.69 30.68|64.83 61.50 65.46 64.67 66.08|\\n|---|---|---|\\n\\n\\n-----\\n\\nsuperior performance in tasks that diverge significantly from the training data, such as classification\\nand clustering tasks.\\n\\n4.4 ATTENTION\\n\\nTask Retr. Rerank. Clust. PairClass. Class. STS Summ. Avg.\\n# of datasets → 15 4 11 3 12 10 1 56\\n\\ncausal attention & last token pooling\\n\\nw/o in-context learning 59.11 57.02',\n",
       " 'question': \"What is the purpose of using Low-Rank Adaptation (LoRA) in fine-tuning the Mistral-7B model, as described in the paper 'Making Text Embedders Few-Shot Learners'?\"}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_question_cache[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"Why does the use of full datasets in training models introduce inconsistencies and potential unfair comparisons, according to the paper 'Making Text Embedders Few-Shot Learners'?\",\n",
       " 'chunk': '-Clustering-{S2S/P2P},_\\nTwentyNewsgroups-Clustering (Lang, 1995).\\n\\n    - STS: STS12 (Agirre et al., 2012), STS22 (Chen et al., 2022), STS-Benchmark (Cer et al.,\\n2017).\\n\\n**Training Detail. We fine-tune the Mistral-7B model using a contrastive loss and conduct the pro-**\\ncess over a single epoch. For efficient fine-tuning, we employ Low-Rank Adaptation (LoRA) (Hu\\net al., 2021), setting the LoRA rank to 64 and the LoRA alpha to 32, with a learning rate of 1e-4.\\nFor retrieval tasks, we use in-batch negatives, a strategy not adopted for other tasks. Each dataset\\nincorporates 7 hard negatives. The batch size is set to 512 for retrieval tasks and 256 for other types\\nof tasks. We maintain consistency by using the same dataset throughout one training step, and the\\nmaximum sequence length is set at 512 tokens. To distill the score from reranker in retrieval tasks,\\nwe use the bge-reranker model as the teacher. For in-context learning training, we implement a\\nrandomized sampling method. For each query, we select between 0 to 5 examples from the in-batch\\ntraining data. The maximum allowable lengths for example queries and documents are set to 256\\ntokens each, and the combined length for a query with examples is set at 2048 tokens.\\n\\n**Evaluation. We evaluate the performance of our model under both zero-shot and few-shot condi-**\\ntions. In the few-shot scenario, a consistent set of in-context examples is applied to each query. The\\nexamples utilized for evaluation are sourced from training datasets. In cases where training datasets\\nare unavailable, examples are generated using ChatGPT.\\n\\n4.2 MAIN RESULTS\\n\\n**MTEB. Table 1 presents the performance of our model, bge-en-icl, evaluated on the MTEB bench-**\\nmark. This evaluation contrasts the results obtained from using the full dataset with those obtained\\nfrom using only the public dataset. When leveraging the full dataset, our model demonstrates strong\\ncapabilities in both zero-shot and few-shot settings, achieving SOTA results in few-shot scenarios.\\nHowever, it is important to note that the use of full datasets may introduce inconsistencies, as different models often rely on varying datasets. Notably, many of these models do not disclose the\\nspecific datasets they use, leading to potential unfair comparisons.\\n\\nFor a fairer comparison and to better understand the impact of in-context learning, we conducts\\nan evaluation using only the public dataset. Under these constraints, our model’s performance in\\n\\n\\n-----\\n\\nTask Retr. Rerank. Clust. PairClass. Class. STS Summ. Avg.\\n# of datasets → 15 4 11 3 12 10 1 56\\n\\nw/ full data\\n\\nE5-mistral-7b-instruct 56.90 60.21 50.26 88.34 78.47 84.66 31.40 66.63\\nGritLM-7B 57.41 60.49 50.61 87.16 79.46 83.35 30.37 66.76\\nSFR-Embedding 59.00 60.64 51.67 88.54 78.33 85.05 31.16 67.56\\nLinq-Embed-Mistral 60.19 60.29 51.42 88.35 80.20 84.97 30.98 68.17\\nvoyage-large-2-instruct 58.28 60.09 53.35 89.24 81.49 84.31 30.84 68.23\\nNV-Embed-v1 59.36 60.59 52.80 86.91 87.35 82.84 31.20 69.32\\nbge-multilingual-gemma2 59.24 59.72 54.65 85.84 88.08 83.88 31.20 69.88\\nstella en 400M v5 58.97 60.16 56.70 87.74 86.67 84.22 31.66 70.11\\ngte-Qwen2-7B-instruct 60.25 61.42 56.92 85.79 86.58 83.04 31.35 70.24\\nSFR-Embedding-2 R 60.18 60.14 56.17 88.07 89.05 81.26 30.71 70.31\\nstella en 1.5B v5 61.01 61.21 57.69 88.07 87.63 84.51 31.49 71.19\\n**bge-en-icl (zero-shot)** 61.67 59.66 57.51 86.93 88.62 83.74 30.75 71.24\\n**bge-en-icl (few-shot)** 62.16 59.82 57.89 88.14 88.95 84.24 30.77 **71.67**\\n\\nw/ public data only\\n\\nE5-mistral-7b-instruct 52.78 60.38 47.78 88.47 76.80 83.77 31.90 64.56\\nGritLM-7B 53.10 61.30 48.90 86.90 77.00 82.80 29.40 64.70\\nLLM2Vec-Mistral-supervised 55.99 58.42 45.54 87.99 76.63 84.09 29.96 64.80\\n**bge-en-icl (zero-shot)** 59.59 56.85 42.61 87.87 75.47 83.30 29.52 64.67\\n**bge-en-icl (few-shot)** 60.08 56.67 46.55 88.51 77.31 83.69 30.68 **66.08**\\n\\nTable 1: Top MTEB leaderboard models as of August 27, 2024.\\n\\nDomain wiki web news healthcare law finance arxiv msmarco Avg.\\n# of datasets → 1 1 1 1 1 1 1 1 8\\n\\nw/ full data\\n\\nE5-mistral-7b-instruct 61.67 44.41 48.18 56.32 19.32 54.79 44.78 59.03 48.56\\nSFR-Embedding 63.46 51.27 52.21 58.76 23.27 56.94 47.75 58.99 51.58\\nNV-Embed-v1 62.84 50.42 51.46 58.53 20.65 49.89 46.10 60.27 50.02\\nLinq-Embed-Mistral 61.04 48.41 49.44 60.18 20.34 50.04 47.56 60.50 49.69\\ngte-Qwen2-7B-instruct 63.46 51.20 54.07 54.20 22.31 58.20 40.27 58.39 50.26\\nstella en 1.5B v5 61.99 50.88 53.87 58.81 23.22 57.26 44.81 61.38 51.53\\nbge-en-icl (zero-shot) 64.61 54.40 55.11 57.25 25.10 54.81 48.46 63.71 52.93\\nbge-en-icl (few-shot) 64.94 55.11 56.02 58.85 28.29 57.16 50.04 64.50 **54.36**\\n\\nw/ public data only\\n\\nbge-en-icl (zero-shot) 64.82 54.96 55.82 57.06 28.87 54.46 49.60 63.25 53.60\\nbge-en-icl (few-shot) 66.98 56.38 57.17 59.54 32.03 58.81 51.36 65.05 **55.92**\\n\\nTable 2: QA (en, nDCG@10) performance on AIR-Bench 24.04.\\n\\nthe zero-shot scenario is on par with, or slightly below, that of other models such as LLM2Vec\\nand GritLM. However, in the few-shot settings, our model show significant enhancements (↑1.41),\\nparticularly in the classification and clustering tasks that were not part of the training data. These\\nimprovements underscore the potential advantages of in-context learning, emphasizing its efficacy\\nin adapting to tasks beyond the direct scope of initial training parameters. Furthermore, in contrast\\nto training exclusively with public datasets, the utilization of full training data effectively familiarizes the model with these datasets. As a result, the model’s ability to generalize effectively is\\ncompromised, leading to only a modest improvement in few-shot settings (↑0.43).\\n\\n**AIR-Bench. The performance of our model is also evaluated using the AIR-Bench dataset. As**\\nillustrated in Tables 2 and 3, the model demonstrates superior performance compared to prior models\\nin both zero-shot and few-shot scenarios, excelling across qa and long-doc tasks. Notably, there is\\nno overlap between the training dataset and the evaluation data for these tasks, highlighting the\\nrobustness of the model in scenarios with limited prior exposure. In the few-shot setting, the model\\nexhibits significant improvements over the zero-shot scenario, achieving gains of 1.43 points in the\\nqa task and 1.08 points in the long-doc task. This improvement underscores the efficacy of in-context\\nlearning in enhancing the model’s generalization capabilities.\\n\\n|Task # of datasets →|Retr. Rerank. Clust. PairClass. Class. STS Summ. 15 4 11 3 12 10 1|Avg. 56|\\n|---|---|---|\\n\\n|E5-mistral-7b-instruct GritLM-7B SFR-Embedding Linq-Embed-Mistral voyage-large-2-instruct NV-Embed-v1 bge-multilingual-gemma2 stella en 400M v5 gte-Qwen2-7B-instruct SFR-Embedding-2 R stella en 1.5B v5 bge-en-icl (zero-shot) bge-en-icl (few-shot)|56.90 60.21 50.26 88.34 78.47 84.66 31.40 57.41 60.49 50.61 87.16 79.46 83.35 30.37 59.00 60.64 51.67 88.54 78.33 85.05 31.16 60.19 60.29 51.42 88.35 80.20 84.97 30.98 58.28 60.09 53.35 89.24 81.49 84.31 30.84 59.36 60.59 52.80 86.91 87.35 82.84 31.20 59.24 59.72 54.65 85.84 88.08 83.88 31.20 58.97 60.16 56.70 87.74 86.67 84.22 31.66 60.25 61.42 56.92 85.79 86.58 83.04 31.35 60.18 60.14 56.17 88.07 89.05 81.26 30.71 61.01 61.21 57.69 88.07 87.63 84.51 31.49 61.67 59.66 57.51 86.93 88.62 83.74 30.75 62.16 59.82 57.89 88.14 88.95 84.24 30.77|66.63 66.76 67.56 68.17 68.23 69.32 69.88 70.11 70.24 70.31 71.19 71.24 71.67|\\n|---|---|---|\\n\\n|E5-mistral-7b-instruct GritLM-7B LLM2Vec-Mistral-supervised bge-en-icl (zero-shot) bge-en-icl (few-shot)|52.78 60.38 47.78 88.47 76.80 83.77 31.90 53.10 61.30 48.90 86.90 77.00 82.80 29.40 55.99 58.42 45.54 87.99 76.63 84.09 29.96 59.59 56.85 42.61 87.87 75.47 83.30 29.52 60.08 56.67 46.55 88.51 77.31 83.69 30.68|64.56 64.70 64.80 64.67 66.08|\\n|---|---|---|\\n\\n|Domain # of datasets →|wiki web news healthcare law finance arxiv msmarco 1 1 1 1 1 1 1 1|Avg. 8|\\n|---|---|---|\\n\\n|E5-mistral-7b-instruct SFR-Embedding NV-Embed-v1 Linq-Embed-Mistral gte-Qwen2-7B-instruct stella en 1.5B v5 bge-en-icl (zero-shot) bge-en-icl (few-shot)|61.67 44.41 48.18 56.32 19.32 54.79 44.78 59.03 63.46 51.27 52.21 58.76 23.27 56.94 47.75 58.99 62.84 50.42 51.46 58.53 20.65 49.89 46.10 60.27 61.04 48.41 49.44 60.18 20.34 50.04 47.56 60.50 63.46 51.20 54.07 54.20 22.31 58.20 40.27 58.39 61.99 50.88 53.87 58.81 23.22 57.26 44.81 61.38 64.61 54.40 55.11 57.25 25.10 54.81 48.46 63.71 64.94 55.11 56.02 58.85 28.29 57.16 50.04 64.50|48.56 51.58 50.02 49.69 50.26 51.53 52.93 54.36|\\n|---|---|---|\\n\\n|bge-en-icl (zero-shot) bge-en-icl (few-shot)|64.82 54.96 55.82 57.06 28.87 54.46 49.60 63.25 66.98 56.38 57.17 59.54 32.03 58.81 51.36 65.05|53.60 55.92|\\n|---|---|---|\\n\\n\\n-----\\n\\nDomain arxiv book healthcare law Avg.\\n# of datasets → 4 2 5 4 15\\n\\nw/ full data\\n\\ntext-embedding-3-large 74.53 73.16 65.83 64.47 68.77\\nE5-mistral-7b-instruct 72.14 72.44 68.44 62.92 68.49\\nSFR-Embedding 72.79 72.41 67.94 64.83 69.00\\nNV-Embed-v1 77.65 75.49 72.38 69.55 73.45\\nLinq-Embed-Mistral 75.46 73.81 71.58 68.58 72.11\\ngte-Qwen2-7B-instruct 63.93 68.51 65.59 65.26 65.45\\nstella en 1.5B v5 73.17 74.38 70.02 69.32 71.25\\nbge-multilingual-gemma2 71.77 76.46 73.96 70.86 72.88\\nbge-en-icl (zero-shot) 78.30 78.21 73.65 67.09 73.75\\nbge-en-icl (few-shot) 79.63 79.36 74.80 67.79 **74.83**\\n\\nw/ public data only\\n\\nbge-en-icl (zero-shot) 79.73 78.66 72.88 70.59 74.86\\nbge-en-icl (few-shot) 79.82 80.37 74.60 71.66 **75.98**\\n\\nTable 3: Long-Doc (en, Recall@10) performance on AIR-Bench 24.04.\\n\\nHowever, when the model is trained exclusively using public data, it achieves better results compared to training with the full dataset. This could be attributed to the presence of an excessive\\namount of MTEB-related data, such as clustering and classification, within the full dataset. Such\\ndata might introduce the risk of overfitting, thereby potentially hampering the model’s generalization\\nperformance on the AIR-Bench dataset.\\n\\n4.3 IN-CONTEXT LEARNING\\n\\nTask Retr. Rerank. Clust. PairClass. Class. STS Summ. Avg.\\n# of datasets → 15 4 11 3 12 10 1 56\\n\\nw/ full data\\n\\nw/o in-context learning 59.11 57.02 42.60 87.99 76.27 83.93 30.50 64.83\\nw/ fix examples (zero-shot) 48.98 56.48 41.84 85.94 74.38 84.31 29.68 61.50\\nw/ fix examples (few-shot) 59.00 56.90 45.75 88.54 75.56 84.67 30.66 65.46\\nw/ in-batch examples (zero-shot) 59.59 56.85 42.61 87.87 75.47 83.30 29.52 64.67\\nw/ in-batch examples (few-shot) 60.08 56.67 46.55 88.51 77.31 83.69 30.68 **66.08**\\n\\nTable 4: Evaluation of various ICL strategies on the MTEB Benchmark.\\n\\nTo evaluate the impact of the ICL strategy, we conduct a series of ablation studies using the MTEB\\nbenchmark. In these studies, we compare the performance of models fine-tuned with the ICL strategy against those fine-tuned without it. Specifically, for ICL training, we employ two distinct training approaches: fixed examples and in-batch examples. In the fixed examples approach, each task\\nwas trained using three predetermined examples.\\n\\nIn Table 4, we present various results from our experiment. When the model is trained without ICL\\nstrategy, its average performance is 64.83. This performance is comparable to GritLM (Muennighoff\\net al., 2024), LLM2Vec (BehnamGhader et al., 2024), etc. When fixed examples are used during\\nICL training, there is a significant decline in zero-shot evaluation performance, with a decrease of\\n3.33 points. This decline is attributed to the model’s consistent exposure to specific training examples, which can impair its zero-shot capabilities. On the other hand, in few-shot scenarios, the\\nmodel demonstrates improved performance, exceeding zero-shot results by 3.96 points and surpassing models trained without ICL by 0.63 points. This also confirms the effectiveness of the ICL\\nstrategy.\\n\\nMeanwhile, the use of in-batch examples, where training may involve zero examples, has preserved\\nthe zero-shot capability of the model. There is a modest decrease of 0.16 points compared to the\\nmodel trained without ICL. Notably, in few-shot scenarios, the performance of the model employing\\nin-batch examples rises to 66.08 (↑1.25), indicating a robust improvement. Furthermore, when compared with the model utilizing fixed examples, the model trained with in-batch examples displays\\n\\n|Domain # of datasets →|arxiv book healthcare law 4 2 5 4|Avg. 15|\\n|---|---|---|\\n\\n|text-embedding-3-large E5-mistral-7b-instruct SFR-Embedding NV-Embed-v1 Linq-Embed-Mistral gte-Qwen2-7B-instruct stella en 1.5B v5 bge-multilingual-gemma2 bge-en-icl (zero-shot) bge-en-icl (few-shot)|74.53 73.16 65.83 64.47 72.14 72.44 68.44 62.92 72.79 72.41 67.94 64.83 77.65 75.49 72.38 69.55 75.46 73.81 71.58 68.58 63.93 68.51 65.59 65.26 73.17 74.38 70.02 69.32 71.77 76.46 73.96 70.86 78.30 78.21 73.65 67.09 79.63 79.36 74.80 67.79|68.77 68.49 69.00 73.45 72.11 65.45 71.25 72.88 73.75 74.83|\\n|---|---|---|\\n\\n|bge-en-icl (zero-shot) bge-en-icl (few-shot)|79.73 78.66 72.88 70.59 79.82 80.37 74.60 71.66|74.86 75.98|\\n|---|---|---|\\n\\n|Task # of datasets →|Retr. Rerank. Clust. PairClass. Class. STS Summ. 15 4 11 3 12 10 1|Avg. 56|\\n|---|---|---|\\n\\n|w/o in-context learning w/ fix examples (zero-shot) w/ fix examples (few-shot) w/ in-batch examples (zero-shot) w/ in-batch examples (few-shot)|59.11 57.02 42.60 87.99 76.27 83.93 30.50 48.98 56.48 41.84 85.94 74.38 84.31 29.68 59.00 56.90 45.75 88.54 75.56 84.67 30.66 59.59 56.85 42.61 87.87 75.47 83.30 29.52 60.08 56.67 46.55 88.51 77.31 83.69 30.68|64.83 61.50 65.46 64.67 66.08|\\n|---|---|---|\\n\\n\\n-----\\n\\nsuperior performance in tasks that diverge significantly from the training data, such as classification\\nand clustering tasks.\\n\\n4.4 ATTENTION\\n\\nTask Retr. Rerank. Clust. PairClass. Class. STS Summ. Avg.\\n# of datasets → 15 4 11 3 12 10 1 56\\n\\ncausal attention & last token pooling\\n\\nw/o in-context learning 59.11 57.02',\n",
       " 'answer': 'According to the paper \"Making Text Embedders Few-Shot Learners\", the use of full datasets in training models introduces inconsistencies and potential unfair comparisons because many models often rely on varying datasets. This can lead to inconsistent results, as different models may have been trained on different data, making it difficult to compare their performance fairly.\\n\\nAdditionally, the paper notes that \"many of these models do not disclose the specific datasets they use\", which further exacerbates the issue of unfair comparisons. By not disclosing the datasets used for training, it is unclear whether the results are due to the model\\'s architecture or the data it was trained on, making it challenging to draw meaningful conclusions about the model\\'s performance.\\n\\nThis highlights the importance of using public datasets and transparently reporting the methods and data used in model development to ensure fair comparisons and reliable results.'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_answer_cache[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can merge cleaned_question_cache and cleaned_answer_cache to get the same fields as the dataset, then will upload cleaned data to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(\n",
    "                    pd.DataFrame(cleaned_question_cache),\n",
    "                    pd.DataFrame(cleaned_answer_cache),\n",
    "                    on=['question', 'chunk'],\n",
    "                    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>source</th>\n",
       "      <th>source_type</th>\n",
       "      <th>chunk</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/Making_Text_Embedders_Few-Shot_Learner...</td>\n",
       "      <td>Making_Text_Embedders_Few-Shot_Learners_2409.1...</td>\n",
       "      <td>paper</td>\n",
       "      <td>-Clustering-{S2S/P2P},_\\nTwentyNewsgroups-Clus...</td>\n",
       "      <td>What is the purpose of using Low-Rank Adaptati...</td>\n",
       "      <td>According to the provided text, the purpose of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0  ../data/Making_Text_Embedders_Few-Shot_Learner...   \n",
       "\n",
       "                                              source source_type  \\\n",
       "0  Making_Text_Embedders_Few-Shot_Learners_2409.1...       paper   \n",
       "\n",
       "                                               chunk  \\\n",
       "0  -Clustering-{S2S/P2P},_\\nTwentyNewsgroups-Clus...   \n",
       "\n",
       "                                            question  \\\n",
       "0  What is the purpose of using Low-Rank Adaptati...   \n",
       "\n",
       "                                              answer  \n",
       "0  According to the provided text, the purpose of...  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = Dataset.from_dict(df_merged.to_dict(orient='list'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['filename', 'source', 'source_type', 'chunk', 'question', 'answer'],\n",
       "    num_rows: 440\n",
       "})"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['filename', 'source', 'source_type', 'chunk', 'question', 'answer'],\n",
       "    num_rows: 501\n",
       "})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = concatenate_datasets([merged_dataset, cleaned_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['filename', 'source', 'source_type', 'chunk', 'question', 'answer'],\n",
       "    num_rows: 941\n",
       "})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combined_dataset.data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.source.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "Making_Text_Embedders_Few-Shot_Learners_2409.15700v1                                                                                   96\n",
       "Time-MoE_Billion-Scale_Time_Series_Foundation_Models_with_Mixture_of\\n__Experts_2409.16040v1                                           77\n",
       "GraphLoRA: Structure-Aware Contrastive Low-Rank Adaptation for Cross-Graph Transfer Learning                                           68\n",
       "Taming Transformers for High Resolution Image Synthesis                                                                                59\n",
       "Using_LLM_for_Real-Time_Transcription_and_Summarization_of\\n__Doctor-Patient_Interactions_into_ePuskesmas_in_Indonesia_2409.17054v1    57\n",
       "Pruning_Multilingual_Large_Language_Models_for_Multilingual_Inference_2409.16911v1                                                     46\n",
       "Unsupervised Text Representation Learning via Instruction-Tuning for Zero-Shot Dense Retrieval                                         40\n",
       "AgentInstruct Toward Generative Teaching With Agentic Flows                                                                            38\n",
       "Towards a graph-based foundation model for network traffic analysis                                                                    32\n",
       "Visual Prompt Tuning for Generative Transfer Learning                                                                                  32\n",
       "EuroLLM_Multilingual_Language_Models_for_Europe_2409.16235v1                                                                           20\n",
       "FastGL: A GPU-Efficient Framework for Accelerating Sampling-Based GNN Training at Large Scale                                          19\n",
       "Can_CLIP_Count_Stars_An_Empirical_Study_on_Quantity_Bias_in_CLIP_2409.15035v1                                                          19\n",
       "MaskBit_Embedding-free_Image_Generation_via_Bit_Tokens_2409.16211v1                                                                    18\n",
       "Zero-Shot_Detection_of_LLM-Generated_Text_using_Token_Cohesiveness_2409.16914v1                                                        17\n",
       "VLEU_a_Method_for_Automatic_Evaluation_for_Generalizability_of\\n__Text-to-Image_Models_2409.14704v1                                    16\n",
       "MonoFormer_One_Transformer_for_Both_Diffusion_and_Autoregression_2409.16280v1                                                          13\n",
       "The_Credibility_Transformer_2409.16653v1                                                                                               12\n",
       "SynChart_Synthesizing_Charts_from_Language_Models_2409.16517v1                                                                          2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[df.answer.str.contains('NO ANSWER FOUND')].shape[0])\n",
    "df = df.loc[~df.answer.str.contains('NO ANSWER FOUND')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['question', 'chunk'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../pipeline/outputs/2024NOV2_combined_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(681, 6)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 89.76ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.94s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/CPSC532/cleaned_arxiv_qa_data/commit/d1066063c22b82c4aebfd077718613a9548f4dc3', commit_message='cleaned data', commit_description='Dataset contains 681 questons and answers from 19 different arxiv pdf files', oid='d1066063c22b82c4aebfd077718613a9548f4dc3', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset.push_to_hub(\n",
    "    repo_id=\"CPSC532/cleaned_arxiv_qa_data\",\n",
    "    config_name=\"2024NOV3\",\n",
    "    token=os.getenv(\"HUGGINGFACE_API_KEY\"),\n",
    "    commit_message=\"cleaned data\",\n",
    "    commit_description=\"Dataset contains 681 questons and answers from 19 different arxiv pdf files\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
