{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the responses generated from the finetuned responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from pydantic import BaseModel, SecretStr\n",
    "\n",
    "import os \n",
    "from typing import Any, AsyncGenerator, Dict, List, TypeVar, Union, cast, AsyncIterator\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of models / data:\n",
    "\n",
    "* model_1. Model that has been finetuned using questions with a source\n",
    "* model_2. Model that has been finetuned using questions without a asource\n",
    "\n",
    "* data_1. Test set with sources in question\n",
    "* data_2. Test set without sources in question \n",
    "\n",
    "* df11 = model_1 + data_1 \n",
    "* df12 = model_1 + data_2\n",
    "* df21 = model_2 + data_1\n",
    "* df22 = model_2 + data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_in_question_test_set = [\n",
    "    # Test Data set Sources in Question\n",
    "    (\n",
    "        'evaluations/2024NOV16_llama_3_1_8b_no_sources_in_question_test_output_model_finetuned_sources_in_question_r_16.csv',\n",
    "        'evaluations/2024NOV14_llama_3_1_8b_sources_in_question_test_output_model_finetuned_sources_in_question_r_16.csv',\n",
    "    ),\n",
    "    (\n",
    "        'evaluations/2024NOV16_llama_3_1_8b_no_sources_in_question_test_output_model_finetuned_sources_in_question_r_32.csv',\n",
    "        'evaluations/2024NOV14_llama_3_1_8b_sources_in_question_test_output_model_finetuned_sources_in_question_r_32.csv'\n",
    "    ),\n",
    "    (\n",
    "        'evaluations/2024NOV16_llama_3_1_8b_no_sources_in_question_test_output_model_finetuned_sources_in_question_r_64.csv',\n",
    "        'evaluations/2024NOV14_llama_3_1_8b_sources_in_question_test_output_model_finetuned_sources_in_question_r_64.csv',\n",
    "    ),\n",
    "    (\n",
    "        'evaluations/2024NOV16_llama_3_1_8b_no_sources_in_question_test_output_model_finetuned_sources_in_question_r_128.csv',\n",
    "        'evaluations/2024NOV14_llama_3_1_8b_sources_in_question_test_output_model_finetuned_sources_in_question_r_128.csv',\n",
    "    ),\n",
    "]\n",
    "\n",
    "no_sources_in_question_test_set = [\n",
    "    # Test Data Set No Sources in Question\n",
    "    (\n",
    "        'evaluations/2024NOV16_llama_3_1_8b_no_sources_in_question_test_output_model_finetuned_no_sources_in_question_r_16.csv',\n",
    "        'evaluations/2024NOV14_llama_3_1_8b_sources_in_question_test_output_model_finetuned_no_sources_in_question_r_16.csv',\n",
    "    ),\n",
    "    (\n",
    "        'evaluations/2024NOV16_llama_3_1_8b_no_sources_in_question_test_output_model_finetuned_no_sources_in_question_r_32.csv',\n",
    "        'evaluations/2024NOV14_llama_3_1_8b_sources_in_question_test_output_model_finetuned_no_sources_in_question_r_32.csv'\n",
    "    ),\n",
    "    (\n",
    "        'evaluations/2024NOV16_llama_3_1_8b_no_sources_in_question_test_output_model_finetuned_no_sources_in_question_r_64.csv',\n",
    "        'evaluations/2024NOV14_llama_3_1_8b_sources_in_question_test_output_model_finetuned_no_sources_in_question_r_64.csv',\n",
    "    ),\n",
    "    (\n",
    "        'evaluations/2024NOV16_llama_3_1_8b_no_sources_in_question_test_output_model_finetuned_no_sources_in_question_r_128.csv',\n",
    "        'evaluations/2024NOV14_llama_3_1_8b_sources_in_question_test_output_model_finetuned_no_sources_in_question_r_128.csv',\n",
    "    ),\n",
    "]\n",
    "\n",
    "r_list = [16, 32, 64, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dfs(df_pairs):\n",
    "    merged_dfs = []\n",
    "\n",
    "    for idx, (file1, file2) in enumerate(df_pairs):\n",
    "        df1 = pd.read_csv(file1)\n",
    "        df2 = pd.read_csv(file2)\n",
    "        # drop rows where true_answer column contains 'NO ANSWER FOUND\n",
    "        df1 = df1[~df1['true_answer'].str.contains('NO ANSWER FOUND')]\n",
    "        df2 = df2[~df2['true_answer'].str.contains('NO ANSWER FOUND')]\n",
    "        \n",
    "        suffix1 = 'finetuned_model_no_sources_in_question'\n",
    "        suffix2 = 'finetuned_model_sources_in_question'\n",
    "        df1 = df1.rename(columns={\"generated_answer\": f\"generated_answer_{suffix1}\"})\n",
    "        df2 = df2.rename(columns={\"generated_answer\": f\"generated_answer_{suffix2}\"})\n",
    "        \n",
    "        merged_df = pd.merge(df1, df2, on=[\"context\", \"question\", \"true_answer\"], suffixes=(f\"_{suffix1}\", f\"_{suffix2}\"))\n",
    "        merged_df['r'] = r_list[idx]\n",
    "        merged_dfs.append(merged_df.reset_index(drop=True))\n",
    "    return merged_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sources_in_question_test_set = merge_dfs(sources_in_question_test_set)\n",
    "merged_no_sources_in_question_test_set = merge_dfs(no_sources_in_question_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(161, 6)\n",
      "(161, 6)\n",
      "(161, 6)\n",
      "(161, 6)\n",
      "(180, 6)\n",
      "(180, 6)\n",
      "(180, 6)\n",
      "(180, 6)\n"
     ]
    }
   ],
   "source": [
    "len(merged_no_sources_in_question_test_set), len(merged_sources_in_question_test_set)\n",
    "for df in merged_sources_in_question_test_set:\n",
    "    print(df.shape)\n",
    "for df in merged_no_sources_in_question_test_set:\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df11 = pd.read_csv('evaluations/2024NOV14_llama_3_1_8b_sources_in_question_test_output_model_finetuned_sources_in_question.csv')\n",
    "# df12 = pd.read_csv('evaluations/2024NOV14_llama_3_1_8b_sources_in_question_test_output_model_finetuned_no_sources_in_question.csv')\n",
    "\n",
    "# df21 = pd.read_csv('evaluations/2024NOV16_llama_3_1_8b_no_sources_in_question_test_output_model_finetuned_sources_in_question.csv')\n",
    "# df22 = pd.read_csv('evaluations/2024NOV16_llama_3_1_8b_no_sources_in_question_test_output_model_finetuned_no_sources_in_question.csv')\n",
    "\n",
    "# df11 = df11[~df11['true_answer'].str.contains('NO ANSWER FOUND')]\n",
    "# df12 = df12[~df12['true_answer'].str.contains('NO ANSWER FOUND')]\n",
    "# df21 = df21[~df21['true_answer'].str.contains('NO ANSWER FOUND')]\n",
    "# df22 = df22[~df22['true_answer'].str.contains('NO ANSWER FOUND')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try BERTScore\n",
    "* Get cosine similarity between True Answer vs Generated Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embedding of generated answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"text-embedding-nomic-embed-text-v1.5@f32\"\n",
    "embedding_func = OpenAIEmbeddings(\n",
    "                                    model=embedding_model,\n",
    "                                    base_url=\"http://localhost:1234/v1\",\n",
    "                                    api_key=cast(SecretStr, os.getenv(\"LLM_CLIENT_API_KEY\", \"lm_studio\")),\n",
    "                                    check_embedding_ctx_length=False,  # https://github.com/langchain-ai/langchain/issues/21318\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>generated_answer_finetuned_model_no_sources_in_question</th>\n",
       "      <th>generated_answer_finetuned_model_sources_in_question</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the Role of Small Models in the LLM Er...</td>\n",
       "      <td>How does the transfer learning technique of we...</td>\n",
       "      <td>The transfer learning technique of weak-to-str...</td>\n",
       "      <td>Large language models can improve health-care ...</td>\n",
       "      <td>The transfer learning technique of weak-to-str...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  What is the Role of Small Models in the LLM Er...   \n",
       "\n",
       "                                            question  \\\n",
       "0  How does the transfer learning technique of we...   \n",
       "\n",
       "                                         true_answer  \\\n",
       "0  The transfer learning technique of weak-to-str...   \n",
       "\n",
       "  generated_answer_finetuned_model_no_sources_in_question  \\\n",
       "0  Large language models can improve health-care ...        \n",
       "\n",
       "  generated_answer_finetuned_model_sources_in_question   r  \n",
       "0  The transfer learning technique of weak-to-str...    16  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_no_sources_in_question_test_set[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>generated_answer_finetuned_model_no_sources_in_question</th>\n",
       "      <th>generated_answer_finetuned_model_sources_in_question</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the Role of Small Models in the LLM Er...</td>\n",
       "      <td>Why do computation-constrained environments fa...</td>\n",
       "      <td>According to the text, computation-constrained...</td>\n",
       "      <td>According to the paper \"What is the Role of Sm...</td>\n",
       "      <td>Model-based evaluation approaches like BERTSCO...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  What is the Role of Small Models in the LLM Er...   \n",
       "\n",
       "                                            question  \\\n",
       "0  Why do computation-constrained environments fa...   \n",
       "\n",
       "                                         true_answer  \\\n",
       "0  According to the text, computation-constrained...   \n",
       "\n",
       "  generated_answer_finetuned_model_no_sources_in_question  \\\n",
       "0  According to the paper \"What is the Role of Sm...        \n",
       "\n",
       "  generated_answer_finetuned_model_sources_in_question   r  \n",
       "0  Model-based evaluation approaches like BERTSCO...    16  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_sources_in_question_test_set[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio \n",
    "# async def get_embeddings_for_answer(df, generated_answer_column, true_answer_column):\n",
    "#     async def get_embedding(text):\n",
    "#         return await embedding_func.aembed_query(text)\n",
    "    \n",
    "#     df['true_answer_embedding'] = await asyncio.gather(*[get_embedding(text) for text in df[generated_answer_column]])\n",
    "#     df['generated_answer_embedding'] = await asyncio.gather(*[get_embedding(text) for text in df[generated_answer_column]])\n",
    "\n",
    "#     return df\n",
    "\n",
    "async def get_embeddings_for_answer(df, ):\n",
    "    async def get_embedding(text):\n",
    "        return await embedding_func.aembed_query(text)\n",
    "    \n",
    "    df['true_answer_embedding'] = await asyncio.gather(*[get_embedding(text) for text in df['true_answer']])\n",
    "    df['generated_answer_finetuned_model_no_sources_in_question_embedding'] = await asyncio.gather(*[get_embedding(text) for text in df['generated_answer_finetuned_model_no_sources_in_question']])\n",
    "    df['generated_answer_finetuned_model_sources_in_question_embedding'] = await asyncio.gather(*[get_embedding(text) for text in df['generated_answer_finetuned_model_sources_in_question']])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in merged_no_sources_in_question_test_set:\n",
    "    df = await get_embeddings_for_answer(df)\n",
    "for df in merged_sources_in_question_test_set:\n",
    "    df = await get_embeddings_for_answer(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df11 = await get_embeddings_for_answer(df11)\n",
    "# df12 = await get_embeddings_for_answer(df12)\n",
    "# df21 = await get_embeddings_for_answer(df21)\n",
    "# df22 = await get_embeddings_for_answer(df22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now generate cosine similarity between true answer and generated answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def calculate_cosine_similarity(df, col1, col2, resulting_col):\n",
    "    # Convert the embeddings to numpy arrays\n",
    "    embeddings1 = np.array(df[col1].tolist())\n",
    "    embeddings2 = np.array(df[col2].tolist())\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(embeddings1, embeddings2)\n",
    "    \n",
    "    # Since we are comparing each row with itself, we need the diagonal of the similarity matrix\n",
    "    df[resulting_col] = np.diag(similarities)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# # Calculate cosine similarity for df11\n",
    "# df11 = calculate_cosine_similarity(df11, 'true_answer_embedding', 'generated_answer_embedding')\n",
    "\n",
    "# # Calculate cosine similarity for df12\n",
    "# df12 = calculate_cosine_similarity(df12, 'true_answer_embedding', 'generated_answer_embedding')\n",
    "\n",
    "# # Calculate cosine similarity for df21\n",
    "# df21 = calculate_cosine_similarity(df21, 'true_answer_embedding', 'generated_answer_embedding')\n",
    "\n",
    "# # Calculate cosine similarity for df22\n",
    "# df22 = calculate_cosine_similarity(df22, 'true_answer_embedding', 'generated_answer_embedding')\n",
    "# print(\"Cosine Similarity Description for df11:\")\n",
    "# print(df11['cosine_similarity'].describe())\n",
    "\n",
    "# print(\"\\nCosine Similarity Description for df12:\")\n",
    "# print(df12['cosine_similarity'].describe())\n",
    "\n",
    "# print(\"\\nCosine Similarity Description for df21:\")\n",
    "# print(df21['cosine_similarity'].describe())\n",
    "\n",
    "# print(\"\\nCosine Similarity Description for df22:\")\n",
    "# print(df22['cosine_similarity'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>generated_answer_finetuned_model_no_sources_in_question</th>\n",
       "      <th>generated_answer_finetuned_model_sources_in_question</th>\n",
       "      <th>r</th>\n",
       "      <th>true_answer_embedding</th>\n",
       "      <th>generated_answer_finetuned_model_no_sources_in_question_embedding</th>\n",
       "      <th>generated_answer_finetuned_model_sources_in_question_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the Role of Small Models in the LLM Er...</td>\n",
       "      <td>How does the transfer learning technique of we...</td>\n",
       "      <td>The transfer learning technique of weak-to-str...</td>\n",
       "      <td>Large language models can improve health-care ...</td>\n",
       "      <td>The transfer learning technique of weak-to-str...</td>\n",
       "      <td>16</td>\n",
       "      <td>[-0.0012398749822750688, 0.04067772626876831, ...</td>\n",
       "      <td>[-0.01908096857368946, 0.09143900871276855, -0...</td>\n",
       "      <td>[0.012546073645353317, 0.005591242108494043, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  What is the Role of Small Models in the LLM Er...   \n",
       "\n",
       "                                            question  \\\n",
       "0  How does the transfer learning technique of we...   \n",
       "\n",
       "                                         true_answer  \\\n",
       "0  The transfer learning technique of weak-to-str...   \n",
       "\n",
       "  generated_answer_finetuned_model_no_sources_in_question  \\\n",
       "0  Large language models can improve health-care ...        \n",
       "\n",
       "  generated_answer_finetuned_model_sources_in_question   r  \\\n",
       "0  The transfer learning technique of weak-to-str...    16   \n",
       "\n",
       "                               true_answer_embedding  \\\n",
       "0  [-0.0012398749822750688, 0.04067772626876831, ...   \n",
       "\n",
       "  generated_answer_finetuned_model_no_sources_in_question_embedding  \\\n",
       "0  [-0.01908096857368946, 0.09143900871276855, -0...                  \n",
       "\n",
       "  generated_answer_finetuned_model_sources_in_question_embedding  \n",
       "0  [0.012546073645353317, 0.005591242108494043, -...              "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_no_sources_in_question_test_set[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in merged_no_sources_in_question_test_set:\n",
    "    df = calculate_cosine_similarity(df, 'true_answer_embedding', 'generated_answer_finetuned_model_no_sources_in_question_embedding', 'cosine_similarity_finetuned_model_no_sources_in_question')\n",
    "    df = calculate_cosine_similarity(df, 'true_answer_embedding', 'generated_answer_finetuned_model_sources_in_question_embedding', 'cosine_similarity_finetuned_model_sources_in_question')\n",
    "\n",
    "for df in merged_sources_in_question_test_set:\n",
    "    df = calculate_cosine_similarity(df, 'true_answer_embedding', 'generated_answer_finetuned_model_no_sources_in_question_embedding', 'cosine_similarity_finetuned_model_no_sources_in_question')\n",
    "    df = calculate_cosine_similarity(df, 'true_answer_embedding', 'generated_answer_finetuned_model_sources_in_question_embedding', 'cosine_similarity_finetuned_model_sources_in_question')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_no_sources_in_question_test_set = pd.concat(merged_no_sources_in_question_test_set)\n",
    "concat_sources_in_question_test_set = pd.concat(merged_sources_in_question_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((720, 11), (644, 11))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_no_sources_in_question_test_set.shape, concat_sources_in_question_test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">cosine_similarity_finetuned_model_no_sources_in_question</th>\n",
       "      <th colspan=\"8\" halign=\"left\">cosine_similarity_finetuned_model_sources_in_question</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>180.0</td>\n",
       "      <td>0.766992</td>\n",
       "      <td>0.126484</td>\n",
       "      <td>0.405244</td>\n",
       "      <td>0.676157</td>\n",
       "      <td>0.773191</td>\n",
       "      <td>0.872571</td>\n",
       "      <td>0.972202</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.698324</td>\n",
       "      <td>0.118764</td>\n",
       "      <td>0.387161</td>\n",
       "      <td>0.623971</td>\n",
       "      <td>0.693299</td>\n",
       "      <td>0.776967</td>\n",
       "      <td>0.969082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>180.0</td>\n",
       "      <td>0.711992</td>\n",
       "      <td>0.111065</td>\n",
       "      <td>0.414118</td>\n",
       "      <td>0.645691</td>\n",
       "      <td>0.705369</td>\n",
       "      <td>0.800605</td>\n",
       "      <td>0.940339</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.770957</td>\n",
       "      <td>0.123415</td>\n",
       "      <td>0.390667</td>\n",
       "      <td>0.677258</td>\n",
       "      <td>0.798602</td>\n",
       "      <td>0.880405</td>\n",
       "      <td>0.959573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>180.0</td>\n",
       "      <td>0.707680</td>\n",
       "      <td>0.122773</td>\n",
       "      <td>0.380262</td>\n",
       "      <td>0.627745</td>\n",
       "      <td>0.702928</td>\n",
       "      <td>0.790911</td>\n",
       "      <td>0.971977</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.701597</td>\n",
       "      <td>0.113621</td>\n",
       "      <td>0.406670</td>\n",
       "      <td>0.626663</td>\n",
       "      <td>0.685806</td>\n",
       "      <td>0.766781</td>\n",
       "      <td>0.952121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>180.0</td>\n",
       "      <td>0.719992</td>\n",
       "      <td>0.121062</td>\n",
       "      <td>0.391365</td>\n",
       "      <td>0.638582</td>\n",
       "      <td>0.721440</td>\n",
       "      <td>0.803980</td>\n",
       "      <td>0.970777</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.694940</td>\n",
       "      <td>0.112707</td>\n",
       "      <td>0.390009</td>\n",
       "      <td>0.626798</td>\n",
       "      <td>0.690050</td>\n",
       "      <td>0.772151</td>\n",
       "      <td>0.966145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cosine_similarity_finetuned_model_no_sources_in_question            \\\n",
       "                                                       count      mean   \n",
       "r                                                                        \n",
       "16                                               180.0        0.766992   \n",
       "32                                               180.0        0.711992   \n",
       "64                                               180.0        0.707680   \n",
       "128                                              180.0        0.719992   \n",
       "\n",
       "                                                                 \\\n",
       "          std       min       25%       50%       75%       max   \n",
       "r                                                                 \n",
       "16   0.126484  0.405244  0.676157  0.773191  0.872571  0.972202   \n",
       "32   0.111065  0.414118  0.645691  0.705369  0.800605  0.940339   \n",
       "64   0.122773  0.380262  0.627745  0.702928  0.790911  0.971977   \n",
       "128  0.121062  0.391365  0.638582  0.721440  0.803980  0.970777   \n",
       "\n",
       "    cosine_similarity_finetuned_model_sources_in_question                      \\\n",
       "                                                    count      mean       std   \n",
       "r                                                                               \n",
       "16                                               180.0     0.698324  0.118764   \n",
       "32                                               180.0     0.770957  0.123415   \n",
       "64                                               180.0     0.701597  0.113621   \n",
       "128                                              180.0     0.694940  0.112707   \n",
       "\n",
       "                                                       \n",
       "          min       25%       50%       75%       max  \n",
       "r                                                      \n",
       "16   0.387161  0.623971  0.693299  0.776967  0.969082  \n",
       "32   0.390667  0.677258  0.798602  0.880405  0.959573  \n",
       "64   0.406670  0.626663  0.685806  0.766781  0.952121  \n",
       "128  0.390009  0.626798  0.690050  0.772151  0.966145  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_no_sources_in_question_test_set.groupby(['r'])[['cosine_similarity_finetuned_model_no_sources_in_question', 'cosine_similarity_finetuned_model_sources_in_question']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">cosine_similarity_finetuned_model_no_sources_in_question</th>\n",
       "      <th colspan=\"8\" halign=\"left\">cosine_similarity_finetuned_model_sources_in_question</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>161.0</td>\n",
       "      <td>0.812035</td>\n",
       "      <td>0.112848</td>\n",
       "      <td>0.477632</td>\n",
       "      <td>0.730970</td>\n",
       "      <td>0.828531</td>\n",
       "      <td>0.911637</td>\n",
       "      <td>0.967758</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.766865</td>\n",
       "      <td>0.116356</td>\n",
       "      <td>0.449759</td>\n",
       "      <td>0.683271</td>\n",
       "      <td>0.779324</td>\n",
       "      <td>0.857104</td>\n",
       "      <td>0.965559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>161.0</td>\n",
       "      <td>0.763273</td>\n",
       "      <td>0.120425</td>\n",
       "      <td>0.460123</td>\n",
       "      <td>0.663333</td>\n",
       "      <td>0.765378</td>\n",
       "      <td>0.865303</td>\n",
       "      <td>0.974845</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.820086</td>\n",
       "      <td>0.116039</td>\n",
       "      <td>0.462697</td>\n",
       "      <td>0.745799</td>\n",
       "      <td>0.833053</td>\n",
       "      <td>0.921470</td>\n",
       "      <td>0.978988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>161.0</td>\n",
       "      <td>0.768057</td>\n",
       "      <td>0.112973</td>\n",
       "      <td>0.460343</td>\n",
       "      <td>0.685435</td>\n",
       "      <td>0.771880</td>\n",
       "      <td>0.850600</td>\n",
       "      <td>0.981624</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.764328</td>\n",
       "      <td>0.113376</td>\n",
       "      <td>0.498180</td>\n",
       "      <td>0.687600</td>\n",
       "      <td>0.767511</td>\n",
       "      <td>0.846940</td>\n",
       "      <td>0.978001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>161.0</td>\n",
       "      <td>0.774672</td>\n",
       "      <td>0.116225</td>\n",
       "      <td>0.518617</td>\n",
       "      <td>0.693755</td>\n",
       "      <td>0.761267</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>0.972599</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.766230</td>\n",
       "      <td>0.116481</td>\n",
       "      <td>0.504274</td>\n",
       "      <td>0.679415</td>\n",
       "      <td>0.769064</td>\n",
       "      <td>0.858330</td>\n",
       "      <td>0.983045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cosine_similarity_finetuned_model_no_sources_in_question            \\\n",
       "                                                       count      mean   \n",
       "r                                                                        \n",
       "16                                               161.0        0.812035   \n",
       "32                                               161.0        0.763273   \n",
       "64                                               161.0        0.768057   \n",
       "128                                              161.0        0.774672   \n",
       "\n",
       "                                                                 \\\n",
       "          std       min       25%       50%       75%       max   \n",
       "r                                                                 \n",
       "16   0.112848  0.477632  0.730970  0.828531  0.911637  0.967758   \n",
       "32   0.120425  0.460123  0.663333  0.765378  0.865303  0.974845   \n",
       "64   0.112973  0.460343  0.685435  0.771880  0.850600  0.981624   \n",
       "128  0.116225  0.518617  0.693755  0.761267  0.877907  0.972599   \n",
       "\n",
       "    cosine_similarity_finetuned_model_sources_in_question                      \\\n",
       "                                                    count      mean       std   \n",
       "r                                                                               \n",
       "16                                               161.0     0.766865  0.116356   \n",
       "32                                               161.0     0.820086  0.116039   \n",
       "64                                               161.0     0.764328  0.113376   \n",
       "128                                              161.0     0.766230  0.116481   \n",
       "\n",
       "                                                       \n",
       "          min       25%       50%       75%       max  \n",
       "r                                                      \n",
       "16   0.449759  0.683271  0.779324  0.857104  0.965559  \n",
       "32   0.462697  0.745799  0.833053  0.921470  0.978988  \n",
       "64   0.498180  0.687600  0.767511  0.846940  0.978001  \n",
       "128  0.504274  0.679415  0.769064  0.858330  0.983045  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_sources_in_question_test_set.groupby(['r'])[['cosine_similarity_finetuned_model_no_sources_in_question', 'cosine_similarity_finetuned_model_sources_in_question']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_sources_in_question_test_set.to_csv('evaluations/sources_in_question_test_set_results.csv', index=False)\n",
    "concat_no_sources_in_question_test_set.to_csv('evaluations/no_sources_in_question_test_set_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chatgpt.com/c/674509ef-9e80-8002-8e80-992c0ae46ee9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owen/anaconda3/envs/gen-ai/lib/python3.12/enum.py:315: RuntimeWarning: coroutine 'get_embeddings_for_answer' was never awaited\n",
      "  or not issubclass(enum_class, Flag)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bertscore(df, reference_col, prediction_col, suffix, lang='en', model_type='roberta-large', device=device, batch_size=16):\n",
    "    P_list, R_list, F1_list = [], [], []\n",
    "    # for i in range(0, len(df), batch_size):\n",
    "    #     batch_df = df.iloc[i:i+batch_size]\n",
    "    P, R, F1 = score(df[prediction_col].tolist(), df[reference_col].tolist(), lang=lang, model_type=model_type, device=device, batch_size=batch_size)\n",
    "    P_list.extend(P.tolist())\n",
    "    R_list.extend(R.tolist())\n",
    "    F1_list.extend(F1.tolist())\n",
    "    df['bertscore_P'+ suffix] = P_list\n",
    "    df['bertscore_R'+ suffix] = R_list\n",
    "    df['bertscore_F1'+ suffix] = F1_list\n",
    "    return df\n",
    "\n",
    "# Calculate BERTScore for df11\n",
    "# df11 = calculate_bertscore(df11, 'true_answer', 'generated_answer')\n",
    "\n",
    "# # Calculate BERTScore for df12\n",
    "# df12 = calculate_bertscore(df12, 'true_answer', 'generated_answer')\n",
    "\n",
    "# # Calculate BERTScore for df21\n",
    "# df21 = calculate_bertscore(df21, 'true_answer', 'generated_answer')\n",
    "\n",
    "# # Calculate BERTScore for df22\n",
    "# df22 = calculate_bertscore(df22, 'true_answer', 'generated_answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "concat_sources_in_question_test_set = calculate_bertscore(concat_sources_in_question_test_set, 'true_answer', 'generated_answer_finetuned_model_sources_in_question', '_finetuned_model_sources_in_question')\n",
    "concat_sources_in_question_test_set = calculate_bertscore(concat_sources_in_question_test_set, 'true_answer', 'generated_answer_finetuned_model_no_sources_in_question', '_finetuned_model_no_sources_in_question')\n",
    "\n",
    "concat_no_sources_in_question_test_set = calculate_bertscore(concat_no_sources_in_question_test_set, 'true_answer', 'generated_answer_finetuned_model_sources_in_question', '_finetuned_model_sources_in_question')\n",
    "concat_no_sources_in_question_test_set = calculate_bertscore(concat_no_sources_in_question_test_set, 'true_answer', 'generated_answer_finetuned_model_no_sources_in_question', '_finetuned_model_no_sources_in_question')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>generated_answer_finetuned_model_no_sources_in_question</th>\n",
       "      <th>generated_answer_finetuned_model_sources_in_question</th>\n",
       "      <th>r</th>\n",
       "      <th>true_answer_embedding</th>\n",
       "      <th>generated_answer_finetuned_model_no_sources_in_question_embedding</th>\n",
       "      <th>generated_answer_finetuned_model_sources_in_question_embedding</th>\n",
       "      <th>cosine_similarity_finetuned_model_no_sources_in_question</th>\n",
       "      <th>cosine_similarity_finetuned_model_sources_in_question</th>\n",
       "      <th>bertscore_P_finetuned_model_sources_in_question</th>\n",
       "      <th>bertscore_R_finetuned_model_sources_in_question</th>\n",
       "      <th>bertscore_F1_finetuned_model_sources_in_question</th>\n",
       "      <th>bertscore_P_finetuned_model_no_sources_in_question</th>\n",
       "      <th>bertscore_R_finetuned_model_no_sources_in_question</th>\n",
       "      <th>bertscore_F1_finetuned_model_no_sources_in_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the Role of Small Models in the LLM Er...</td>\n",
       "      <td>Why do computation-constrained environments fa...</td>\n",
       "      <td>According to the text, computation-constrained...</td>\n",
       "      <td>According to the paper \"What is the Role of Sm...</td>\n",
       "      <td>Model-based evaluation approaches like BERTSCO...</td>\n",
       "      <td>16</td>\n",
       "      <td>[0.017459962517023087, 0.12867099046707153, -0...</td>\n",
       "      <td>[-0.022837018594145775, 0.08393042534589767, -...</td>\n",
       "      <td>[-0.0029367581009864807, 0.06684571504592896, ...</td>\n",
       "      <td>0.783405</td>\n",
       "      <td>0.791908</td>\n",
       "      <td>0.871762</td>\n",
       "      <td>0.885544</td>\n",
       "      <td>0.878599</td>\n",
       "      <td>0.854539</td>\n",
       "      <td>0.865411</td>\n",
       "      <td>0.859941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  What is the Role of Small Models in the LLM Er...   \n",
       "\n",
       "                                            question  \\\n",
       "0  Why do computation-constrained environments fa...   \n",
       "\n",
       "                                         true_answer  \\\n",
       "0  According to the text, computation-constrained...   \n",
       "\n",
       "  generated_answer_finetuned_model_no_sources_in_question  \\\n",
       "0  According to the paper \"What is the Role of Sm...        \n",
       "\n",
       "  generated_answer_finetuned_model_sources_in_question   r  \\\n",
       "0  Model-based evaluation approaches like BERTSCO...    16   \n",
       "\n",
       "                               true_answer_embedding  \\\n",
       "0  [0.017459962517023087, 0.12867099046707153, -0...   \n",
       "\n",
       "  generated_answer_finetuned_model_no_sources_in_question_embedding  \\\n",
       "0  [-0.022837018594145775, 0.08393042534589767, -...                  \n",
       "\n",
       "  generated_answer_finetuned_model_sources_in_question_embedding  \\\n",
       "0  [-0.0029367581009864807, 0.06684571504592896, ...               \n",
       "\n",
       "   cosine_similarity_finetuned_model_no_sources_in_question  \\\n",
       "0                                           0.783405          \n",
       "\n",
       "   cosine_similarity_finetuned_model_sources_in_question  \\\n",
       "0                                           0.791908       \n",
       "\n",
       "   bertscore_P_finetuned_model_sources_in_question  \\\n",
       "0                                         0.871762   \n",
       "\n",
       "   bertscore_R_finetuned_model_sources_in_question  \\\n",
       "0                                         0.885544   \n",
       "\n",
       "   bertscore_F1_finetuned_model_sources_in_question  \\\n",
       "0                                          0.878599   \n",
       "\n",
       "   bertscore_P_finetuned_model_no_sources_in_question  \\\n",
       "0                                           0.854539    \n",
       "\n",
       "   bertscore_R_finetuned_model_no_sources_in_question  \\\n",
       "0                                           0.865411    \n",
       "\n",
       "   bertscore_F1_finetuned_model_no_sources_in_question  \n",
       "0                                           0.859941    "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_sources_in_question_test_set.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "def print_bertscore_descriptions_grouped_by_r(df, name):\n",
    "    grouped = df.groupby('r')\n",
    "    \n",
    "    for r_value, group in grouped:\n",
    "        desc_P_sources = group['bertscore_P_finetuned_model_sources_in_question'].describe()\n",
    "        desc_R_sources = group['bertscore_R_finetuned_model_sources_in_question'].describe()\n",
    "        desc_F1_sources = group['bertscore_F1_finetuned_model_sources_in_question'].describe()\n",
    "\n",
    "        desc_P_no_sources = group['bertscore_P_finetuned_model_no_sources_in_question'].describe()\n",
    "        desc_R_no_sources = group['bertscore_R_finetuned_model_no_sources_in_question'].describe()\n",
    "        desc_F1_no_sources = group['bertscore_F1_finetuned_model_no_sources_in_question'].describe()\n",
    "\n",
    "        print(f\"BERTScore Description for {name} (r={r_value}, Finetuned model Sources in Question):\")\n",
    "        print(f\"{'Metric':<10} {'Count':<10} {'Mean':<10} {'Std':<10} {'Min':<10} {'25%':<10} {'50%':<10} {'75%':<10} {'Max':<10}\")\n",
    "        print(f\"{'P':<10} {desc_P_sources['count']:<10.2f} {desc_P_sources['mean']:<10.4f} {desc_P_sources['std']:<10.4f} {desc_P_sources['min']:<10.4f} {desc_P_sources['25%']:<10.4f} {desc_P_sources['50%']:<10.4f} {desc_P_sources['75%']:<10.4f} {desc_P_sources['max']:<10.4f}\")\n",
    "        print(f\"{'R':<10} {desc_R_sources['count']:<10.2f} {desc_R_sources['mean']:<10.4f} {desc_R_sources['std']:<10.4f} {desc_R_sources['min']:<10.4f} {desc_R_sources['25%']:<10.4f} {desc_R_sources['50%']:<10.4f} {desc_R_sources['75%']:<10.4f} {desc_R_sources['max']:<10.4f}\")\n",
    "        print(f\"{'F1':<10} {desc_F1_sources['count']:<10.2f} {desc_F1_sources['mean']:<10.4f} {desc_F1_sources['std']:<10.4f} {desc_F1_sources['min']:<10.4f} {desc_F1_sources['25%']:<10.4f} {desc_F1_sources['50%']:<10.4f} {desc_F1_sources['75%']:<10.4f} {desc_F1_sources['max']:<10.4f}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        print(f\"BERTScore Description for {name} (r={r_value}, Finetuned model No Sources in Question):\")\n",
    "        print(f\"{'Metric':<10} {'Count':<10} {'Mean':<10} {'Std':<10} {'Min':<10} {'25%':<10} {'50%':<10} {'75%':<10} {'Max':<10}\")\n",
    "        print(f\"{'P':<10} {desc_P_no_sources['count']:<10.2f} {desc_P_no_sources['mean']:<10.4f} {desc_P_no_sources['std']:<10.4f} {desc_P_no_sources['min']:<10.4f} {desc_P_no_sources['25%']:<10.4f} {desc_P_no_sources['50%']:<10.4f} {desc_P_no_sources['75%']:<10.4f} {desc_P_no_sources['max']:<10.4f}\")\n",
    "        print(f\"{'R':<10} {desc_R_no_sources['count']:<10.2f} {desc_R_no_sources['mean']:<10.4f} {desc_R_no_sources['std']:<10.4f} {desc_R_no_sources['min']:<10.4f} {desc_R_no_sources['25%']:<10.4f} {desc_R_no_sources['50%']:<10.4f} {desc_R_no_sources['75%']:<10.4f} {desc_R_no_sources['max']:<10.4f}\")\n",
    "        print(f\"{'F1':<10} {desc_F1_no_sources['count']:<10.2f} {desc_F1_no_sources['mean']:<10.4f} {desc_F1_no_sources['std']:<10.4f} {desc_F1_no_sources['min']:<10.4f} {desc_F1_no_sources['25%']:<10.4f} {desc_F1_no_sources['50%']:<10.4f} {desc_F1_no_sources['75%']:<10.4f} {desc_F1_no_sources['max']:<10.4f}\")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Description for Sources in Question Test Set (r=16, Finetuned model Sources in Question):\n",
      "Metric     Count      Mean       Std        Min        25%        50%        75%        Max       \n",
      "P          161.00     0.8394     0.0263     0.7766     0.8221     0.8409     0.8566     0.8981    \n",
      "R          161.00     0.8383     0.0269     0.7686     0.8203     0.8358     0.8568     0.9080    \n",
      "F1         161.00     0.8387     0.0248     0.7911     0.8206     0.8383     0.8564     0.8995    \n",
      "\n",
      "\n",
      "BERTScore Description for Sources in Question Test Set (r=16, Finetuned model No Sources in Question):\n",
      "Metric     Count      Mean       Std        Min        25%        50%        75%        Max       \n",
      "P          161.00     0.8427     0.0318     0.6351     0.8250     0.8459     0.8627     0.9173    \n",
      "R          161.00     0.8444     0.0273     0.7718     0.8287     0.8465     0.8614     0.9112    \n",
      "F1         161.00     0.8434     0.0272     0.6981     0.8260     0.8463     0.8611     0.9142    \n",
      "\n",
      "\n",
      "BERTScore Description for Sources in Question Test Set (r=32, Finetuned model Sources in Question):\n",
      "Metric     Count      Mean       Std        Min        25%        50%        75%        Max       \n",
      "P          161.00     0.8481     0.0295     0.7281     0.8295     0.8514     0.8649     0.9313    \n",
      "R          161.00     0.8470     0.0264     0.7883     0.8295     0.8477     0.8638     0.9109    \n",
      "F1         161.00     0.8474     0.0259     0.7618     0.8292     0.8474     0.8636     0.9129    \n",
      "\n",
      "\n",
      "BERTScore Description for Sources in Question Test Set (r=32, Finetuned model No Sources in Question):\n",
      "Metric     Count      Mean       Std        Min        25%        50%        75%        Max       \n",
      "P          161.00     0.8369     0.0258     0.7825     0.8164     0.8386     0.8546     0.9015    \n",
      "R          161.00     0.8336     0.0265     0.7730     0.8156     0.8330     0.8480     0.9146    \n",
      "F1         161.00     0.8351     0.0244     0.7870     0.8176     0.8329     0.8485     0.9051    \n",
      "\n",
      "\n",
      "BERTScore Description for Sources in Question Test Set (r=64, Finetuned model Sources in Question):\n",
      "Metric     Count      Mean       Std        Min        25%        50%        75%        Max       \n",
      "P          161.00     0.8357     0.0272     0.7713     0.8161     0.8345     0.8503     0.9162    \n",
      "R          161.00     0.8357     0.0278     0.7750     0.8163     0.8311     0.8545     0.9119    \n",
      "F1         161.00     0.8356     0.0256     0.7876     0.8203     0.8311     0.8520     0.9090    \n",
      "\n",
      "\n",
      "BERTScore Description for Sources in Question Test Set (r=64, Finetuned model No Sources in Question):\n",
      "Metric     Count      Mean       Std        Min        25%        50%        75%        Max       \n",
      "P          161.00     0.8371     0.0267     0.7604     0.8206     0.8361     0.8535     0.9121    \n",
      "R          161.00     0.8350     0.0256     0.7647     0.8196     0.8330     0.8487     0.9040    \n",
      "F1         161.00     0.8359     0.0241     0.7625     0.8206     0.8333     0.8482     0.9074    \n",
      "\n",
      "\n",
      "BERTScore Description for Sources in Question Test Set (r=128, Finetuned model Sources in Question):\n",
      "Metric     Count      Mean       Std        Min        25%        50%        75%        Max       \n",
      "P          161.00     0.8390     0.0280     0.7744     0.8196     0.8404     0.8597     0.9258    \n",
      "R          161.00     0.8376     0.0261     0.7825     0.8200     0.8338     0.8565     0.9087    \n",
      "F1         161.00     0.8382     0.0254     0.7874     0.8187     0.8382     0.8570     0.9034    \n",
      "\n",
      "\n",
      "BERTScore Description for Sources in Question Test Set (r=128, Finetuned model No Sources in Question):\n",
      "Metric     Count      Mean       Std        Min        25%        50%        75%        Max       \n",
      "P          161.00     0.8403     0.0279     0.7742     0.8229     0.8374     0.8592     0.9113    \n",
      "R          161.00     0.8380     0.0260     0.7726     0.8204     0.8374     0.8526     0.9079    \n",
      "F1         161.00     0.8390     0.0251     0.7857     0.8196     0.8375     0.8540     0.9091    \n",
      "\n",
      "\n",
      "BERTScore Description for No Sources in Question Test Set (r=16, Finetuned model Sources in Question):\n",
      "Metric     Count      Mean       Std        Min        25%        50%        75%        Max       \n",
      "P          180.00     0.8250     0.0271     0.7249     0.8100     0.8257     0.8400     0.8928    \n",
      "R          180.00     0.8229     0.0233     0.7225     0.8100     0.8215     0.8371     0.9032    \n",
      "F1         180.00     0.8238     0.0223     0.7237     0.8118     0.8227     0.8356     0.8978    \n",
      "\n",
      "\n",
      "BERTScore Description for No Sources in Question Test Set (r=16, Finetuned model No Sources in Question):\n",
      "Metric     Count      Mean       Std        Min        25%        50%        75%        Max       \n",
      "P          180.00     0.8352     0.0300     0.7097     0.8176     0.8365     0.8531     0.9212    \n",
      "R          180.00     0.8374     0.0274     0.7316     0.8185     0.8363     0.8556     0.9024    \n",
      "F1         180.00     0.8360     0.0256     0.7266     0.8211     0.8318     0.8522     0.9117    \n",
      "\n",
      "\n",
      "BERTScore Description for No Sources in Question Test Set (r=32, Finetuned model Sources in Question):\n",
      "Metric     Count      Mean       Std        Min        25%        50%        75%        Max       \n",
      "P          180.00     0.8343     0.0263     0.7328     0.8203     0.8348     0.8519     0.9000    \n",
      "R          180.00     0.8365     0.0272     0.7266     0.8195     0.8373     0.8532     0.9013    \n",
      "F1         180.00     0.8352     0.0236     0.7363     0.8219     0.8348     0.8494     0.8918    \n",
      "\n",
      "\n",
      "BERTScore Description for No Sources in Question Test Set (r=32, Finetuned model No Sources in Question):\n",
      "Metric     Count      Mean       Std        Min        25%        50%        75%        Max       \n",
      "P          180.00     0.8264     0.0270     0.7295     0.8087     0.8288     0.8417     0.9109    \n",
      "R          180.00     0.8249     0.0243     0.7232     0.8092     0.8235     0.8371     0.9088    \n",
      "F1         180.00     0.8254     0.0220     0.7263     0.8123     0.8244     0.8378     0.9098    \n",
      "\n",
      "\n",
      "BERTScore Description for No Sources in Question Test Set (r=64, Finetuned model Sources in Question):\n",
      "Metric     Count      Mean       Std        Min        25%        50%        75%        Max       \n",
      "P          180.00     0.8219     0.0279     0.7298     0.8092     0.8226     0.8365     0.8946    \n",
      "R          180.00     0.8237     0.0214     0.7307     0.8102     0.8231     0.8371     0.8796    \n",
      "F1         180.00     0.8226     0.0216     0.7313     0.8111     0.8217     0.8330     0.8817    \n",
      "\n",
      "\n",
      "BERTScore Description for No Sources in Question Test Set (r=64, Finetuned model No Sources in Question):\n",
      "Metric     Count      Mean       Std        Min        25%        50%        75%        Max       \n",
      "P          180.00     0.8245     0.0277     0.7253     0.8078     0.8257     0.8424     0.8894    \n",
      "R          180.00     0.8255     0.0239     0.7299     0.8104     0.8260     0.8408     0.8934    \n",
      "F1         180.00     0.8248     0.0228     0.7276     0.8104     0.8232     0.8388     0.8891    \n",
      "\n",
      "\n",
      "BERTScore Description for No Sources in Question Test Set (r=128, Finetuned model Sources in Question):\n",
      "Metric     Count      Mean       Std        Min        25%        50%        75%        Max       \n",
      "P          180.00     0.8239     0.0245     0.7322     0.8104     0.8258     0.8370     0.9066    \n",
      "R          180.00     0.8215     0.0226     0.7344     0.8078     0.8210     0.8352     0.9053    \n",
      "F1         180.00     0.8225     0.0202     0.7333     0.8103     0.8207     0.8328     0.9027    \n",
      "\n",
      "\n",
      "BERTScore Description for No Sources in Question Test Set (r=128, Finetuned model No Sources in Question):\n",
      "Metric     Count      Mean       Std        Min        25%        50%        75%        Max       \n",
      "P          180.00     0.8266     0.0282     0.7259     0.8130     0.8271     0.8407     0.9042    \n",
      "R          180.00     0.8281     0.0232     0.7347     0.8138     0.8283     0.8426     0.8914    \n",
      "F1         180.00     0.8271     0.0222     0.7303     0.8145     0.8239     0.8384     0.8893    \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_bertscore_descriptions_grouped_by_r(concat_sources_in_question_test_set, \"Sources in Question Test Set\")\n",
    "print_bertscore_descriptions_grouped_by_r(concat_no_sources_in_question_test_set, \"No Sources in Question Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bertscore_descriptions(df, name):\n",
    "    desc_P = df['bertscore_P'].describe()\n",
    "    desc_R = df['bertscore_R'].describe()\n",
    "    desc_F1 = df['bertscore_F1'].describe()\n",
    "    \n",
    "    print(f\"BERTScore Description for {name}:\")\n",
    "    print(f\"{'Metric':<10} {'Count':<10} {'Mean':<10} {'Std':<10} {'Min':<10} {'25%':<10} {'50%':<10} {'75%':<10} {'Max':<10}\")\n",
    "    print(f\"{'P':<10} {desc_P['count']:<10.2f} {desc_P['mean']:<10.4f} {desc_P['std']:<10.4f} {desc_P['min']:<10.4f} {desc_P['25%']:<10.4f} {desc_P['50%']:<10.4f} {desc_P['75%']:<10.4f} {desc_P['max']:<10.4f}\")\n",
    "    print(f\"{'R':<10} {desc_R['count']:<10.2f} {desc_R['mean']:<10.4f} {desc_R['std']:<10.4f} {desc_R['min']:<10.4f} {desc_R['25%']:<10.4f} {desc_R['50%']:<10.4f} {desc_R['75%']:<10.4f} {desc_R['max']:<10.4f}\")\n",
    "    print(f\"{'F1':<10} {desc_F1['count']:<10.2f} {desc_F1['mean']:<10.4f} {desc_F1['std']:<10.4f} {desc_F1['min']:<10.4f} {desc_F1['25%']:<10.4f} {desc_F1['50%']:<10.4f} {desc_F1['75%']:<10.4f} {desc_F1['max']:<10.4f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# print_bertscore_descriptions(df11, \"df11\")\n",
    "# print_bertscore_descriptions(df12, \"df12\")\n",
    "# print_bertscore_descriptions(df21, \"df21\")\n",
    "# print_bertscore_descriptions(df22, \"df22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
