{
  "pipeline_config": {
    "config_name": "no_sources",
    "include_source": false,
    "max_documents": 50,
    "max_questions_per_chunk": 10,
    "test_ratio": 0.1,
    "document_chunk_size": 5000,
    "document_chunk_overlap": 100,
    "rag_chunk_size": 500,
    "rag_chunk_overlap": 100,
    "batch_size": 10,
    "llm_model": "gpt-4o-mini",
    "embedding_model": "text-embedding-nomic-embed-text-v1.5@f32"
  },
  "finetune_config": {
    "output_model_name": "eval_no_sources_finetune",
    "base_model_name": "unsloth/Llama-3.2-3B-Instruct",
    "load_in_4bit": true,
    "max_seq_length": 16000,
    "batch_size": 1,
    "gradient_accumulation_steps": 1,
    "packing": true,
    "num_train_epochs": 10,
    "quantization_methods": ["q4_k_m"]
  },
  "judge_config": {
    "temperature": 0.0,
    "rag_k": 3,
    "base_model": {
        "max_replica": 5,
        "scale_to_zero_timeout": 30,
        "parallelism": 10,
        "model_repository": "unsloth/Llama-3.2-3B-Instruct-GGUF",
        "gguf_file": "Llama-3.2-3B-Instruct-Q4_K_M.gguf",
        "endpoint_name": "llama-32-3b-instruct-q4"
    },
    "finetuned_model": {
        "max_replica": 5,
        "scale_to_zero_timeout": 30,
        "parallelism": 10,
        "model_repository": "CPSC532/eval_no_sources_finetune",
        "gguf_file": "unsloth.Q4_K_M.gguf",
        "endpoint_name": "eval-no-sources-finetune-q4"
    }
  }
}
