{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jfVz96Auw6a"
      },
      "source": [
        "## Using Unsloth to finetune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXimWVZ7-iRI"
      },
      "source": [
        "## Install Prerequisite Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AM4qXDrpvdl8",
        "outputId": "a2c1330f-d09e-49cf-a8ef-af4279411f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (24.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (5.10.4)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat) (5.7.2)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbformat) (5.7.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (0.21.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.6)\n",
            "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-iwdy4nkx/unsloth_873541d9499a436886688ed685e584ad\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-iwdy4nkx/unsloth_873541d9499a436886688ed685e584ad\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit 228b3cf46ec4401b81194267ed0091eb62a56c6b\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unsloth_zoo>=2024.11.1 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading unsloth_zoo-2024.11.7-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.2)\n",
            "Collecting tyro (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading tyro-0.9.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: transformers>=4.46.1 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.46.2)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\n",
            "Collecting protobuf<4.0.0 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.26.2)\n",
            "Collecting hf_transfer (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting bitsandbytes>=0.43.3 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.11.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.20.3)\n",
            "Collecting triton (from unsloth_zoo>=2024.11.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2024.11.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.1.1)\n",
            "Collecting trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth_zoo>=2024.11.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading trl-0.12.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2024.11.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.13.2)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo>=2024.11.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading cut_cross_entropy-24.11.4-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2024.11.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.0.0)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.2)\n",
            "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2024.11.7-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.2-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m112.1/112.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading trl-0.12.1-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m310.9/310.9 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cut_cross_entropy-24.11.4-py3-none-any.whl (22 kB)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: unsloth\n",
            "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unsloth: filename=unsloth-2024.11.9-py3-none-any.whl size=166001 sha256=ca6e255b3c18b9fc36302473c606c6db8f914529445daf0de9df4c703e440d53\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-p3lkg4lx/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
            "Successfully built unsloth\n",
            "Installing collected packages: unsloth, triton, shtab, protobuf, hf_transfer, tyro, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.44.1 cut_cross_entropy-24.11.4 hf_transfer-0.1.8 protobuf-3.20.3 shtab-1.7.1 triton-3.1.0 trl-0.12.1 tyro-0.9.2 unsloth-2024.11.9 unsloth_zoo-2024.11.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "66206925fd51490180bc2613d00e0d74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xformers\n",
            "  Downloading xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting trl<0.9.0\n",
            "  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Downloading xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl (16.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers, trl\n",
            "  Attempting uninstall: trl\n",
            "    Found existing installation: trl 0.12.1\n",
            "    Uninstalling trl-0.12.1:\n",
            "      Successfully uninstalled trl-0.12.1\n",
            "Successfully installed trl-0.8.6 xformers-0.0.28.post3\n"
          ]
        }
      ],
      "source": [
        "# This is necessary for colab\n",
        "!pip install python-dotenv\n",
        "!pip install datasets\n",
        "!pip install plotly\n",
        "!pip install nbformat\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrRIVF20-nmp"
      },
      "source": [
        "## Load `.env`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bEjj4o6luw6a",
        "outputId": "20fd3471-06a4-473e-a215-209399057ce3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "from dotenv import find_dotenv, load_dotenv\n",
        "\n",
        "# The file .env should contain the line (where xxxxxxx is replaced):\n",
        "# export HUGGINGFACE_API_KEY=xxxxxxx\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i-raT4C-xbX"
      },
      "source": [
        "## Important Global Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-7S3GvWl-6a1"
      },
      "outputs": [],
      "source": [
        "FINETUNING_DATASET_NAME=\"CPSC532/arxiv_qa_data\"\n",
        "\n",
        "with open('config.json') as f:\n",
        "    config = json.load(f)\n",
        "finetune_config = config[\"finetune_config\"]\n",
        "pipeline_config = config[\"pipeline_config\"]\n",
        "\n",
        "CONFIG_NAME = pipeline_config[\"config_name\"]\n",
        "OUTPUT_MODEL_NAME = finetune_config[\"output_model_name\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG5PA94w_Js6"
      },
      "source": [
        "## API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "i6Fy6R3u6WMD"
      },
      "outputs": [],
      "source": [
        "# Could also insert the token here directly\n",
        "HF_TOKEN = os.getenv(\"HUGGINGFACE_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9GIrquruw6c"
      },
      "source": [
        "Leveraging Unsloth notebooks for finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "X-AVcbVxuw6c"
      },
      "outputs": [],
      "source": [
        "max_seq_length = finetune_config[\"max_seq_length\"] # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = finetune_config[\"load_in_4bit\"] # Use 4bit quantization to reduce memory usage. Can be False.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zbpyQksvuw6c",
        "outputId": "d29ead55-3963-4981-a607-448c430e0222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484,
          "referenced_widgets": [
            "e9a46fdf97c441979dcf2e00f6824600",
            "c6ceafd970774d78be3a2b63fdd95c7b",
            "eb6987c57de64d069613d40ce52c1816",
            "ccf0607ae2c348ed8aad469b8027c92e",
            "90b5deeb15bc41bdaf15b67a3470e912",
            "ad3852c150ba4d46a12b0ebbfe16c66f",
            "a3b483f123ac42d59c632e41b9ad9c5d",
            "baaaa4709e0e4995814aba3308e6059b",
            "1f3fdb9c3d4c4682a83d4e45ec3ee70f",
            "f8a2eb9064534c7ebfd24dbfb7d3a990",
            "8077ab26468f4dc78d643c5f25c2579a",
            "8bc02adbf30844b9ad6ab85c5d62a23b",
            "0b673803a166459994ea1b50dad22fc2",
            "ae9b4b75c80b4cedb2c3ad0a78c4243a",
            "e14da6f6f598400e8d460c17c8d6866a",
            "8f85779bcbee4413949f7fb93d170eec",
            "7579e7aba99c448e96a592f417664ef0",
            "7cae9cf2f8fb4739b2dd9cf7afcc9043",
            "2ec70383f0ff40c2934b88866c814d00",
            "3f32ecba698d4c648a23c651d307f348",
            "3e198c9b107a429eb341b78a82a9b2a0",
            "f477deecf5684ab5b6acfeeac55c769a",
            "8164a37749e84cda9c4ea4ccfe397cf6",
            "74dba62e01f743998d712feb1c85cb30",
            "79965e275de84efdb5eaed196cdd85d3",
            "f9afa4bb9a504cf89a3070da1cbdfe42",
            "deeadc1851e94c0da8424df0924cba21",
            "b90b10e650bf4df39a64b6cc2bfa6405",
            "36dc98de0d2e4e0e8204b0cda22a46e2",
            "118adf57673444c6a805007c498d7b4d",
            "af35caf25e16410f9f726a6b755bcf8d",
            "9f792370b78146a3a891f8fbfee0859d",
            "9146e776d68b49c9a0f3479fe63f987c",
            "36fe8a18e39d443fb1aebbb597b96360",
            "778b14dd7f734cb7b234311e0f3ac723",
            "b91f6b3be0fa4e8f8f758dcd771d9a0d",
            "83db2ff8e4bd4c28b5c3f11aea5d84e8",
            "9ecc8a3e039e41f08982e8fc5346290e",
            "954a2d0bbd06428abd2f332fd1e2b8be",
            "bad9ca03cf1d46fd8ca50da074625bc2",
            "1e943cedfed045719f7886fc7b00b1d5",
            "6bca23b51576402da16d64ec97c3496d",
            "deaa3a796f2144ba8dca806925942f54",
            "a56e234f043c44feb9ef4e92291af29a",
            "546cff63d364467d8b5827543c7677f7",
            "dc784f8401a84b1aba3467afb69c37f8",
            "7d2acda1543b407dbd16cff4dc99564e",
            "4b983dbce2d44985994af742b95b7aef",
            "4b3beb1e8f6a435cba06c92bfecc4870",
            "0c1755477b364719b3e72cb2babd3988",
            "67c6b0b81b6648869d81ced62d0bd111",
            "03ced17f64bb4b86b932227413d8bcd0",
            "b418fb0b1a094c4190825a56b3cd6a0b",
            "e78c61b4cfef460a8777486e9e1d0255",
            "128976ec3e6940ada32c848b892714b8"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2024.11.9: Fast Llama patching. Transformers = 4.46.2.\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.5.1+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9a46fdf97c441979dcf2e00f6824600"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bc02adbf30844b9ad6ab85c5d62a23b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8164a37749e84cda9c4ea4ccfe397cf6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36fe8a18e39d443fb1aebbb597b96360"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "546cff63d364467d8b5827543c7677f7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = finetune_config[\"base_model_name\"], # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    # model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "u_hNjiv6uw6d",
        "outputId": "28861544-629e-4f41-92f9-f87abfaa86ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.11.9 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9KgTPrPtuw6d"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsrNxEANuw6d"
      },
      "source": [
        "## Get dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ScHEQ1xIuw6e"
      },
      "outputs": [],
      "source": [
        "if HF_TOKEN is None:\n",
        "  raise EnvironmentError(\"A hugggingface token is necessary!\")\n",
        "dataset_finetune = load_dataset(\n",
        "    FINETUNING_DATASET_NAME,\n",
        "    CONFIG_NAME,\n",
        "    split=\"train\",\n",
        "    token=HF_TOKEN\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "SNPyjibDuw6e",
        "outputId": "dc69425c-a909-45e9-cc6a-3a91bb480b88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['filename', 'source', 'source_type', 'chunk', 'question', 'answer', 'pass_through'],\n",
              "    num_rows: 1259\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "dataset_finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "bWmiW2KHuw6e",
        "outputId": "78b94997-81e5-4dbf-b009-4d9f166249ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Summarize the concept of agentic flows and their application in generating higher quality data as mentioned in the paper 'AgentInstruct Toward Generative Teaching With Agentic Flows'.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "\n",
        "dataset_finetune['question'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "G2ajkNV_uw6f",
        "outputId": "937515f0-8779-4a56-d767-7ad6b284fd99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Agentic flows refer to a structured approach in generating and transforming text through various specialized agents, each designed to produce specific types of content. This concept is utilized to enhance the quality of synthetic data, which can be used for model customization and continual improvement.\\n\\nIn the context of the paper \"AgentInstruct Toward Generative Teaching With Agentic Flows,\" agentic flows are applied through different types of generators, such as:\\n\\n1. **Argument Passage Generator**: Creates passages that articulate arguments, sometimes with logical inconsistencies.\\n2. **Debate Passage Generator**: Mimics the structure and content of debate transcripts.\\n3. **Conversation Passage Generator**: Generates dialogues.\\n4. **Meeting Transcript Generator**: Produces transcripts of meetings.\\n5. **Poem Generator**: Creates poetic texts.\\n6. **Satirical Passage Generator**: Infuses texts with satire.\\n7. **Instructional Passage Generator**: Generates instructional manuals.\\n8. **Long Text Generator**: Extends original texts by adding information.\\n9. **Identity Agent**: Replicates the input text verbatim.\\n\\nThese agents work together to create diverse and high-quality synthetic data that can be used to train models more effectively. The continual improvement aspect comes from the ability of these agents to generate data that is not only relevant but also of higher quality than the base model, thereby enhancing the overall performance of language models.\\n\\nThe structured approach of agentic flows allows for a systematic generation of varied content types, which can be tailored to specific needs, ultimately leading to better model customization and improved learning outcomes.\\n\\n**Source**: The concept of agentic flows and their application is derived from the paper \"AgentInstruct Toward Generative Teaching With Agentic Flows.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "\n",
        "dataset_finetune['answer'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1JNeUeduw6f"
      },
      "source": [
        "## Convert dataset to messages format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "VmjwsgvDuw6f"
      },
      "outputs": [],
      "source": [
        "def convert_to_messages_format(example):\n",
        "    return [\n",
        "        {\"role\": \"user\", \"content\": example['question']},\n",
        "        {\"role\": \"assistant\", \"content\": example['answer']},\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "g2Y3xb2Euw6f",
        "outputId": "e1aa0a6c-0494-4a8d-fbef-3314bb14f82d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69,
          "referenced_widgets": [
            "439aa16185c445e693ebfd9df5a7348e",
            "47bdcbe51bd44169b29803237ee6fd3a",
            "626b2d9ab4134d469f93150328461801",
            "46cdf69ee8054827a73432ba836eb113",
            "9a4088900ee34774917802eafacefe52",
            "f00e138570c9425cb51c08e29e530df9",
            "a32c72ff644147ef9018a464c2d90aa7",
            "39641dfd24e240afb6b83e1f7e864078",
            "c3d064a1665342bb82f9e5826ac7c090",
            "96521fb94d2e48fe9f22385d9cb2f3af",
            "f579599bc03f4654a0e3c8cc5f9b9f79"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1259 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "439aa16185c445e693ebfd9df5a7348e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "dataset_finetune = dataset_finetune.map(\n",
        "    lambda x: {\n",
        "        'conversations' : convert_to_messages_format(x)\n",
        "        }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FxM4PRoauw6f",
        "outputId": "acc183ad-77a7-4280-bcf7-14466fa516ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69,
          "referenced_widgets": [
            "929f70350bfe4a33a74886f42b790553",
            "eabc8b40075242aa9020d66b80eb961e",
            "17c954735f8643c8a69f8aad9f88221b",
            "4865409cfd7e4b4697892af5ae26bdcc",
            "39b08054fda74628b5a803139f784afc",
            "c1146fa8cf68456295bd29aa97fd5f2a",
            "77f22353b65047bf815b51ed9ca0c008",
            "54434d4da82644b6ac9f02a13ccae91d",
            "3154704dd55b4f14b68eda14e845778a",
            "90093e86d91b4a49a0553755be3c3b37",
            "32a817bbfd854b5bae32bb31278c63c5"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1259 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "929f70350bfe4a33a74886f42b790553"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "dataset_finetune = dataset_finetune.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "UE4KL4gpuw6g",
        "outputId": "6800b968-912b-40ba-bbb5-e2914bc6af99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSummarize the concept of agentic flows and their application in generating higher quality data as mentioned in the paper \\'AgentInstruct Toward Generative Teaching With Agentic Flows\\'.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAgentic flows refer to a structured approach in generating and transforming text through various specialized agents, each designed to produce specific types of content. This concept is utilized to enhance the quality of synthetic data, which can be used for model customization and continual improvement.\\n\\nIn the context of the paper \"AgentInstruct Toward Generative Teaching With Agentic Flows,\" agentic flows are applied through different types of generators, such as:\\n\\n1. **Argument Passage Generator**: Creates passages that articulate arguments, sometimes with logical inconsistencies.\\n2. **Debate Passage Generator**: Mimics the structure and content of debate transcripts.\\n3. **Conversation Passage Generator**: Generates dialogues.\\n4. **Meeting Transcript Generator**: Produces transcripts of meetings.\\n5. **Poem Generator**: Creates poetic texts.\\n6. **Satirical Passage Generator**: Infuses texts with satire.\\n7. **Instructional Passage Generator**: Generates instructional manuals.\\n8. **Long Text Generator**: Extends original texts by adding information.\\n9. **Identity Agent**: Replicates the input text verbatim.\\n\\nThese agents work together to create diverse and high-quality synthetic data that can be used to train models more effectively. The continual improvement aspect comes from the ability of these agents to generate data that is not only relevant but also of higher quality than the base model, thereby enhancing the overall performance of language models.\\n\\nThe structured approach of agentic flows allows for a systematic generation of varied content types, which can be tailored to specific needs, ultimately leading to better model customization and improved learning outcomes.\\n\\n**Source**: The concept of agentic flows and their application is derived from the paper \"AgentInstruct Toward Generative Teaching With Agentic Flows.\"<|eot_id|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "dataset_finetune['text'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aysQr1W4uw6g"
      },
      "source": [
        "## Set Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "PACKING = True"
      ],
      "metadata": {
        "id": "op1ZBUoaOaqB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "22E9pLsnuw6g",
        "outputId": "33a01e3b-643c-4741-c38d-53116875bcad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69,
          "referenced_widgets": [
            "1618e769a6a74adf974941dc9e26484d",
            "67923dd2e6ef4bb0a37c119a3f09ecc5",
            "ce346334ba9342ab9a2f148683ecf729",
            "296b2b148bf74b15bdec21add629ac74",
            "5e20bfb51684488189f3c7dcdb77bbfa",
            "f76ea79a0e234b41bc27b18c91545927",
            "5af6a967bb2343e7ae204db4d82ab2e4",
            "c9e76fb52e094264a4c97ea71533dee3",
            "1a92527d38a749c4a03e5296b49c73d9",
            "e094d98d59c54dd0af90854281837f2d",
            "e851d802f4934835aa44011ca3d96bd3"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1618e769a6a74adf974941dc9e26484d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset_finetune,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    dataset_num_proc = 1,  # Affects memory usage\n",
        "    packing = finetune_config[\"packing\"], # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = finetune_config[\"batch_size\"], # Affects memory usage\n",
        "        gradient_accumulation_steps = finetune_config[\"gradient_accumulation_steps\"],\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = finetune_config[\"num_train_epochs\"], # Set this for 1 full training run.\n",
        "        # max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\"\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMbXl2BIuw6g"
      },
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. Look into this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ztgyBsIwuw6g",
        "outputId": "4eb95cd1-6e55-4587-82b4-6ba5aa0b64f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69,
          "referenced_widgets": [
            "5424158385084a368de8f3c68fe67b53",
            "e36410af31964e268c17b597d16172b7",
            "6477412953374157a8e5944d3863b5d2",
            "6b87f6cd8457421f83ea2b38f5da80c6",
            "d164403dc6c643cdb6a5c032c062494b",
            "d26c4c5fc62649d59e9b46c1ed383060",
            "4be973a3aa784146a5a70893b1b5ef4d",
            "d1423f13b4034629a0d2897a1fe08d49",
            "f02cfa4b9d5d4536af1fae502e40af96",
            "4edce530ad004200b9dc077bc06fffb8",
            "e097aa8f91b843859b16dde90bd781ea"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/38 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5424158385084a368de8f3c68fe67b53"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ClC-xQP4uw6g",
        "outputId": "ce5063ab-89fd-41ef-e68b-b82fb0a2ccc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' enabling the model to generate answers based on real-time document retrieval.\\n\\n3. **Chain-of-Thought Reasoning**: RAFT employs a chain-of-thought (CoT) response style, which encourages the model to articulate its reasoning process when generating answers. This contrasts with traditional fine-tuning methods that may focus solely on providing direct answers without elaboration. The CoT approach not only aids in understanding but also helps prevent overfitting by enriching the model\\'s comprehension of the task.\\n\\n4. **Dataset Organization**: The RAFT methodology involves organizing the training dataset such that some portions lack golden documents in their context. This design decision is aimed at enhancing the model\\'s ability to operate effectively even when complete information is not available, a challenge that traditional fine-tuning methods may not adequately address.\\n\\n5. **Performance Across Diverse Datasets**: RAFT has been shown to consistently outperform existing fine-tuning methods across various datasets, including PubMed, HotpotQA, and Gorilla. This indicates that RAFT\\'s unique training strategy is more effective in adapting language models for specialized domain tasks compared to traditional approaches.\\n\\nIn summary, RAFT\\'s innovative techniques, such as the use of distractor documents, open-book training, chain-of-thought reasoning, and strategic dataset organization, collectively enhance the model\\'s performance in domain-specific Retrieval-Augmented Generation tasks, setting it apart from conventional fine-tuning methods.\\n\\nSource: RAFT: Adapting Language Model to Domain Specific RAG, Tianjun Zhang et al.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow does the inclusion of irrelevant documents in training impact a model\\'s ability to process information, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe inclusion of irrelevant documents in training significantly impacts a model\\'s ability to process information, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'. The RAFT method addresses this challenge by integrating a training strategy that combines both golden (highly relevant) documents and distractor (irrelevant) documents. This mixed training approach is essential because training solely with golden documents can reduce a model\\'s capacity to identify and disregard irrelevant information.\\n\\nSpecifically, RAFT employs a strategy where the model is trained with varying numbers of distractor documents alongside the golden document. This setup encourages the model to learn how to focus on pertinent information while ignoring irrelevant content. The paper emphasizes that this methodology not only enhances the model\\'s performance but also increases its robustness against variations in the number of documents encountered during testing.\\n\\nThe experiments detailed in the paper reveal that training with distractor documents leads to superior performance compared to configurations that include only the golden document. For example, models trained with one golden document and multiple distractor documents (e.g., four distractors) perform significantly better in real-world scenarios where they must navigate both relevant and irrelevant information.\\n\\nMoreover, the findings suggest that including a certain percentage of training data without the golden documents in the context can be beneficial for downstream tasks, as it helps the model adapt to situations where it must discern relevant from irrelevant information. This is particularly critical for retrieval-augmented generation tasks, where the model\\'s ability to sift through irrelevant text is vital for effective performance.\\n\\nIn summary, RAFT\\'s approach of incorporating distractor documents during training refines the model\\'s ability to effectively process and utilize relevant content while disregarding irrelevant text, thereby enhancing its overall robustness in retrieval-augmented generation tasks.\\n\\n**Source:** RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat are the implications of using domain-specific knowledge in the RAFT framework for answering questions, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe implications of using domain-specific knowledge in the RAFT framework for answering questions are significant and multifaceted. The RAFT (Retrieval-Augmented Fine Tuning) methodology enhances the performance of pre-trained Large Language Models (LLMs) by effectively integrating domain-specific knowledge, which is crucial for improving accuracy in specialized applications.\\n\\n1. **Improved Accuracy in Domain-Specific Contexts**: RAFT is designed to optimize LLMs for \"open-book\" exam scenarios, where the model can reference external documents to answer questions. By training the model to distinguish between relevant (golden) documents and irrelevant (distractor) documents, RAFT enhances the model\\'s ability to accurately answer questions based on the specific context of the domain. This is particularly important in specialized fields such as medicine or law, where precise information is critical.\\n\\n2. **Enhanced Reasoning Capabilities**: The framework incorporates a chain-of-thought (CoT) approach, which guides the model in reasoning through the information presented in the relevant documents. This method not only helps the model arrive at the correct answer but also enriches its understanding of the material, thereby improving overall accuracy and robustness against overfitting to concise answers. The CoT approach has been shown to significantly enhance training robustness, as evidenced by performance gains in various datasets.\\n\\n3. **Training with Distractor Documents**: One of the key innovations of RAFT is its training process, which includes distractor documents. By teaching the model to ignore irrelevant information, RAFT helps it focus on the most pertinent data, thereby improving its reasoning capabilities. This aspect is crucial in real-world applications where the model may encounter a vast array of information, and the ability to filter out noise is essential for effective performance.\\n\\n4. **Demonstrated Performance Improvements**: The paper reports that RAFT consistently outperforms existing fine-tuning methods across various datasets, including PubMed, HotpotQA, and Gorilla. This indicates that the integration of domain-specific knowledge through RAFT not only enhances the model\\'s performance but also provides a significant advancement in adapting LLMs for specialized domains.\\n\\nIn summary, the implications of using domain-specific knowledge in the RAFT framework are profound, leading to improved accuracy, enhanced reasoning capabilities, effective training with distractor documents, and demonstrated performance improvements across various specialized datasets.\\n\\nSource: RAFT: Adapting Language Model to Domain Specific RAG, Tianjun Zhang et al.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat are the implications of not utilizing domain-specific knowledge during the training of in-context retrieval methods, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe implications of not utilizing domain-specific knowledge during the training of in-context retrieval methods, as discussed in \"RAFT: Adapting Language Model to Domain Specific RAG,\" are significant. The paper emphasizes that existing in-context retrieval methods do not take advantage of the specific knowledge available in a fixed domain during the training phase. This is likened to taking an open-book exam without any prior studying, meaning that while the model can access external documents during inference, it does not learn from these documents in a structured manner during training.\\n\\nThis lack of engagement with domain-specific knowledge leads to several critical issues:\\n\\n1. **Suboptimal Performance**: The model is not adequately prepared for the specific types of questions and documents it will encounter, resulting in suboptimal performance. Without training on the dynamics between questions, relevant documents, and correct answers, the model struggles to effectively discern relevant information from distractor documents.\\n\\n2. **Missed Learning Opportunities**: The authors argue that these methods miss the opportunity to train the model on the relationships between questions and the relevant documents. This is particularly detrimental in specialized domains where accuracy is crucial, as the model fails to develop the necessary skills to navigate the complexities of the domain-specific information.\\n\\n3. **Ineffective Information Retrieval**: By not incorporating domain-specific knowledge during training, the model lacks the ability to effectively utilize the information it retrieves. This results in a failure to ignore distractor documents and focus on the relevant ones, which is essential for accurate retrieval-augmented generation tasks.\\n\\nIn contrast, the proposed RAFT method addresses these shortcomings by training the model to recognize and utilize relevant documents while learning to ignore distractors. This approach enhances the model\\'s ability to perform in domain-specific retrieval-augmented generation tasks, ultimately leading to improved performance across various datasets.\\n\\nSource: \"RAFT: Adapting Language Model to Domain Specific RAG\" by Tianjun Zhang et al.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow does the conclusion about training data proportions in \\'RAFT: Adapting Language Model to Domain Specific RAG\\' contribute to the field of natural language processing?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe conclusion about training data proportions in \\'RAFT: Adapting Language Model to Domain Specific RAG\\' significantly contributes to the field of natural language processing (NLP) by challenging conventional wisdom regarding the use of golden documents in training models for Retrieval-Augmented Generation (RAG) tasks. The authors of the paper found that the optimal proportion of training data containing golden documents varies across different datasets, with effective values identified as 40%, 60%, and 100%. This finding suggests that incorporating a mix of training data, including distractor documents (which do not provide the correct context), can enhance the model\\'s performance.\\n\\nThis approach is particularly relevant in \"open-book\" settings where the model must discern relevant information from a mixture of documents. By training the model with both golden and distractor documents, it learns to ignore irrelevant information and focus on what is pertinent to answering questions. This method not only improves the model\\'s ability to extract relevant information but also enhances its robustness in real-world applications where the context may not always be perfectly aligned with the questions posed.\\n\\nThe implications of these findings are profound for the development of NLP models, as they suggest that a balanced training strategy can lead to better generalization and performance in domain-specific tasks. This research encourages further exploration into the optimal training configurations for various applications, potentially leading to more effective and adaptable language models in specialized domains.\\n\\n**Source:** RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow might the findings of the RAFT paper, \"RAFT: Adapting Language Model to Domain Specific RAG,\" influence future research in language model adaptation?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe findings of the RAFT paper, \"RAFT: Adapting Language Model to Domain Specific RAG,\" are likely to influence future research in language model adaptation in several significant ways:\\n\\n1. **Methodological Advancement**: The introduction of Retrieval-Augmented Fine Tuning (RAFT) as a novel training methodology provides a structured approach to enhance the performance of pre-trained Large Language Models (LLMs) in domain-specific Retrieval-Augmented Generation (RAG) tasks. Future research may build upon this framework, exploring variations or improvements to the RAFT methodology to further refine model adaptation techniques.\\n\\n2. **Focus on Domain-Specific Knowledge**: The paper emphasizes the importance of effectively incorporating domain-specific knowledge into LLMs. This focus may encourage researchers to investigate additional strategies for integrating specialized information, potentially leading to the development of new models or training techniques tailored for various fields such as medicine, law, or technology.\\n\\n3. **Handling Distractor Documents**: RAFT\\'s approach to training models to distinguish between relevant and irrelevant (distractor) documents is a critical aspect of its methodology. Future research may explore more sophisticated techniques for document retrieval and filtering, enhancing the model\\'s ability to focus on pertinent information while ignoring distractions.\\n\\n4. **Open-Book Exam Scenarios**: The concept of preparing LLMs for \"open-book\" exam scenarios, where models can reference external documents, opens up new avenues for research. This could lead to the exploration of how LLMs can be trained to effectively utilize external knowledge sources in real-time, improving their performance in dynamic environments.\\n\\n5. **Chain-of-Thought Reasoning**: The emphasis on chain-of-thought-style responses in RAFT highlights the importance of reasoning capabilities in LLMs. Future studies may investigate how to further enhance reasoning processes within language models, potentially leading to more advanced cognitive capabilities in AI systems.\\n\\n6. **Evaluation Across Diverse Datasets**: The consistent performance improvements demonstrated by RAFT across various datasets (e.g., PubMed, HotpotQA, Gorilla) suggest that the methodology is robust and versatile. This could inspire researchers to conduct comparative studies on different adaptation techniques across a wider range of datasets and applications.\\n\\nIn summary, the RAFT paper sets a foundation for future research in language model adaptation by introducing innovative methodologies, emphasizing domain-specific knowledge integration, and highlighting the importance of reasoning and document relevance in enhancing model performance.\\n\\nSource: RAFT: Adapting Language Model to Domain Specific RAG, Tianjun Zhang et al.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nIn what ways does the proposed Retrieval-Augmented Fine Tuning (RAFT) strategy improve upon the limitations of the two main candidates discussed in the paper \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe proposed Retrieval-Augmented Fine Tuning (RAFT) strategy improves upon the limitations of the two main candidates discussed in the paper \"RAFT: Adapting Language Model to Domain Specific RAG\" in several key ways:\\n\\n1. **Addressing In-Context Learning Limitations**: The first candidate, In-Context Learning through Retrieval-Augmented Generation (RAG), allows the model to reference documents when answering questions. However, it does not fully utilize the learning opportunities available in a fixed domain setting. RAFT enhances this by training the model to discern between relevant and irrelevant documents during the learning process. Specifically, RAFT trains the model to ignore distractor documents that do not aid in answering the question, thereby improving its ability to focus on pertinent information.\\n\\n2. **Improving Supervised Fine-Tuning**: The second candidate, Supervised Fine-Tuning, aims to align the model with end tasks and user preferences but often fails to incorporate RAG at test time or does not account for retrieval imperfections during training. RAFT overcomes these challenges by integrating RAG into the fine-tuning process. It prepares the training data such that each data point includes a question, a set of documents (both relevant and distractor), and a chain-of-thought style answer derived from the relevant document. This approach not only enhances the model\\'s understanding of domain-specific knowledge but also ensures robustness against irrelevant information.\\n\\n3. **Chain-of-Thought Reasoning**: RAFT emphasizes the use of chain-of-thought reasoning in its responses, which helps the model to articulate its reasoning process more clearly. This is particularly beneficial in complex question-answering scenarios where reasoning is crucial for deriving the correct answer from the provided documents.\\n\\n4. **Performance Across Datasets**: The paper reports that RAFT consistently outperforms both supervised fine-tuning and RAG across various datasets, including PubMed, HotpotQA, and Gorilla. This demonstrates RAFT\\'s effectiveness in adapting pre-trained LLMs for in-domain RAG tasks, showcasing its potential as a robust training strategy.\\n\\nIn summary, RAFT improves upon the limitations of the existing methods by effectively combining the strengths of both RAG and supervised fine-tuning, focusing on relevant information, and enhancing reasoning capabilities, which leads to better performance in specialized domains.\\n\\n(Source: RAFT: Adapting Language Model to Domain Specific RAG)<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nIn what ways does the RAFT method improve a model\\'s performance in real-world retrieval scenarios, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe RAFT method improves a model\\'s performance in real-world retrieval scenarios by implementing a training strategy that effectively incorporates both relevant and irrelevant documents, referred to as golden and distractor documents, respectively. This dual-document training approach is essential because relying solely on golden documents can hinder a model\\'s ability to differentiate between relevant and irrelevant information.\\n\\nSpecifically, RAFT trains the model with varying numbers of distractor documents alongside a golden document. This mixed training setup encourages the model to learn to identify and focus on pertinent information while disregarding irrelevant content. The paper emphasizes that this methodology not only enhances the model\\'s performance but also increases its robustness against variations in the number of documents encountered during testing.\\n\\nExperimental results presented in the paper demonstrate that models trained with a combination of one golden document and multiple distractor documents (for example, four distractors) significantly outperform those trained with only golden documents. This is particularly relevant in real-world scenarios where models must navigate through both relevant and irrelevant information.\\n\\nAdditionally, RAFT employs a chain-of-thought (CoT) response style, which aids the model in reasoning and improves its ability to process information effectively. The integration of CoT prompts has been shown to enhance training robustness and overall accuracy, as it guides the model in understanding the context and reasoning behind the answers.\\n\\nIn summary, the RAFT method\\'s incorporation of distractor documents during training refines the model\\'s ability to process and utilize relevant content while ignoring irrelevant text, thereby enhancing its overall performance and robustness in retrieval-augmented generation tasks.\\n\\n**Source:** RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow might the performance of a language model be affected by its inability to learn from relevant documents during training, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe performance of a language model can be significantly impacted by its inability to learn from relevant documents during training, as discussed in \"RAFT: Adapting Language Model to Domain Specific RAG.\" The paper emphasizes that existing in-context retrieval methods do not leverage the specific knowledge available in a domain during the training phase. This is likened to taking an open-book exam without prior studying, where the model can access external documents during inference but fails to learn from them in a structured manner during training.\\n\\nOne of the main issues with these in-context retrieval methods is that they do not prepare the model for the specific types of questions and documents it will encounter. This lack of preparation can lead to suboptimal performance, particularly in specialized domains where accuracy is crucial. The authors argue that these methods miss the opportunity to train the model on the relationships between questions, relevant documents, and correct answers. Consequently, the model does not develop the necessary skills to effectively discern relevant information from distractor documents, which is a critical shortcoming in specialized applications.\\n\\nIn contrast, the RAFT method addresses this challenge by training the model to recognize and utilize relevant documents while learning to ignore distractors. This approach enhances the model\\'s ability to perform in domain-specific retrieval-augmented generation tasks. By incorporating distractor documents during training, RAFT helps the model become more robust in identifying pertinent information, ultimately improving its performance in answering questions accurately within a specific domain.\\n\\nSource: \"RAFT: Adapting Language Model to Domain Specific RAG\" by Tianjun Zhang et al.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat evidence does the RAFT paper provide to support the effectiveness of using distractor documents in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe RAFT paper provides substantial evidence supporting the effectiveness of using distractor documents in its training methodology. The primary argument is that training solely with golden (highly relevant) documents can impair a model\\'s ability to identify and disregard irrelevant information. To counter this, RAFT integrates both golden and distractor documents during training, which enhances the model\\'s robustness against irrelevant text encountered in retrieval-augmented generation (RAG) tasks.\\n\\nSpecifically, the paper details experiments where models were trained with varying numbers of distractor documents alongside a golden document. The results indicate that configurations including distractor documents consistently outperform those trained with only the golden document. For example, models trained with one golden document and multiple distractors (e.g., four distractors) demonstrated significantly improved performance in real-world scenarios, where they must navigate both relevant and irrelevant information.\\n\\nFurthermore, the paper emphasizes that the optimal training setup involves a mix of golden and distractor documents, which not only helps the model learn to focus on pertinent information but also prepares it for the variability in document relevance encountered during testing. The findings suggest that including distractor documents in training is beneficial for enhancing the model\\'s ability to process and utilize relevant content effectively.\\n\\nIn summary, the RAFT methodology\\'s incorporation of distractor documents is shown to refine the model\\'s discernment capabilities, leading to improved performance in domain-specific RAG tasks, as evidenced by experiments across various datasets such as PubMed, HotpotQA, and Gorilla.\\n\\n**Source:** RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat specific performance improvements were observed when varying the proportion of golden documents in the training data, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe paper \\'RAFT: Adapting Language Model to Domain Specific RAG\\' reveals specific performance improvements when varying the proportion of golden documents in the training data. The authors investigate the optimal percentage of training instances that should include golden documents, denoted as P%. Their findings indicate that the optimal proportion varies across different datasets, with effective P% values identified as 40%, 60%, and 100%. \\n\\nThe study suggests that incorporating a mix of training data, which includes both golden documents and distractor documents (irrelevant content), can enhance the model\\'s performance on Retrieval-Augmented Generation (RAG) tasks. Specifically, training with a combination of golden documents and distractor documents helps the model learn to extract relevant information more effectively, improving its ability to answer questions related to the documents. \\n\\nThe results demonstrate that training solely with golden documents can lead to poorer performance, as it may hinder the model\\'s ability to discern and disregard irrelevant information. By including distractor documents during training, the model becomes more robust and better equipped to handle real-world scenarios where it must filter out irrelevant content while focusing on pertinent information.\\n\\nIn summary, the paper concludes that a balanced approach, where a portion of the training data lacks golden documents, can significantly enhance the model\\'s robustness and performance in RAG tasks, rather than relying exclusively on golden documents for training.\\n\\n**Source:** RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nIn what ways do the datasets used for the RAFT model evaluation differ in terms of their focus and structure, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe datasets used for the RAFT model evaluation differ significantly in their focus and structure, catering to various domains and question-answering tasks. Hereâ€™s a detailed breakdown:\\n\\n1. **Natural Questions (NQ)**: This dataset is centered on open-domain question-answering, primarily utilizing Wikipedia content. It focuses on common knowledge topics, such as movies and sports, making it suitable for evaluating general knowledge retrieval.\\n\\n2. **Trivia QA**: Similar to NQ, Trivia QA also employs Wikipedia as its source and is designed for open-domain question-answering. It emphasizes general knowledge, providing a broad range of questions that test the model\\'s ability to retrieve factual information.\\n\\n3. **HotpotQA**: This dataset stands out as it requires reasoning over multiple documents. It is designed for open-domain question-answering but adds complexity by necessitating the synthesis of information from various sources, thus evaluating the model\\'s reasoning capabilities.\\n\\n4. **HuggingFace Hub**: This dataset is part of the APIBench framework and focuses on the model\\'s ability to generate correct, functional, and executable API calls based on documentation. It is structured to assess the model\\'s performance in a programming context.\\n\\n5. **Torch Hub**: Like the HuggingFace Hub dataset, this dataset is also from the APIBench and is aimed at generating executable API calls, further emphasizing the model\\'s capabilities in handling programming-related queries.\\n\\n6. **TensorFlow Hub**: This dataset shares similar objectives with the HuggingFace and Torch Hub datasets, focusing on generating executable API calls, thus contributing to the evaluation of the model\\'s performance in technical domains.\\n\\n7. **PubMed QA**: This dataset is specifically tailored for biomedical research question-answering. It focuses on answering medical and biology-related questions, making it essential for evaluating the model\\'s performance in specialized, domain-specific contexts.\\n\\nThe diversity in these datasets allows for a comprehensive evaluation of the RAFT model across both general and specialized domains. By including datasets that range from general knowledge to technical and biomedical contexts, the evaluation captures the model\\'s robustness and effectiveness in handling various types of question-answering tasks.\\n\\nSource: RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat are the advantages and disadvantages of using In-Context Learning through Retrieval-Augmented Generation (RAG) for adapting LLMs to specialized domains, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nIn the paper \"RAFT: Adapting Language Model to Domain Specific RAG,\" the advantages and disadvantages of using In-Context Learning through Retrieval-Augmented Generation (RAG) for adapting Large Language Models (LLMs) to specialized domains are discussed as follows:\\n\\n### Advantages:\\n1. **Access to Domain-Specific Information**: RAG allows LLMs to reference external documents when answering questions, which is crucial for specialized domains where general knowledge may not suffice. This capability enhances the model\\'s performance by providing relevant context and information.\\n\\n2. **Improved Reasoning**: The RAFT method incorporates a chain-of-thought-style response, which helps improve the model\\'s reasoning abilities. By training the model to cite relevant information verbatim from documents, it can better understand the relationship between questions and answers.\\n\\n3. **Performance Across Diverse Datasets**: The RAFT approach has shown consistent improvements in model performance across various datasets, such as PubMed, HotpotQA, and Gorilla. This indicates that RAG can effectively adapt LLMs to different specialized tasks.\\n\\n### Disadvantages:\\n1. **Limited Learning Opportunities**: While RAG-based in-context learning methods allow for referencing documents, they do not fully leverage the learning opportunities provided by a fixed domain setting. This can limit the model\\'s ability to generalize and learn from the specific characteristics of the domain.\\n\\n2. **Challenges with Distractor Documents**: The presence of distractor documents during training can complicate the learning process. The model must learn to differentiate between relevant and irrelevant information, which can be challenging and may affect its performance if not managed properly.\\n\\n3. **Dependence on Retrieval Quality**: The effectiveness of RAG is heavily reliant on the quality of the retrieval process. If the retriever fails to identify the most relevant documents, the model\\'s performance can suffer, leading to inaccurate or incomplete answers.\\n\\nIn summary, while In-Context Learning through RAG offers significant advantages in accessing domain-specific information and improving reasoning, it also presents challenges related to learning limitations, distractor management, and retrieval quality.\\n\\n(Source: RAFT: Adapting Language Model to Domain Specific RAG)<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow does the training strategy of RAFT differ from traditional methods that use only relevant documents, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nRAFT differs from traditional methods that use only relevant documents by incorporating a training strategy that includes both golden (highly relevant) documents and distractor (irrelevant) documents. This mixed training approach is essential because relying solely on golden documents can impair the model\\'s ability to identify and ignore irrelevant information.\\n\\nIn RAFT, the model is trained with varying numbers of distractor documents alongside the golden document. This setup encourages the model to learn to focus on pertinent information while disregarding irrelevant content. The paper emphasizes that this methodology not only improves the model\\'s performance but also enhances its robustness against variations in the number of documents encountered during testing.\\n\\nExperimental results indicate that models trained with a combination of one golden document and multiple distractor documents (e.g., four distractors) perform significantly better in real-world scenarios where they must navigate both relevant and irrelevant information. This training strategy ultimately refines the model\\'s ability to effectively process and utilize relevant content while ignoring distractions, thereby improving its overall performance in retrieval-augmented generation tasks.\\n\\n**Source:** RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat role does user preference play in the Supervised Fine-Tuning approach for adapting LLMs, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nIn the paper \"RAFT: Adapting Language Model to Domain Specific RAG,\" user preference plays a significant role in the Supervised Fine-Tuning (SFT) approach for adapting Large Language Models (LLMs) to specialized domains. The SFT method is designed to align the model\\'s responses more closely with the specific needs and expectations of users by training it on datasets that include question-answer pairs relevant to the target domain.\\n\\nKey aspects of how user preference is integrated into the SFT approach include:\\n\\n1. **Alignment with User Tasks**: The SFT process involves training the model to recognize and respond to user queries based on the patterns and information present in the domain-specific documents. This means that the model learns to prioritize the types of answers that users are likely to find most useful or relevant, thereby enhancing its performance in real-world applications.\\n\\n2. **Incorporation of User Feedback**: Although the paper does not explicitly detail mechanisms for incorporating user feedback, the SFT methodology inherently allows for adjustments based on user interactions. By continuously refining the training dataset with new question-answer pairs that reflect user preferences, the model can adapt to changing user needs over time.\\n\\n3. **Improving Answering Style**: The SFT approach also focuses on aligning the model\\'s answering style with user expectations. This is crucial in specialized domains where the format and tone of responses may vary significantly. By training on domain-specific data, the model becomes more adept at providing answers that resonate with users, thus improving user satisfaction.\\n\\n4. **Challenges in Retrieval**: The paper notes that existing fine-tuning methods often do not account for the imperfections in the retrieval process during training. This can impact how well the model aligns with user preferences, as inaccuracies in retrieved documents may lead to less relevant or incorrect answers. The proposed RAFT method aims to address these challenges by training the model to effectively ignore distractor documents and focus on relevant information.\\n\\nIn summary, user preference is central to the SFT approach in RAFT, as it guides the model\\'s training to ensure that the responses are not only accurate but also tailored to the specific needs and expectations of users in specialized domains.\\n\\n(Source: RAFT: Adapting Language Model to Domain Specific RAG)<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow does the proportion of golden versus distractor documents in training data affect model robustness, based on the insights from the paper \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe proportion of golden versus distractor documents in training data significantly impacts model robustness, as discussed in the paper \\'RAFT: Adapting Language Model to Domain Specific RAG.\\' The authors emphasize that training solely with golden documents can hinder the model\\'s ability to effectively filter out irrelevant information. Instead, they advocate for a training strategy that incorporates both golden documents and a mix of distractor documents. This approach enhances the model\\'s performance by making it more resilient to variations in the number of documents encountered during testing.\\n\\nIn Section 5.1, titled \"Making Model Robust to top-K RAG,\" the authors highlight that integrating distractor documents during training allows the model to better discern and disregard irrelevant content. Their experiments demonstrate that configurations including a greater number of distractor documents consistently outperform those trained only with golden documents. This finding indicates that the model becomes more adept at handling real-world scenarios where it must navigate through both relevant and irrelevant information.\\n\\nFurthermore, in Section 4.4, the paper explores the optimal proportion of training data that should consist of golden documents. The results suggest that including a certain percentage of distractor documentsâ€”without the corresponding golden documentsâ€”can be beneficial for the model\\'s performance on Retrieval-Augmented Generation (RAG) tasks. The optimal proportion of golden documents varies across datasets, with findings indicating that training with a mix of both types of documents enhances the model\\'s ability to process and extract relevant information effectively.\\n\\nOverall, the insights from the paper underscore the importance of a well-calibrated training environment that includes distractor documents, which prepares the model for diverse scenarios it may encounter in practice.\\n\\n**Source:** RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nCan you explain how the APIBench datasets contribute to understanding the RAFT model\\'s capabilities in generating API calls, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe APIBench datasets play a crucial role in evaluating the RAFT model\\'s capabilities in generating API calls, as they are specifically designed to measure the model\\'s ability to produce correct, functional, and executable API calls based on documentation. The datasets included in APIBench, such as HuggingFace Hub, Torch Hub, and TensorFlow Hub, focus on coding and API documentation, which is essential for assessing how well the RAFT model can adapt its language generation capabilities to the domain of programming and API usage.\\n\\n1. **HuggingFace Hub**: This dataset is part of the APIBench and is used to evaluate the model\\'s performance in generating API calls that are not only syntactically correct but also semantically appropriate based on the provided documentation.\\n\\n2. **Torch Hub**: Similar to the HuggingFace dataset, this dataset assesses the model\\'s ability to generate executable API calls, ensuring that the outputs can be directly utilized in programming tasks.\\n\\n3. **TensorFlow Hub**: This dataset also contributes to the APIBench, focusing on the same objectives as the previous two, thereby providing a comprehensive evaluation of the model\\'s performance across different API documentation.\\n\\nThe inclusion of these datasets allows for a thorough assessment of the RAFT model\\'s performance in generating API calls, which is critical for applications that require precise interaction with programming interfaces. By evaluating the model on these domain-specific datasets, researchers can determine how effectively RAFT can leverage its training to produce accurate and functional API calls, thereby enhancing its utility in real-world programming scenarios.\\n\\nOverall, the APIBench datasets are integral to understanding the RAFT model\\'s capabilities, as they provide a focused lens through which to evaluate its performance in generating API calls, ensuring that the model is not only capable of understanding language but also proficient in applying that understanding in a technical context.\\n\\nSource: RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow does the RAFT methodology improve the performance of Large Language Models in specific domains, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe RAFT methodology, as discussed in \"RAFT: Adapting Language Model to Domain Specific RAG,\" significantly enhances the performance of Large Language Models (LLMs) in specific domains by employing a novel training approach known as Retrieval-Augmented Fine Tuning (RAFT). This methodology is particularly effective in \"open-book\" exam scenarios, where the model can reference external documents to answer questions accurately.\\n\\nOne of the key improvements RAFT offers is its ability to train the model to differentiate between relevant documents (golden documents) and irrelevant ones (distractor documents). During training, the model learns to ignore distractor documents that do not contribute to answering a given question. This is achieved by training the model to cite verbatim from the relevant documents, which not only aids in generating accurate answers but also enhances the model\\'s reasoning capabilities. The incorporation of a chain-of-thought (CoT) response style further bolsters the model\\'s ability to reason through complex queries, leading to improved accuracy and robustness against irrelevant information.\\n\\nThe paper demonstrates that RAFT consistently outperforms existing fine-tuning methods across various datasets, including PubMed, HotpotQA, and Gorilla. This performance boost is attributed to several design decisions, such as training with distractor documents and organizing datasets to include instances where golden documents are absent. These strategies ensure that the model is well-prepared to handle domain-specific Retrieval-Augmented Generation (RAG) tasks effectively.\\n\\nIn summary, RAFT enhances LLM performance in specialized domains by integrating effective training techniques that leverage both retrieval and fine-tuning, ultimately leading to more accurate and contextually relevant responses.\\n\\nSource: RAFT: Adapting Language Model to Domain Specific RAG, Tianjun Zhang et al.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhy is it important to include both general knowledge and domain-specific datasets when assessing a model like RAFT, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'? <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nIncluding both general knowledge and domain-specific datasets when assessing a model like RAFT is crucial for several reasons:\\n\\n1. **Comprehensive Evaluation**: The use of diverse datasets allows for a thorough evaluation of the model\\'s performance across various contexts. General knowledge datasets, such as Natural Questions (NQ) and Trivia QA, focus on common knowledge topics, while domain-specific datasets like PubMed QA target specialized fields such as biomedical research. This combination ensures that the model is tested on its ability to handle both broad and niche inquiries.\\n\\n2. **Robustness and Effectiveness**: By assessing the model on both types of datasets, researchers can gauge its robustness and effectiveness in different scenarios. For instance, the model\\'s performance on general knowledge tasks can indicate its foundational reasoning capabilities, while its performance on domain-specific tasks can reveal how well it adapts to specialized information and contexts.\\n\\n3. **Real-World Applicability**: In practical applications, models often encounter a mix of general and domain-specific questions. Training and evaluating RAFT on a variety of datasets prepares it for real-world scenarios where users may ask questions that require both general knowledge and specialized understanding.\\n\\n4. **Improved Reasoning**: The RAFT model employs techniques such as ignoring distractor documents and citing relevant information verbatim, which enhances its reasoning abilities. Evaluating the model on datasets that require different reasoning strategies (like HotpotQA\\'s multi-document reasoning) helps to ensure that it can synthesize information effectively across various types of questions.\\n\\n5. **Adaptation to Specialized Domains**: As the paper discusses, adapting large language models to specialized domains is essential for many applications. The inclusion of domain-specific datasets allows the model to learn and adopt the appropriate answering style and context, which is critical for maximizing accuracy in specialized tasks.\\n\\nIn summary, the integration of both general knowledge and domain-specific datasets in the assessment of RAFT is vital for ensuring a well-rounded evaluation of the model\\'s capabilities, enhancing its robustness, and preparing it for real-world applications.\\n\\nSource: RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nCan you explain the relationship between model resilience and the inclusion of distractor documents as outlined in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe relationship between model resilience and the inclusion of distractor documents, as outlined in \\'RAFT: Adapting Language Model to Domain Specific RAG\\', is fundamentally about enhancing the model\\'s ability to handle irrelevant information during the retrieval-augmented generation (RAG) process. The paper emphasizes that training solely with golden (highly relevant) documents can limit the model\\'s capacity to effectively filter out irrelevant data. \\n\\nIn Section 5.1, titled \"Making Model Robust to top-K RAG,\" the authors argue that integrating distractor documentsâ€”documents that are not relevant to the taskâ€”into the training process significantly improves the model\\'s robustness. They conducted experiments showing that models trained exclusively on golden documents often perform worse than those trained with a mix of golden and distractor documents. This mixed training approach allows the model to become more resilient to variations in the number of documents it encounters during testing, as it learns to discern which documents are useful and which are not.\\n\\nFurthermore, Section 4.4 discusses the optimal proportion of training data that should consist of golden documents versus distractor documents. The findings suggest that including a certain percentage of distractor documents can enhance the model\\'s performance on RAG tasks, reinforcing the idea that a well-calibrated training environmentâ€”one that includes both relevant and irrelevant informationâ€”is crucial for preparing the model for real-world applications where it must navigate through a mix of document types.\\n\\nOverall, the inclusion of distractor documents is shown to be a key strategy in improving the model\\'s resilience, enabling it to maintain consistent performance even when faced with fluctuations in the relevance of the documents it processes during testing.\\n\\n**Source:** RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow does RAFT improve the model\\'s ability to answer questions in domain-specific settings according to the findings in \\'RAFT: Adapting Language Model to Domain Specific RAG\\' in the paper?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nRAFT, or Retrieval-Augmented Fine Tuning, improves the model\\'s ability to answer questions in domain-specific settings through several key methodologies and findings outlined in the paper \"RAFT: Adapting Language Model to Domain Specific RAG.\" Here are the main aspects of how RAFT enhances performance:\\n\\n1. **Training with Relevant and Distractor Documents**: RAFT trains the model to differentiate between relevant (golden) documents that contain the answers and distractor documents that do not. This is crucial because it teaches the model to focus on the pertinent information while ignoring irrelevant data. The training data is structured such that a portion of the questions is paired with both golden and distractor documents, while another portion includes only distractor documents. This approach compels the model to learn to extract answers from the context rather than simply memorizing them.\\n\\n2. **Chain-of-Thought Reasoning**: RAFT incorporates a chain-of-thought (CoT) reasoning style in its responses. By generating answers that include a reasoning process, the model can better articulate how it arrived at a conclusion. This method not only improves the clarity of the answers but also enhances the model\\'s reasoning capabilities, making it more adept at handling complex questions that require logical deductions.\\n\\n3. **Simulated Imperfect Retrieval Setting**: The training process simulates an imperfect retrieval environment, which is reflective of real-world scenarios where the model may not always have access to the most relevant documents. By training under these conditions, RAFT prepares the model to be more robust and effective in actual deployment situations where retrieval may not be perfect.\\n\\n4. **Performance Across Diverse Datasets**: The findings indicate that RAFT consistently outperforms traditional supervised fine-tuning methods and other RAG-based approaches across various datasets, including PubMed, HotpotQA, and Gorilla. For instance, the RAFT model demonstrated significant improvements in extracting information and reasoning correctly, achieving gains of up to 35.25% on specific tasks compared to baseline models.\\n\\n5. **Independence from Retriever Quality**: RAFT\\'s design allows it to be independent of the quality of the retriever used during testing. This means that even if the retrieval process is not optimal, the model is still capable of leveraging its training to provide accurate answers based on the documents it has been fine-tuned on.\\n\\nIn summary, RAFT enhances the model\\'s performance in domain-specific settings by effectively training it to utilize relevant information while ignoring distractions, employing reasoning processes in its answers, and preparing it for real-world retrieval challenges. This comprehensive approach leads to improved accuracy and robustness in answering domain-specific questions.\\n\\n**Source**: RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow do distractor documents contribute to the model\\'s performance in Retrieval-Augmented Generation tasks according to the findings in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe findings in \\'RAFT: Adapting Language Model to Domain Specific RAG\\' indicate that distractor documents play a crucial role in enhancing the model\\'s performance in Retrieval-Augmented Generation (RAG) tasks. Specifically, the paper highlights that training solely with golden (highly relevant) documents can limit the model\\'s ability to effectively filter out irrelevant information. To counter this, the RAFT approach integrates both golden documents and a mix of distractor documents during training.\\n\\nIn Section 5.1, titled \"Making Model Robust to top-K RAG,\" the authors present evidence that training with only golden documents often leads to poorer performance compared to configurations that include distractor documents. The experiments conducted show that models trained with a higher number of distractor documents demonstrate improved resilience to variations in the number of documents encountered during testing. This adaptability is essential for real-world applications where models must navigate through both relevant and irrelevant information.\\n\\nFurthermore, Section 4.4 discusses the optimal proportion of training data that should consist of golden documents, suggesting that including a certain percentage of distractor documents can enhance the model\\'s overall performance on RAG tasks. This underscores the importance of a balanced training environment that prepares the model to handle diverse scenarios, ultimately improving its efficiency in identifying and utilizing pertinent content.\\n\\nOverall, the inclusion of distractor documents is shown to significantly bolster the model\\'s robustness and performance in RAG tasks, validating the effectiveness of the RAFT methodology.\\n\\n**Source:** RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat implications do the findings of the paper \\'RAFT: Adapting Language Model to Domain Specific RAG\\' have for future research in training language models for domain-specific tasks?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe findings of the paper \\'RAFT: Adapting Language Model to Domain Specific RAG\\' have significant implications for future research in training language models for domain-specific tasks. The authors emphasize the importance of incorporating distractor documents during the training process, which enhances the model\\'s robustness and ability to handle irrelevant information. \\n\\n1. **Training with Distractor Documents**: The paper highlights that training solely with highly relevant (golden) documents can limit the model\\'s performance. By integrating distractor documents, the model learns to differentiate between relevant and irrelevant information, which is crucial for real-world applications where it must process mixed information. This approach is validated by experiments showing that models trained with a mix of golden and distractor documents outperform those trained only with golden documents.\\n\\n2. **Proportion of Training Data**: The research also explores the optimal proportion of training data that should consist of golden documents. It suggests that including a certain percentage of distractor documents can enhance the model\\'s performance on Retrieval-Augmented Generation (RAG) tasks. This finding encourages future studies to investigate the balance of training data composition to maximize model effectiveness.\\n\\n3. **Chain-of-Thought Reasoning**: The RAFT methodology incorporates a chain-of-thought response style, which aids in reasoning and improves the model\\'s ability to generate coherent and contextually relevant answers. This aspect of the training strategy could inspire further research into how reasoning processes can be integrated into language models for better performance in specialized domains.\\n\\n4. **Open-Book Settings**: The paper positions RAFT as a strategy for \"open-book\" settings, where the model can reference external documents during question answering. This opens avenues for research into how language models can be adapted for environments where they need to access and utilize external information dynamically.\\n\\n5. **Evaluation Across Domains**: The consistent improvements observed across various datasets, such as PubMed, HotpotQA, and Gorilla, suggest that the RAFT approach is broadly applicable. Future research could explore the adaptability of this training strategy across different domains and tasks, potentially leading to more generalized frameworks for domain-specific language model training.\\n\\nIn summary, the findings from the RAFT paper advocate for a more nuanced approach to training language models, emphasizing the integration of distractor documents, the importance of data composition, and the potential for enhanced reasoning capabilities. These insights are likely to shape future research directions in the field of domain-specific language model training.\\n\\n**Source:** RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhere does the RAFT methodology apply in the context of open-book exams for LLMs, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe RAFT methodology applies specifically in the context of \"domain-specific open-book exams\" for Large Language Models (LLMs). In this setting, the LLM is expected to answer questions using information from a specific set of documents that it has been fine-tuned on. The RAFT approach is designed to enhance the model\\'s ability to utilize this domain-specific knowledge effectively while also improving its performance in the presence of distractor documents.\\n\\n### Key Aspects of RAFT in Open-Book Exams:\\n\\n1. **Training with Contextual Documents**: RAFT trains the model using a combination of relevant (golden) documents and distractor documents. This training setup allows the model to learn to identify which documents are useful for answering a given question and which are not. The model is trained to generate answers based on the relevant documents while ignoring the distractors.\\n\\n2. **Chain-of-Thought Reasoning**: RAFT emphasizes the generation of answers that include a chain-of-thought reasoning process. This means that the model not only provides an answer but also explains how it arrived at that answer by referencing the relevant documents. This reasoning process is crucial for improving the model\\'s accuracy and its ability to handle complex questions.\\n\\n3. **Simulated Imperfect Retrieval**: The methodology incorporates a simulated retrieval process during training, where the model learns to deal with the imperfections of document retrieval. This is akin to preparing for an open-book exam where the student must navigate through various sources of information, some of which may not be relevant.\\n\\n4. **Robustness Against Distractors**: By training the model to recognize and disregard distractor documents, RAFT enhances the model\\'s robustness in real-world scenarios where not all retrieved documents will be relevant. This is particularly important in domain-specific applications where the accuracy of the answers is paramount.\\n\\n5. **Evaluation in Domain-Specific Contexts**: RAFT has been shown to improve performance across various datasets, including PubMed, HotpotQA, and Gorilla, indicating its effectiveness in adapting LLMs for specific domains where accurate information retrieval and reasoning are critical.\\n\\nIn summary, RAFT is a training methodology that prepares LLMs for domain-specific open-book exams by teaching them to effectively utilize relevant documents, reason through their answers, and handle distractors, thereby enhancing their overall performance in specialized tasks.\\n\\n**Source**: RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow does the RAFT methodology specifically address the challenges of retrieval-augmented generation in specialized contexts, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe RAFT methodology specifically addresses the challenges of retrieval-augmented generation (RAG) in specialized contexts by implementing several key strategies:\\n\\n1. **Focus on Accuracy**: RAFT emphasizes the importance of maximizing accuracy in specialized domains, where general knowledge reasoning is less relevant. The methodology is designed to ensure that the model can provide precise answers based on specific documents, which is crucial for tasks like question answering in legal or medical contexts.\\n\\n2. **Integration of Domain-Specific Knowledge**: By adapting pre-trained Large Language Models (LLMs) through fine-tuning, RAFT allows these models to learn and incorporate domain-specific knowledge that may not be present in their original training data. This is particularly important for applications that require up-to-date or specialized information, such as recent news articles or proprietary enterprise documents.\\n\\n3. **Enhanced Performance in In-Domain RAG**: The RAFT approach improves the model\\'s performance in domain-specific RAG settings by training it to effectively utilize relevant documents while ignoring distractor documents. This training process helps the model to reason accurately and provide contextually appropriate answers based on the retrieved information.\\n\\n4. **Robustness Against Distractors**: One of the significant challenges in RAG is the presence of irrelevant or distracting information. RAFT trains models to discern between relevant and irrelevant documents, enhancing their ability to navigate through potentially distracting information and arrive at the correct answer. This robustness is critical in real-world applications where the retrieval process may not always yield perfect results.\\n\\n5. **Learning Opportunities in Open-Book Settings**: RAFT capitalizes on the \"open-book\" nature of domain-specific tasks by preparing models to reference and utilize the information they have been trained on. This contrasts with traditional methods that may not fully leverage the learning opportunities available in fixed domain settings.\\n\\n6. **Chain-of-Thought Reasoning**: The methodology incorporates a chain-of-thought-style response, which aids in improving the model\\'s reasoning capabilities. By training the model to cite verbatim from relevant documents, RAFT enhances the clarity and accuracy of the answers generated.\\n\\nIn summary, the RAFT methodology effectively addresses the challenges of retrieval-augmented generation in specialized contexts by focusing on accuracy, integrating domain-specific knowledge, enhancing performance in RAG, ensuring robustness against distractors, maximizing learning opportunities, and employing chain-of-thought reasoning.\\n\\nSource: RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow does the adaptation of LLMs improve their ability to handle domain-specific tasks compared to their general training, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAdapting Large Language Models (LLMs) for domain-specific tasks significantly enhances their performance compared to their general training, as discussed in the paper \"RAFT: Adapting Language Model to Domain Specific RAG.\" Here are the key improvements highlighted in the text:\\n\\n1. **Maximizing Accuracy**: The adaptation process shifts the focus from general knowledge reasoning to maximizing accuracy based on specific documents. In specialized domains, such as legal or medical fields, the need for precise and relevant information is paramount, making general knowledge less critical.\\n\\n2. **Incorporating Domain-Specific Knowledge**: Through adaptation, LLMs can learn and integrate specialized knowledge that may not be included in their general training data. This is particularly important for applications requiring up-to-date or niche information, such as recent news articles or proprietary enterprise documents.\\n\\n3. **Enhanced Performance in Retrieval-Augmented Generation (RAG)**: The RAFT methodology improves the model\\'s ability to perform in domain-specific RAG settings. By training the model to effectively utilize relevant documents while ignoring distractor documents, RAFT enhances the model\\'s reasoning capabilities and accuracy in providing answers based on the context of retrieved information.\\n\\n4. **Robustness Against Distractors**: The adaptation process trains models to differentiate between relevant and irrelevant information, which is crucial in real-world applications where retrieval processes may yield imperfect results. This ability allows the model to navigate through distracting information and arrive at correct answers.\\n\\n5. **Maximizing Learning Opportunities**: The paper argues that existing methods often fail to leverage the learning opportunities presented by fixed domain settings. By fine-tuning LLMs, RAFT prepares models for the \"open-book\" nature of domain-specific tasks, enabling them to reference and utilize the information they have been trained on effectively.\\n\\nIn summary, the adaptation of LLMs for specialized domains is essential for improving accuracy, incorporating relevant knowledge, enhancing performance in specific tasks, ensuring robustness against irrelevant information, and maximizing learning opportunities in context-rich environments.\\n\\nSource: RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat role does accuracy play in the adaptation of LLMs for specialized domains, and why is it prioritized over general knowledge reasoning, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'? <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAccuracy plays a critical role in the adaptation of Large Language Models (LLMs) for specialized domains, as discussed in the paper \"RAFT: Adapting Language Model to Domain Specific RAG.\" The primary focus in these contexts is to maximize accuracy based on specific documents rather than relying on general knowledge reasoning. Here are the key points regarding the importance of accuracy and its prioritization over general knowledge reasoning:\\n\\n1. **Maximizing Accuracy**: In specialized domains, such as legal or medical fields, the need for precise and relevant information is paramount. The paper emphasizes that the goal shifts from general knowledge reasoning to ensuring that the model provides accurate answers based on the specific documents it has access to.\\n\\n2. **Incorporating Domain-Specific Knowledge**: Adapting LLMs allows them to learn and integrate knowledge that is specific to the domain, which may not be included in the general training data. This is particularly important for applications that require current or specialized information, such as recent news articles or proprietary enterprise documents.\\n\\n3. **Performance in Domain-Specific Retrieval-Augmented Generation (RAG)**: The RAFT methodology enhances the model\\'s performance in domain-specific RAG settings by training it to effectively utilize relevant documents while ignoring distractor documents. This focus on accuracy ensures that the model can reason and provide correct answers based on the context of the retrieved information.\\n\\n4. **Robustness Against Distractors**: The adaptation process trains models to differentiate between relevant and irrelevant information. This is crucial in real-world applications where the retrieval process may not yield perfect results, and the model must navigate through distracting information to arrive at the correct answer.\\n\\n5. **Enhancing Learning Opportunities**: The paper argues that existing methods often do not fully leverage the learning opportunities available in fixed domain settings. By fine-tuning LLMs, RAFT prepares models for the \"open-book\" nature of domain-specific tasks, allowing them to reference and utilize the information they have been trained on effectively.\\n\\nIn summary, accuracy is prioritized in the adaptation of LLMs for specialized domains because it directly impacts the model\\'s ability to provide relevant and precise answers, which is essential for effective performance in tasks that require domain-specific knowledge.\\n\\nSource: RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat are the key benefits of fine-tuning Large Language Models for specific industries, as outlined in the RAFT paper, \\'RAFT: Adapting Language Model to Domain Specific RAG\\'? <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAdapting Large Language Models (LLMs) for specific industries, as outlined in the RAFT paper \"RAFT: Adapting Language Model to Domain Specific RAG,\" offers several key benefits:\\n\\n1. **Maximizing Accuracy**: The primary focus in specialized domains is to enhance accuracy based on specific documents rather than general knowledge reasoning. This is particularly important for tasks like question answering in fields such as law or medicine, where precise information is critical.\\n\\n2. **Incorporating Domain-Specific Knowledge**: Fine-tuning allows LLMs to learn and integrate specialized knowledge that may not be included in the general training data. This is vital for applications that require current or niche information, such as recent news articles or proprietary enterprise documents.\\n\\n3. **Improving Performance in In-Domain Retrieval-Augmented Generation (RAG)**: The RAFT methodology enhances the model\\'s performance in domain-specific RAG settings by training it to effectively utilize relevant documents while disregarding distractor documents. This leads to improved reasoning and accuracy in responses based on the context of the retrieved information.\\n\\n4. **Robustness Against Distractors**: The adaptation process trains models to differentiate between relevant and irrelevant information, which is essential in real-world applications where retrieval may not always yield perfect results. This capability allows the model to navigate through distracting information to provide accurate answers.\\n\\n5. **Enhancing Learning Opportunities**: The paper argues that existing methods often do not fully exploit the learning opportunities available in fixed domain settings. By fine-tuning LLMs, RAFT prepares models for the \"open-book\" nature of domain-specific tasks, enabling them to reference and utilize the information they have been trained on effectively.\\n\\nIn summary, fine-tuning LLMs for specific industries is crucial for improving accuracy, integrating relevant knowledge, enhancing performance in specific tasks, ensuring robustness against irrelevant information, and maximizing learning opportunities in context-rich environments.\\n\\nSource: RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhy is the Chain-of-Thought (CoT) approach significant in enhancing the performance of language models according to the findings in the paper \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe Chain-of-Thought (CoT) approach is significant in enhancing the performance of language models as highlighted in the paper \\'RAFT: Adapting Language Model to Domain Specific RAG\\' for several reasons:\\n\\n1. **Improved Understanding**: The CoT approach enriches the model\\'s understanding by providing a reasoning chain that guides it to the answer. This is crucial because simply providing an answer can lead to overfitting, where the model learns to memorize answers rather than understand the underlying context. By incorporating a reasoning process, the model can better grasp the relationships and context within the data.\\n\\n2. **Reduction of Overfitting**: The paper notes that without CoT, the model may experience a rapid decrease in loss, which can lead to overfitting on concise answers. The CoT prompts help maintain a balance, allowing the model to learn more robustly and generalize better to unseen data.\\n\\n3. **Quantitative Improvements**: The findings in the paper demonstrate that integrating CoT significantly enhances training robustness. For instance, the results show gains of 9.66% and 14.93% in performance on the Hotpot QA and HuggingFace datasets, respectively, when CoT is included. This quantitative evidence underscores the effectiveness of the CoT approach in improving model accuracy.\\n\\n4. **Guided Reasoning**: The CoT prompts serve as a structured way for the model to process information, which is particularly beneficial in complex question-answering scenarios. By following a logical reasoning path, the model can derive answers that are more accurate and contextually relevant, as illustrated in the comparative example where the RAFT model correctly identifies the screenwriter, while the domain-specifically fine-tuned (DSF) model fails.\\n\\n5. **Training Methodology**: The paper emphasizes that employing CoT in the training methodology allows for a more comprehensive dataset that includes both standard instructional tuning and context comprehension. This dual approach is essential for preserving and enhancing the model\\'s ability to process text effectively.\\n\\nIn summary, the CoT approach is significant because it enhances the model\\'s understanding, reduces the risk of overfitting, provides quantitative performance improvements, guides reasoning, and contributes to a more effective training methodology. These factors collectively lead to a more robust and capable language model in the context of domain-specific retrieval-augmented generation tasks.\\n\\n**Source**: \\'RAFT: Adapting Language Model to Domain Specific RAG\\'<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nIn what ways does incorporating domain-specific knowledge into LLMs enhance their performance in specialized applications, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nIncorporating domain-specific knowledge into Large Language Models (LLMs) significantly enhances their performance in specialized applications, as discussed in the paper \"RAFT: Adapting Language Model to Domain Specific RAG.\" Here are the key ways this enhancement occurs:\\n\\n1. **Maximizing Accuracy**: The primary objective in specialized domains is to maximize accuracy based on specific documents rather than relying on general knowledge reasoning. The paper emphasizes that in contexts such as legal or medical document analysis, precise and relevant information is crucial for tasks like question answering.\\n\\n2. **Integration of Relevant Knowledge**: The adaptation process allows LLMs to learn and incorporate domain-specific knowledge that may not be included in the general training data. This is particularly important for applications requiring up-to-date or specialized information, such as recent news articles or proprietary enterprise documents created after the model\\'s training cutoff.\\n\\n3. **Enhanced Performance in Retrieval-Augmented Generation (RAG)**: The RAFT methodology improves LLM performance in domain-specific RAG settings by training models to effectively utilize relevant documents while ignoring distractor documents. This targeted training enhances the model\\'s reasoning capabilities and accuracy in providing answers based on the context of the retrieved information.\\n\\n4. **Robustness Against Distractors**: The RAFT approach trains models to differentiate between relevant and irrelevant information, which is essential in real-world applications where retrieval processes may yield imperfect results. This ability to navigate through distracting information is critical for arriving at correct answers.\\n\\n5. **Maximizing Learning Opportunities**: The paper argues that existing methods, such as in-context learning through RAG, often do not fully leverage the learning opportunities available in fixed domain settings. By fine-tuning LLMs, RAFT prepares models for the \"open-book\" nature of domain-specific tasks, enabling them to reference and utilize the information they have been trained on effectively.\\n\\nIn summary, incorporating domain-specific knowledge into LLMs through methods like RAFT is vital for improving accuracy, integrating relevant knowledge, enhancing performance in specific tasks, ensuring robustness against irrelevant information, and maximizing learning opportunities in context-rich environments.\\n\\nSource: RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow might the absence of distractor documents impact the performance of a language model in practical scenarios, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe absence of distractor documents in the RAFT training process can significantly impact the performance of a language model in practical scenarios. Here are the key points regarding this issue:\\n\\n1. **Reduced Robustness**: Without distractor documents, the model may become less robust against irrelevant information. The training process that includes distractors helps the model learn to identify and ignore non-contributory content. In real-world applications, where models often encounter a mix of relevant and irrelevant information, this ability is crucial for maintaining accuracy.\\n\\n2. **Lack of Real-World Simulation**: The training with only relevant documents does not adequately simulate real-world conditions, where models must sift through both useful and non-useful information. This lack of exposure can hinder the model\\'s ability to perform effectively in practical scenarios, as it may struggle to navigate through irrelevant data during inference.\\n\\n3. **Limited Contextual Learning**: Training solely on relevant documents may lead to a superficial understanding of the relationship between questions and answers. The inclusion of distractor documents encourages the model to derive answers from the context of relevant documents, fostering deeper reasoning and comprehension. Without this, the model may rely more on memorization rather than understanding.\\n\\n4. **Narrow Training Data Diversity**: The absence of distractor documents limits the diversity of training data. This can result in a model that is less adaptable to various scenarios it may encounter during inference. The inclusion of distractors helps the model generalize better, as it learns to generate answers based on context rather than solely on memorized information.\\n\\n5. **Weakened Chain-of-Thought Reasoning**: The RAFT methodology emphasizes a chain-of-thought reasoning process. Without distractor documents, the model may not develop the ability to articulate its reasoning effectively, which can lead to decreased accuracy in answering questions. The reasoning process is enhanced by the need to differentiate between relevant and irrelevant information.\\n\\nIn summary, the absence of distractor documents can lead to a less capable language model that struggles with robustness, contextual understanding, and reasoning in practical applications. This highlights the importance of incorporating distractors in the training process to prepare models for real-world challenges.\\n\\n**Source**: RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat challenges might arise when training a model without the use of distractor documents, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nTraining a model without the use of distractor documents, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\', presents several challenges:\\n\\n1. **Reduced Robustness**: Without distractor documents, the model may struggle to differentiate between relevant and irrelevant information. This lack of training on irrelevant content can lead to a model that is less robust in real-world scenarios where it encounters a mix of useful and non-useful information. The model may become overly reliant on the presence of relevant documents, failing to develop the necessary skills to ignore distractions.\\n\\n2. **Limited Real-World Simulation**: The absence of distractor documents means that the training does not accurately simulate real-world conditions, where models must sift through both relevant and irrelevant documents. This can hinder the model\\'s ability to perform effectively in practical applications, as it may not be prepared to handle the complexities of real-world data retrieval.\\n\\n3. **Inadequate Contextual Learning**: Training without distractor documents may lead to a superficial understanding of the relationship between questions and relevant information. The model might memorize answers rather than learn to derive them from context, which is essential for reasoning and generating accurate responses. This could result in a lack of depth in the model\\'s comprehension and reasoning capabilities.\\n\\n4. **Lack of Training Data Diversity**: The training data would lack the diversity that distractor documents provide. This diversity is crucial for helping the model generalize better to different scenarios it may encounter during inference. Without it, the model may perform well on training data but fail to adapt to new or varied contexts.\\n\\n5. **Weak Chain-of-Thought Reasoning**: The RAFT methodology emphasizes the importance of chain-of-thought reasoning, which is enhanced by the need to differentiate between relevant and irrelevant information. Without distractor documents, the model may not develop the ability to articulate its reasoning effectively, leading to poorer performance in answering questions.\\n\\nIn summary, training a model without distractor documents can significantly impair its robustness, contextual understanding, and reasoning abilities, ultimately affecting its performance in domain-specific retrieval-augmented generation tasks.\\n\\n**Source**: RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the RAFT method and how does it improve document processing capabilities in language models, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe RAFT method, as discussed in the text, is a training strategy designed to enhance the performance of language models (LLMs) in answering questions within specific domains, particularly in \"open-book\" settings where the model must reference external documents. RAFT stands for \"Retrieval-Augmented Fine-Tuning,\" and it incorporates several key techniques to improve document processing capabilities.\\n\\n### Key Features of RAFT:\\n\\n1. **Context Processing and Information Extraction**: The RAFT method addresses the model\\'s initial lack of training in context processing and extracting useful information. By incorporating RAFT, the model is trained to not only match its answering style with the required format but also to enhance its ability to process documents effectively. This dual focus allows the model to derive relevant context from the provided documents, which is crucial for accurate question answering.\\n\\n2. **Chain-of-Thought (CoT) Integration**: RAFT employs a Chain-of-Thought approach, which involves providing a reasoning chain that guides the model to the answer while enriching its understanding of the context. This method helps prevent overfitting to concise answers by encouraging the model to engage in deeper reasoning. The text indicates that integrating CoT significantly enhances training robustness, as evidenced by performance improvements on various datasets, such as gains of 9.66% and 14.93% in the Hotpot QA and HuggingFace datasets, respectively.\\n\\n3. **Training with Distractor Documents**: RAFT also incorporates a strategy of training with both golden (highly relevant) documents and distractor (irrelevant) documents. This approach helps the model learn to discern and disregard irrelevant information, which is particularly important when the model encounters a mix of relevant and irrelevant content during retrieval-augmented generation (RAG) tasks. The training setup typically includes one golden document alongside four distractor documents, which has been shown to enhance the model\\'s robustness against irrelevant text.\\n\\n4. **Dynamic Training Data Composition**: The method explores the proportion of training data that should include golden documents. Findings suggest that including a portion of training data without the golden document can actually improve the model\\'s performance on RAG tasks. This indicates that a balanced training dataset, which sometimes lacks the correct context, can enhance the model\\'s ability to generalize and perform well in real-world scenarios.\\n\\n### Conclusion\\n\\nIn summary, the RAFT method improves document processing capabilities in language models by enhancing context comprehension, integrating reasoning through Chain-of-Thought, training with a mix of relevant and irrelevant documents, and dynamically adjusting the training data composition. These strategies collectively contribute to a more robust and effective model for answering questions based on external documents.\\n\\n**Source**: RAFT: Adapting Language Model to Domain Specific RAG.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat are the implications of training in a simulated imperfect retrieval environment for the model\\'s real-world application, as discussed in \\'RAFT: Adapting Language Model to Domain Specific RAG\\'?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe implications of training in a simulated imperfect retrieval environment for the model\\'s real-world application, as discussed in \"RAFT: Adapting Language Model to Domain Specific RAG,\" are significant for enhancing the model\\'s robustness and effectiveness in practical scenarios. Here are the key points:\\n\\n1. **Real-World Relevance**: By simulating an imperfect retrieval environment during training, RAFT prepares the model for real-world situations where the retrieval of relevant documents may not always be optimal. This reflects the challenges faced in practical applications, where the model must operate with potentially incomplete or irrelevant'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "1s_BgcwMuw6g",
        "outputId": "d25aa3c4-d705-4005-dafc-c088b14a6b3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'                                                                                                                                                                                                                                                                                                                                                                                    \\n\\nKnowledge distillation enhances the capabilities of smaller models derived from large language models (LLMs) through several mechanisms, as discussed in the paper \"What is the Role of Small Models in the LLM Era: A Survey.\" Here are the key mechanisms:\\n\\n1. **Knowledge Transfer**: Knowledge distillation involves training a smaller student model to replicate the behavior of a larger teacher model. This process allows the student model to learn from the rich representations and capabilities of the teacher model, enabling it to achieve performance levels that are comparable to those of larger models despite having fewer parameters.\\n\\n2. **Efficiency and Resource Optimization**: Smaller models are generally more efficient in terms of computational resources, making them suitable for deployment in environments with limited computational power. Knowledge distillation allows these smaller models to maintain high performance without the extensive resource requirements associated with LLMs, which is particularly beneficial for applications on mobile devices or in edge computing scenarios.\\n\\n3. **Task-Specific Fine-Tuning**: Smaller models can be fine-tuned on specific tasks using the knowledge gained from LLMs. This is especially advantageous in domains where data is scarce or specialized, such as in biomedical or legal fields. By leveraging the extensive training and generalization capabilities of LLMs, smaller models can improve their effectiveness in niche applications.\\n\\n4. **Mitigating Diminishing Returns**: The paper notes that increasing model sizes often leads to diminishing returns in performance across various tasks. Knowledge distillation helps address this issue by allowing smaller models to capture essential features and skills from larger models, thus optimizing performance without necessitating larger model sizes.\\n\\n5. **Robustness and Efficacy**: Fine-tuning smaller models with augmented samples generated through knowledge distillation can significantly enhance their robustness and efficacy. This approach enables smaller models to perform well even in challenging scenarios, where they might otherwise struggle due to limited training data.\\n\\n6. **Advanced Distillation Techniques**: The paper discusses various advanced techniques within knowledge distillation, such as Chain-of-Thought distillation, which extracts rationales from LLMs to provide additional supervision, thereby enhancing the reasoning capabilities of smaller models. Instruction Following Distillation is another technique aimed at improving the zero-shot performance of smaller models by fine-tuning them with instruction-like prompt-response pairs.\\n\\nIn summary, knowledge distillation is a crucial technique that enables smaller models to harness the strengths of LLMs, improving their performance, efficiency, and applicability in various contexts while addressing challenges related to computational constraints and data scarcity.\\n\\n**Source**: \"What is the Role of Small Models in the LLM Era: A Survey\"<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                                  \\n\\nRetrieval Augmented Generation (RAG) primarily utilizes three types of documents for extracting relevant information:\\n\\n1. **Textual Documents**: This is the most common source for RAG methods, which includes resources like Wikipedia, cross-lingual texts, and domain-specific corpora (e.g., medical and legal domains). Lightweight retrieval models, such as sparse BM25 and dense BERT-based retrievers, are typically employed to extract relevant text from these sources.\\n\\n2. **Structured Knowledge**: This category includes verified sources such as knowledge bases and databases that provide precise information. Examples include KnowledgeGPT, which allows LLMs to retrieve information from knowledge bases, and T-RAG, which enhances answers by concatenating retrieved tables with the query. StructGPT further augments generation by retrieving from hybrid sources, including knowledge bases, tables, and databases.\\n\\n3. **Other Sources**: This encompasses codes, tools, and images, enabling LLMs to leverage external information for enhanced reasoning. For instance, DocPrompting uses a BM25 retriever to obtain relevant code documentation before code generation, while Toolformer demonstrates that LMs can self-learn to use external tools, such as translators and calculators, through simple APIs.\\n\\nBy integrating these diverse sources, RAG effectively addresses the challenge of generating factually inaccurate content, often referred to as hallucinations.\\n\\nSource: \\'What is the Role of Small Models in the LLM Era: A Survey\\'<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                              \\n\\nSmall models can enhance the sustainability of AI practices in terms of resource consumption in several ways, as discussed in the paper \"What is the Role of Small Models in the LLM Era: A Survey.\" Here are the key points:\\n\\n1. **Resource Efficiency**: Small models (SMs) are inherently more resource-efficient compared to large language models (LLMs). They require significantly less computational power and energy for training and inference, making them suitable for deployment in environments with limited resources. This efficiency can lead to lower operational costs and reduced environmental impact.\\n\\n2. **Cost-Effectiveness**: The use of small models can lead to substantial reductions in the costs associated with deploying AI systems. Organizations can implement AI solutions more sustainably by utilizing smaller models for specific applications, which can help in conserving financial and computational resources.\\n\\n3. **Task-Specific Adaptation**: Small models can be fine-tuned for specific tasks more easily than larger models. This adaptability allows for quicker deployment and more tailored solutions, which can be particularly beneficial in industries that require rapid responses and efficient resource use.\\n\\n4. **Data Selection Techniques**: The survey highlights the importance of data selection methods, such as importance resampling, which can optimize the training of small models. By improving the performance of small models with less data, these techniques enable effective training in low-resource settings, further enhancing sustainability.\\n\\n5. **Collaboration with Large Models**: Small models can complement large models by serving as plug-ins for specific tasks. This hybrid approach allows organizations to leverage the strengths of both model types, optimizing resource usage while maintaining high performance.\\n\\n6. **Accessibility and Ethical Considerations**: The use of small models democratizes access to AI technologies, enabling smaller organizations and developers to utilize AI without the need for extensive computational resources. This increased accessibility can foster innovation and promote ethical AI practices by allowing a broader range of stakeholders to participate in AI development.\\n\\nIn summary, small models contribute to more sustainable AI practices by being resource-efficient, cost-effective, adaptable, and accessible, ultimately leading to reduced environmental impact and promoting ethical considerations in AI deployment.\\n\\n**Source**: The insights are derived from the paper \"What is the Role of Small Models in the LLM Era: A Survey.\"<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                                 \\n\\nPrompt-based learning facilitates few-shot or zero-shot learning in large language models (LLMs) by utilizing carefully crafted prompts that include demonstration examples structured within natural language templates. This approach leverages the concept of In-Context Learning (ICL), which allows LLMs to adapt to new tasks without requiring parameter updates. Instead, the model relies on the context provided by the prompts to understand and perform the task at hand.\\n\\nKey aspects of how prompt-based learning achieves this include:\\n\\n1. **Minimal Data Requirement**: By embedding a few examples within the prompt, LLMs can generalize from these instances to perform tasks with little to no labeled data. This is particularly useful in scenarios where obtaining labeled data is challenging or resource-intensive.\\n\\n2. **Use of Small Models**: Small models can enhance the prompt-based learning process by optimizing prompts or generating them autonomously. For example, the Uprise method utilizes a lightweight retriever to automatically fetch prompts for zero-shot tasks, reducing the manual effort involved in prompt engineering.\\n\\n3. **Decomposing Complex Problems**: Techniques like DaSLaM employ small models to break down complex problems into simpler subproblems. This decomposition allows the larger models to tackle tasks that require fewer reasoning steps, thereby improving performance across various reasoning datasets.\\n\\n4. **Verification and Output Refinement**: Small models can also be used to verify or rewrite the outputs generated by LLMs. This verification process helps ensure that the responses are accurate and relevant, leading to better overall performance without the need for fine-tuning the larger models.\\n\\nIn summary, prompt-based learning effectively enables few-shot and zero-shot learning in LLMs by utilizing structured prompts, leveraging small models for optimization and verification, and allowing LLMs to adapt to new tasks with minimal data input.\\n\\nSource: \\'What is the Role of Small Models in the LLM Era: A Survey\\'<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                                 \\n\\nLightweight retrieval models play a crucial role in the context of Retrieval Augmented Generation (RAG) by efficiently extracting relevant document fragments from various sources. According to the paper \\'What is the Role of Small Models in the LLM Era: A Survey\\', RAG enhances the capabilities of Large Language Models (LLMs) by addressing their limitations in memorizing specific knowledge and handling domain-specific tasks.\\n\\nThe document sources utilized in RAG can be categorized into three main types:\\n\\n1. **Textual Documents**: This is the most prevalent source for RAG methods, which includes resources like Wikipedia, cross-lingual texts, and domain-specific corpora (e.g., medical and legal texts). Lightweight retrieval models, such as sparse BM25 and dense BERT-based retrievers, are commonly employed to extract relevant text from these sources. These models are designed to be efficient and effective, allowing for quick retrieval of pertinent information that can enhance the generation process.\\n\\n2. **Structured Knowledge**: This category includes verified sources such as knowledge bases and databases, which provide precise information. For instance, KnowledgeGPT enables LLMs to retrieve data from knowledge bases, while T-RAG enhances responses by integrating retrieved tables with the query. StructGPT further improves generation by retrieving from hybrid sources, including knowledge bases and databases, showcasing the versatility of lightweight models in accessing structured information.\\n\\n3. **Other Sources**: This encompasses a variety of resources, including code, tools, and images, which allow LLMs to utilize external information for improved reasoning. For example, DocPrompting uses a BM25 retriever to gather relevant code documentation prior to code generation, and Toolformer demonstrates how LMs can learn to use external tools through APIs, enhancing their performance.\\n\\nBy leveraging these diverse sources through lightweight retrieval models, RAG effectively mitigates the issue of generating factually inaccurate content, often referred to as hallucinations. This integration of external knowledge not only enhances the accuracy of the generated content but also broadens the knowledge boundaries of LLMs.\\n\\nSource: \\'What is the Role of Small Models in the LLM Era: A Survey\\'<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                                   \\n\\nRetrieval Augmented Generation (RAG) can utilize various structured knowledge sources, which differ from traditional textual documents in several ways. According to the paper \\'What is the Role of Small Models in the LLM Era: A Survey\\', structured knowledge sources include:\\n\\n1. **Knowledge Bases and Databases**: These sources are typically verified and provide precise information. For instance, KnowledgeGPT allows LLMs to retrieve information directly from knowledge bases, ensuring that the data is accurate and reliable. This contrasts with textual documents, which may contain unverified or less reliable information.\\n\\n2. **Enhanced Retrieval Techniques**: RAG methods like T-RAG enhance the generation process by concatenating retrieved tables with the query, allowing for more structured and organized information retrieval. This structured approach is different from the more free-form retrieval of textual documents, which may not have a defined structure.\\n\\n3. **Hybrid Sources**: StructGPT further augments generation by retrieving from a combination of knowledge bases, tables, and databases. This hybrid approach allows RAG to leverage structured data alongside traditional text, providing a richer context for generating responses.\\n\\nIn summary, structured knowledge sources in RAG provide verified, precise information and utilize advanced retrieval techniques that differ from the more general and potentially less reliable nature of textual documents. This capability helps mitigate issues like hallucinations in LLM outputs by ensuring that the information used is accurate and well-structured.\\n\\nSource: \\'What is the Role of Small Models in the LLM Era: A Survey\\'<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                                \\n\\nRetrieval methods like BM25 and BERT-based retrievers function within the Retrieval Augmented Generation (RAG) framework by serving as lightweight models that extract relevant document fragments from various sources. RAG enhances the capabilities of Large Language Models (LLMs) by incorporating external knowledge, which helps mitigate the generation of factually inaccurate content, often referred to as hallucinations.\\n\\n1. **BM25**: This is a sparse retrieval model that operates based on term frequency and inverse document frequency, allowing it to rank documents based on their relevance to a given query. It is particularly effective for textual documents, which are the most commonly used retrieval sources in RAG methods. BM25 retrieves relevant text from resources such as Wikipedia and domain-specific corpora, making it a valuable tool for enhancing the information available to LLMs.\\n\\n2. **BERT-based Retrievers**: These are dense retrieval models that leverage the contextual embeddings generated by BERT to understand the semantic meaning of queries and documents. BERT-based retrievers can capture more nuanced relationships between words and phrases, allowing for more accurate retrieval of relevant information. This is especially useful in scenarios where the context of the query is critical for finding the right documents.\\n\\nBoth BM25 and BERT-based retrievers are integral to the RAG framework as they enable the extraction of relevant information from diverse sources, including structured knowledge bases and other tools. For instance, KnowledgeGPT allows LLMs to retrieve information from knowledge bases, while T-RAG enhances answers by concatenating retrieved tables with the query. This integration of retrieval methods ensures that LLMs can access up-to-date and domain-specific knowledge, thereby improving their performance on various tasks.\\n\\nIn summary, BM25 and BERT-based retrievers play crucial roles in RAG by efficiently sourcing relevant information, which enhances the overall capabilities of LLMs and addresses their limitations in knowledge retention and accuracy.\\n\\nSource: \\'What is the Role of Small Models in the LLM Era: A Survey\\'<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                                 \\n\\nRetrieval Augmented Generation (RAG) enhances the functionality of Large Language Models (LLMs) by integrating information from various external sources, as discussed in the paper \\'What is the Role of Small Models in the LLM Era: A Survey\\'. These sources are categorized into three main types:\\n\\n1. **Textual Documents**: This is the primary source for RAG methods, which includes widely used resources like Wikipedia, cross-lingual texts, and domain-specific corpora (e.g., medical and legal texts). RAG employs lightweight retrieval models, such as sparse BM25 and dense BERT-based retrievers, to extract relevant text from these documents, thereby enriching the LLM\\'s responses with accurate and contextually relevant information.\\n\\n2. **Structured Knowledge**: This category includes verified sources such as knowledge bases and databases, which provide precise information. For instance, KnowledgeGPT allows LLMs to retrieve data from knowledge bases, while T-RAG enhances responses by integrating retrieved tables with the query. StructGPT further improves generation by retrieving from a combination of knowledge bases, tables, and databases, ensuring that the information is both accurate and relevant.\\n\\n3. **Other Sources**: This encompasses codes, tools, and even images, which allow LLMs to utilize external information for improved reasoning. An example is DocPrompting, which uses a BM25 retriever to access relevant code documentation before generating code. Toolformer illustrates how LMs can learn to use external tools, such as translators and calculators, through simple APIs, thereby expanding their functional capabilities.\\n\\nBy leveraging these diverse sources, RAG effectively addresses the challenge of generating factually inaccurate content, commonly referred to as hallucinations, thus enhancing the reliability and accuracy of LLM outputs.\\n\\nSource: \\'What is the Role of Small Models in the LLM Era: A Survey\\'<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                                 \\n\\nThe collaboration between Small Models (SMs) and Large Language Models (LLMs) can lead to more efficient AI systems in several ways, as discussed in the paper \"What is the Role of Small Models in the LLM Era: A Survey\":\\n\\n1. **Resource Efficiency**: LLMs are resource-intensive, requiring substantial computational power and energy. In contrast, SMs are more resource-efficient, making them suitable for environments with limited resources. This efficiency allows for the deployment of AI systems that can operate effectively without the high costs associated with LLMs.\\n\\n2. **Data Curation**: SMs can enhance LLMs by curating high-quality training data. This involves selecting and refining pre-training and instruction-tuning datasets, which can improve the performance of LLMs. By leveraging SMs for data curation, the overall training process becomes more efficient, leading to better-performing models with potentially less data.\\n\\n3. **Task Specialization**: While LLMs are designed to handle a wide range of tasks, SMs can be fine-tuned for specific applications, often outperforming LLMs in those areas. This specialization allows for the development of tailored solutions that are both effective and efficient, addressing specific needs without the overhead of a larger model.\\n\\n4. **Interpretability**: SMs are generally more interpretable than LLMs, which is crucial in fields like healthcare and finance where understanding model decisions is essential. By integrating SMs into AI systems, organizations can benefit from the interpretability of smaller models while still leveraging the power of LLMs for complex tasks.\\n\\n5. **Scalability and Flexibility**: The collaboration between SMs and LLMs allows for the creation of scalable systems that can adapt to various tasks and requirements. This balance between the power of LLMs and the efficiency of SMs enables the development of AI solutions that are both high-performing and cost-effective.\\n\\nIn summary, the collaboration between SMs and LLMs fosters a more efficient AI ecosystem by enhancing resource usage, improving data quality, enabling task specialization, increasing interpretability, and providing scalability and flexibility in AI applications.\\n\\nSource: Chen, L., & Varoquaux, G. (2024). What is the Role of Small Models in the LLM Era: A Survey.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                                \\n\\nUsing biased or toxic data in the instruction tuning of small language models can lead to several significant consequences, as discussed in \"What is the Role of Small Models in the LLM Era: A Survey.\" Here are the key potential consequences:\\n\\n1. **Degraded Model Performance**: The presence of biased or toxic data can adversely affect the performance of small language models. When models are trained on such data, they may learn and replicate these biases, resulting in outputs that are not only inaccurate but also potentially harmful or misleading.\\n\\n2. **Ethical Implications**: Training on biased or toxic datasets raises ethical concerns. Models may perpetuate harmful stereotypes, misinformation, or toxic language, which can have real-world consequences, particularly in sensitive applications. The paper emphasizes the importance of curating datasets that promote fairness and reduce the risk of ethical violations.\\n\\n3. **Generalization Issues**: Models trained on biased data may struggle to generalize effectively to real-world scenarios. If the training data does not accurately represent the diversity of inputs the model will encounter, it may fail to perform well in practical applications, leading to a lack of reliability.\\n\\n4. **Impact on User Trust**: The use of biased or toxic data can erode user trust in AI systems. If users encounter outputs that reflect biases or toxic language, they may lose confidence in the model\\'s capabilities and its ethical grounding, which can hinder the adoption of AI technologies.\\n\\n5. **Evaluation Challenges**: The evaluation of instruction-tuned models is heavily influenced by the quality of the data used for tuning. If the evaluation datasets contain biases or errors, it can mislead the assessment of the model\\'s capabilities, resulting in an inaccurate understanding of its performance.\\n\\nIn summary, the paper highlights that the quality and ethical considerations of training data are foundational to the success of instruction tuning in small language models, affecting their performance, ethical implications, generalization capabilities, user trust, and evaluation accuracy.\\n\\n**Source**: \"What is the Role of Small Models in the LLM Era: A Survey.\"<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                                   \\n\\nInterpretability plays a crucial role in the preference for smaller models in high-stakes applications like healthcare and finance, as discussed in \"What is the Role of Small Models in the LLM Era: A Survey.\" Here are the key points regarding this aspect:\\n\\n1. **Human-Understandable Explanations**: The primary goal of interpretability is to provide explanations of a model\\'s internal reasoning process in a way that is understandable to humans. Smaller models, such as shallow or tree-based models, typically offer better interpretability compared to larger, more complex models like deep neural networks. This transparency is essential in fields where decisions must be easily understood by non-experts, such as doctors or financial analysts.\\n\\n2. **Trust and Usability**: In high-stakes environments, the ability to audit and explain model decisions is critical. Smaller models are often preferred because they can produce decisions that are more easily interpretable, fostering trust among users who need to rely on these models for important decisions. For instance, in healthcare, a doctor must understand the rationale behind a model\\'s recommendation to make informed decisions about patient care.\\n\\n3. **Regulatory Compliance**: Industries like finance are subject to strict regulations that require clear explanations for automated decisions. Smaller models, due to their inherent simplicity, can more readily meet these regulatory demands, making them a more suitable choice for applications where compliance is necessary.\\n\\n4. **Trade-offs in Model Selection**: When selecting models for specific tasks, there is a need to balance model complexity with the requirement for human understanding. Smaller models can provide sufficient performance while ensuring that their workings are transparent, which is particularly important in scenarios where the consequences of decisions are significant.\\n\\nIn summary, the interpretability of smaller models makes them particularly advantageous in high-stakes applications, where understanding model decisions is essential for trust, usability, and compliance with regulations.\\n\\n**Source**: \"What is the Role of Small Models in the LLM Era: A Survey\"<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                             \\n\\nEvaluating open-ended text generated by large language models (LLMs) presents several significant challenges, as highlighted in the paper \"What is the Role of Small Models in the LLM Era: A Survey.\" Here are the key challenges discussed:\\n\\n1. **Limitations of Traditional Evaluation Metrics**: Traditional evaluation methods, such as BLEU and ROUGE, focus primarily on surface-form similarity. These metrics often fail to capture the nuanced semantic meaning and compositional diversity of the generated text. As a result, they may not accurately reflect the quality or relevance of the output.\\n\\n2. **Nuanced Semantic Understanding**: Open-ended text generation involves producing content that can vary widely in terms of meaning and context. Evaluating such text requires a deeper understanding of semantics, which traditional metrics do not provide. This complexity makes it difficult to assess the quality of generated text effectively.\\n\\n3. **Compositional Diversity**: The diversity in how information can be expressed in natural language adds another layer of difficulty. Generated texts can be valid and high-quality even if they do not match the reference outputs closely, making it challenging to evaluate them using conventional metrics.\\n\\n4. **Automated Evaluation Approaches**: To address these limitations, the paper discusses model-based evaluation approaches that utilize smaller models to automatically assess performance. However, the effectiveness of these approaches in capturing the full range of quality in generated text remains a challenge.\\n\\n5. **Uncertainty Estimation**: Some methods use small natural language inference (NLI) models to estimate the uncertainty of LLM responses. This introduces additional complexity, as accurately gauging uncertainty in generated text is inherently difficult.\\n\\nOverall, the evaluation of open-ended text generated by LLMs is complicated by the inadequacy of traditional metrics, the need for nuanced semantic understanding, and the challenges associated with automated evaluation methods.\\n\\n**Source**: \"What is the Role of Small Models in the LLM Era: A Survey\" (Chang et al., 2024).<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                             \\n\\nThe significance of using domain-specific corpora in Retrieval Augmented Generation (RAG) lies in its ability to enhance the performance of large language models (LLMs) by providing them with relevant, specialized knowledge that they may not possess inherently. As discussed in the paper \\'What is the Role of Small Models in the LLM Era: A Survey\\', RAG employs lightweight retrieval models to extract pertinent document fragments from various sources, including domain-specific corpora, which are crucial for tasks requiring specialized expertise.\\n\\n1. **Enhanced Knowledge Retrieval**: Domain-specific corpora, such as those from medical or legal fields, allow RAG to retrieve highly relevant information that is tailored to specific queries. This targeted retrieval helps mitigate the limitations of LLMs, which may struggle with tasks that require up-to-date or specialized knowledge.\\n\\n2. **Mitigating Hallucinations**: By incorporating external knowledge from domain-specific sources, RAG effectively reduces the occurrence of hallucinationsâ€”instances where the model generates factually incorrect or nonsensical information. The use of verified and precise information from these corpora enhances the factual accuracy of the generated content.\\n\\n3. **Improved Contextual Understanding**: The integration of domain-specific knowledge into the LLM\\'s input allows for a better contextual understanding of queries. This is particularly important in fields where terminology and concepts are specialized and may not be well-represented in general training datasets.\\n\\n4. **Efficiency in Resource Utilization**: Utilizing domain-specific corpora can be more resource-efficient than training large models from scratch for every specific domain. Instead, a lightweight retriever can be employed to access relevant information, allowing for quicker adaptations to various tasks without the need for extensive retraining.\\n\\nIn summary, the use of domain-specific corpora in RAG\\'s retrieval process is significant as it enhances the model\\'s ability to provide accurate, relevant, and contextually appropriate responses, thereby improving overall performance in specialized tasks.\\n\\nSource: \\'What is the Role of Small Models in the LLM Era: A Survey\\'<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                                \\n\\nThe integration of diverse sources in Retrieval Augmented Generation (RAG) significantly reduces the occurrence of factual inaccuracies, often referred to as hallucinations, by leveraging a variety of retrieval methods to access external knowledge. According to the paper \\'What is the Role of Small Models in the LLM Era: A Survey\\', RAG employs a lightweight retriever to extract relevant document fragments from three main types of sources:\\n\\n1. **Textual Documents**: This is the most prevalent source type in RAG, including resources like Wikipedia, cross-lingual texts, and domain-specific corpora (e.g., medical and legal texts). RAG utilizes lightweight retrieval models, such as sparse BM25 and dense BERT-based retrievers, to effectively extract pertinent information from these textual sources. By drawing from a wide array of documents, RAG can provide more accurate and contextually relevant information.\\n\\n2. **Structured Knowledge**: This category includes verified sources such as knowledge bases and databases, which offer precise information. For instance, KnowledgeGPT allows LLMs to retrieve data from knowledge bases, while T-RAG enhances responses by integrating retrieved tables with the query. The use of structured knowledge ensures that the information is reliable and reduces the likelihood of inaccuracies.\\n\\n3. **Other Sources**: This encompasses various forms of information, including code, tools, and images. For example, DocPrompting retrieves relevant code documentation to assist in code generation, while Toolformer enables LMs to learn to use external tools like translators and calculators. This diversity in sources allows RAG to enhance reasoning capabilities and provide accurate outputs.\\n\\nBy incorporating knowledge from these varied sources, RAG effectively addresses the limitations of LLMs, which may struggle with memorizing specific knowledge or providing up-to-date information. The integration of external knowledge not only enriches the content generated but also mitigates the risk of generating factually incorrect information, thereby improving the overall reliability of the outputs.\\n\\nSource: \\'What is the Role of Small Models in the LLM Era: A Survey\\'<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                            \\n\\nThe weak-to-strong paradigm is a notable approach in AI development, particularly in the context of aligning large language models (LLMs) with human values and preferences. This paradigm contrasts with other alignment strategies, such as reinforcement learning with human feedback (RLHF), by utilizing smaller, less capable models as supervisors for larger models. Hereâ€™s a detailed comparison of the weak-to-strong paradigm with other alignment strategies:\\n\\n### Key Features of the Weak-to-Strong Paradigm:\\n\\n1. **Supervision by Smaller Models**: The weak-to-strong paradigm employs smaller models to generate labels or guidance for larger models. This method allows the larger models to learn from the outputs of the smaller models, which is particularly beneficial when evaluating complex outputs that may exceed human understanding. For example, smaller models can assess the correctness and safety of outputs generated by LLMs, addressing the challenges posed by the increasing complexity of tasks.\\n\\n2. **Generalization Beyond Limitations**: By fine-tuning large models on labels produced by smaller models, the larger models can generalize better and potentially surpass the limitations of their weaker supervisors. This is crucial in scenarios where human evaluators may struggle to provide adequate supervision due to the complexity of the tasks involved.\\n\\n3. **Diverse Supervision**: The paradigm advocates for using a diverse set of specialized weak models instead of relying on a single generalist model. This diversity enhances the supervision process, leading to a more robust alignment of the larger model with human preferences.\\n\\n4. **Reliability Estimation**: Incorporating reliability estimation across multiple outputs from weak models can improve the alignment process. This method helps filter out uncertain data and adjust the weight of reliable data, enhancing the overall effectiveness of the alignment.\\n\\n5. **Broad Applicability**: The weak-to-strong paradigm is not confined to language models; it has also been applied to other domains, such as vision foundation models, demonstrating its potential impact across various fields in AI.\\n\\n### Comparison with Other Alignment Strategies:\\n\\n- **Reinforcement Learning with Human Feedback (RLHF)**: While RLHF focuses on rewarding behaviors favored by humans and penalizing those rated poorly, the weak-to-strong paradigm provides a structured way to leverage smaller models to guide larger models. RLHF can be resource-intensive and may struggle with the increasing complexity of tasks, whereas the weak-to-strong paradigm can effectively manage this complexity by utilizing smaller models for supervision.\\n\\n- **Direct Human Supervision**: Traditional alignment strategies often rely on direct human supervision, which can be limited by human evaluators\\' capacity to assess complex outputs. The weak-to-strong paradigm mitigates this limitation by using smaller models to provide guidance, allowing for more scalable and effective alignment as LLMs evolve into superhuman models.\\n\\n### Future Directions:\\nThe paper suggests that while the weak-to-strong framework is effective, there remains a gap in performance recovery between weak and strong models. Future research should focus on ensuring that the strong model has a deep understanding of the task and can correct the weak modelâ€™s errors. Additionally, a deeper understanding of the mechanisms governing the success of alignment methods is necessary.\\n\\nIn summary, the weak-to-strong paradigm offers a promising alternative to traditional alignment strategies by effectively utilizing smaller models to enhance the alignment of larger models with human values, particularly as LLMs become increasingly complex and capable.\\n\\n**Source**: Chen, L., & Varoquaux, G. (2024). What is the Role of Small Models in the LLM Era: A Survey.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                           \\n\\nUncertainty estimation can significantly enhance the evaluation of text generated by large language models (LLMs) by addressing the limitations of traditional evaluation metrics and providing a more nuanced understanding of the generated content. As discussed in \"What is the Role of Small Models in the LLM Era: A Survey,\" the evaluation of open-ended text generation faces several challenges, including the inadequacy of metrics like BLEU and ROUGE, which primarily focus on surface-form similarity and often fail to capture the semantic richness and compositional diversity of the text.\\n\\n1. **Addressing Limitations of Traditional Metrics**: Traditional metrics do not adequately reflect the quality or relevance of generated text, as they overlook the deeper semantic meanings and the variety of valid expressions in natural language. Uncertainty estimation can help by providing insights into the confidence levels of the generated outputs, allowing evaluators to discern which outputs are more reliable or meaningful.\\n\\n2. **Nuanced Semantic Understanding**: Evaluating open-ended text requires a sophisticated understanding of semantics, which traditional metrics lack. By employing uncertainty estimation, evaluators can better gauge the semantic coherence and relevance of the generated text, as it allows for a more detailed analysis of the model\\'s responses.\\n\\n3. **Compositional Diversity**: The inherent diversity in language means that multiple valid outputs can exist for a given prompt. Uncertainty estimation can help identify high-quality outputs that may not closely match reference texts but are still contextually appropriate and meaningful.\\n\\n4. **Automated Evaluation Approaches**: The paper discusses the use of smaller models to automate the evaluation process. By incorporating uncertainty estimation through small natural language inference (NLI) models, evaluators can assess the reliability of LLM responses more effectively. This method introduces an additional layer of complexity but ultimately enhances the evaluation process by providing a clearer picture of the model\\'s performance.\\n\\n5. **Proxy Models for Performance Prediction**: Uncertainty estimation can also be utilized in proxy models to predict LLM performance, which can reduce computational costs associated with model selection and fine-tuning. This approach allows for a more efficient evaluation process while still capturing the essential qualities of the generated text.\\n\\nIn summary, uncertainty estimation improves the evaluation of text generated by LLMs by addressing the shortcomings of traditional metrics, enhancing semantic understanding, accommodating compositional diversity, and facilitating automated evaluation methods. This leads to a more comprehensive assessment of the quality and relevance of generated content.\\n\\n**Source**: \"What is the Role of Small Models in the LLM Era: A Survey\" (Chang et al., 2024).<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                            \\n\\nThe choice of evaluation datasets significantly impacts the assessment of instruction-tuned models, as discussed in \"What is the Role of Small Models in the LLM Era: A Survey.\" Here are the key points regarding this impact:\\n\\n1. **Model Performance Evaluation**: The evaluation datasets used to assess instruction-tuned models must be representative of the tasks the models are designed to perform. If the evaluation data does not align with the intended tasks, it can lead to misleading conclusions about the model\\'s capabilities. The paper emphasizes that the quality and relevance of the evaluation datasets are crucial for accurately gauging model performance.\\n\\n2. **Data Quality and Selection**: High-quality evaluation datasets are essential for providing a reliable assessment of the model\\'s performance. Poorly curated datasets can introduce noise and errors, which may skew the evaluation results. The paper highlights that using datasets that are carefully selected based on quality, coverage, and necessity can enhance the evaluation process.\\n\\n3. **Bias and Ethical Considerations**: The evaluation datasets should also be free from biases and toxic content. The paper discusses the importance of ensuring that the evaluation data promotes fairness and does not perpetuate harmful stereotypes. This ethical consideration is vital for developing responsible AI systems.\\n\\n4. **Generalization to Real-World Applications**: The choice of evaluation datasets affects the model\\'s ability to generalize to real-world applications. Datasets that closely mimic real-world scenarios can help in assessing how well the model will perform outside of controlled environments. The paper notes that task-aware data selection can significantly improve the model\\'s understanding and execution of instructions.\\n\\n5. **Feedback Mechanism**: The evaluation process itself serves as a feedback mechanism for further tuning and improvement of the models. If the evaluation datasets are flawed, the feedback may lead to incorrect adjustments in the model, hindering its development. The paper underscores that the evaluation datasets should be robust and accurately reflect the tasks to ensure effective model refinement.\\n\\nIn summary, the choice of evaluation datasets is critical in assessing instruction-tuned models, influencing their perceived performance, generalization capabilities, ethical implications, and the overall effectiveness of the tuning process.\\n\\n**Source**: \"What is the Role of Small Models in the LLM Era: A Survey.\"<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                             \\n\\nThe ethical considerations for selecting data for training small language models, as discussed in \"What is the Role of Small Models in the LLM Era: A Survey,\" include several key aspects:\\n\\n1. **Bias and Fairness**: It is crucial to ensure that the training data is free from biases that could lead to unfair or discriminatory outcomes. The paper emphasizes the importance of curating datasets that promote fairness and mitigate the risk of perpetuating harmful stereotypes or misinformation. This involves actively identifying and removing biased content from the training datasets.\\n\\n2. **Toxicity Reduction**: Alongside bias, the presence of toxic content in training data poses significant ethical challenges. The paper highlights the need to curate datasets that minimize toxic language and harmful narratives, which can adversely affect the model\\'s outputs and societal perceptions.\\n\\n3. **Data Quality**: The quality of the training data is paramount, as it directly impacts the model\\'s performance and reliability. High-quality data ensures that the model learns relevant patterns and information, which is essential for effective instruction tuning. Poor quality data can lead to unreliable outputs, which raises ethical concerns regarding the deployment of such models in real-world applications.\\n\\n4. **Task Relevance**: Selecting data that is relevant to the specific tasks the model is intended to perform is vital. The paper notes that instruction tuning requires datasets that closely align with the intended tasks, enhancing the model\\'s ability to generalize and perform effectively in practical scenarios.\\n\\n5. **Evaluation and Feedback**: The evaluation of instruction-tuned models is heavily dependent on the quality of the data used for tuning. If the evaluation datasets are not representative or contain errors, it can mislead the assessment of the model\\'s capabilities, leading to ethical implications regarding the trustworthiness of the model\\'s outputs.\\n\\nIn summary, the ethical considerations in data selection for small language models revolve around ensuring fairness, reducing toxicity, maintaining high data quality, ensuring task relevance, and conducting accurate evaluations. These factors are essential for developing responsible and ethical AI solutions.\\n\\n**Source**: \"What is the Role of Small Models in the LLM Era: A Survey.\"<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                              \\n\\nOrganizations can implement several strategies to optimize resource allocation in light of diminishing returns from larger models, as discussed in \"What is the Role of Small Models in the LLM Era: A Survey.\" Here are the key strategies:\\n\\n1. **Adopt Smaller Models**: Organizations should consider using smaller models that can achieve comparable performance to larger models but with significantly lower computational costs. The paper emphasizes that smaller models (SMs) are often more efficient and can be effectively utilized for many applications, particularly in resource-constrained environments.\\n\\n2. **Task-Specific Optimization**: Not all tasks require the complexity of large language models (LLMs). Organizations can focus on developing and deploying specialized smaller models tailored to specific tasks. This approach can enhance efficiency and performance while reducing unnecessary resource expenditure.\\n\\n3. **Utilize Knowledge Distillation**: Implementing techniques such as knowledge distillation can help smaller models achieve performance levels closer to those of larger models. This method involves training a smaller model to replicate the behavior of a larger, more complex model, thus optimizing resource use while maintaining effectiveness.\\n\\n4. **Enhance Interpretability**: Smaller models tend to be more interpretable than larger models, which is crucial in fields like healthcare and finance where understanding model decisions is essential. Organizations can leverage this interpretability to build trust and usability in high-stakes applications.\\n\\n5. **Collaborative Frameworks**: The paper suggests that a collaborative approach between LLMs and SMs can optimize resource usage. Organizations can use smaller models to enhance the performance of larger models, or vice versa, creating a balanced system that maximizes efficiency and effectiveness.\\n\\n6. **Focus on Research and Development**: There is a need for ongoing research to explore alternative training methods and model architectures that can improve performance without increasing model size. Organizations should invest in R&D to discover innovative solutions that enhance the capabilities of smaller models.\\n\\nBy implementing these strategies, organizations can better manage their resources while still achieving high performance in their AI applications.\\n\\n**Source**: \"What is the Role of Small Models in the LLM Era: A Survey\"<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                                 \\n\\nThe findings in \"What is the Role of Small Models in the LLM Era: A Survey\" have significant implications for the future workforce in AI development, particularly concerning skills related to small model optimization. Here are the key implications:\\n\\n1. **Hybrid Model Development**: The survey indicates that small models (SMs) can complement large language models (LLMs) by serving as efficient plug-ins. This suggests that future AI developers will need to acquire skills in integrating and optimizing both small and large models to create hybrid systems that leverage the strengths of each. Understanding how to balance the computational efficiency of small models with the performance capabilities of large models will be crucial.\\n\\n2. **Cost-Effective Solutions**: As organizations increasingly seek to implement AI solutions, the ability to utilize small models for specific applications can lead to more sustainable practices. This trend emphasizes the need for AI professionals to develop skills in optimizing small models to reduce computational costs and energy consumption, making AI deployment more feasible for businesses with limited resources.\\n\\n3. **Data Selection Techniques**: The survey highlights the importance of data selection methods, such as importance resampling, in optimizing small models. Future AI developers will need to focus on refining these techniques to enhance the performance of small models, particularly in low-resource settings. Skills in data curation and selection will become increasingly valuable.\\n\\n4. **Task-Specific Adaptation**: Small models are noted for their adaptability to specific tasks, which allows for faster deployment and tailored solutions. This implies that AI professionals should cultivate skills in fine-tuning and adapting small models for various applications across different industries, such as healthcare and finance.\\n\\n5. **Research and Innovation**: The findings suggest a shift in research focus towards the effective integration of small models into existing AI ecosystems. This will require AI developers to engage in innovative research and development, exploring new model architectures, training methodologies, and evaluation metrics that prioritize the unique advantages of small models.\\n\\n6. **Ethical and Accessibility Considerations**: The use of small models can democratize access to AI technologies, enabling smaller organizations to leverage AI without extensive computational resources. This accessibility fosters innovation and ethical AI practices, indicating that future AI professionals should be equipped with skills that promote inclusivity and responsible AI development.\\n\\nIn summary, the implications of the findings from the survey suggest that the future workforce in AI development will need to focus on hybrid model integration, cost-effective optimization, data selection techniques, task-specific adaptation, innovative research, and ethical considerations to effectively utilize small models alongside large language models.\\n\\n**Source**: The insights are derived from the paper \"What is the Role of Small Models in the LLM Era: A Survey.\"<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                                 \\n\\nTo ensure that the data selected for instruction tuning is both high-quality and relevant, several strategies can be employed as discussed in \"What is the Role of Small Models in the LLM Era: A Survey.\" Here are the key strategies:\\n\\n1. **Model-Oriented Data Selection (MoDS)**: This approach utilizes a small language model, such as DeBERTa, to evaluate instruction data based on criteria like quality, coverage, and necessity. By leveraging the capabilities of small models, researchers can systematically select data that is most beneficial for instruction tuning.\\n\\n2. **LESS Framework**: The LESS framework demonstrates how smaller models can be used to identify influential data not only for larger models but also across different model families. This targeted data selection technique optimizes the instruction tuning process by ensuring that the most relevant data is utilized.\\n\\n3. **Quality Assessment Techniques**: Employing classifiers trained to assess content quality can help in filtering out noisy, toxic, and irrelevant data. Techniques such as calculating perplexity scores with a proxy language model can also aid in selecting high-quality data that is more likely to enhance model performance.\\n\\n4. **Data Reweighting**: This strategy involves assigning domain-specific weights to different text sources, which can improve the generalization capabilities of the model. A small proxy model can be trained to set these weights, allowing for a more nuanced approach to data selection.\\n\\n5. **Curating Instruction-Tuning Data**: Research indicates that strong alignment can be achieved with smaller, carefully curated datasets. For instance, fine-tuning on just 1,000 well-selected instruction examples can yield a well-aligned model, emphasizing the principle that \"less is more\" in data selection.\\n\\n6. **Ethical Considerations**: Ensuring that the data is free from biases and toxic content is crucial. Curating datasets that promote fairness and reduce harmful stereotypes is essential for developing ethical AI solutions.\\n\\n7. **Evaluation and Feedback**: The quality of the evaluation datasets used for assessing instruction-tuned models is critical. If these datasets are not representative or contain errors, they can mislead the evaluation of the model\\'s capabilities.\\n\\nIn summary, employing targeted data selection techniques, ensuring quality and relevance, and addressing ethical considerations are vital strategies for effective instruction tuning in small language models.\\n\\n**Source**: \"What is the Role of Small Models in the LLM Era: A Survey.\"<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                          \\n\\nThe paper \"What is the Role of Small Models in the LLM Era: A Survey\" suggests several techniques to improve the interpretability of large language models (LLMs). Here are the key techniques highlighted in the text:\\n\\n1. **Attention Visualization**: This technique involves visualizing the attention weights of the model to understand which parts of the input data the model focuses on when making predictions. By analyzing these attention patterns, researchers can gain insights into the decision-making process of the model.\\n\\n2. **Feature Importance Analysis**: This method assesses the contribution of different features (input variables) to the model\\'s predictions. By identifying which features are most influential, practitioners can better understand how the model interprets input data and the rationale behind its outputs.\\n\\n3. **Layer-wise Relevance Propagation (LRP)**: LRP is a technique used to decompose the output of a model back to its input features, providing a clear mapping of how input features contribute to the final decision. This can help in identifying biases and understanding the model\\'s behavior in a more granular way.\\n\\n4. **Knowledge Distillation**: While primarily a technique for model compression, knowledge distillation can also enhance interpretability. By training a smaller model (student) to mimic the behavior of a larger model (teacher), the smaller model can retain some of the interpretability advantages while achieving comparable performance.\\n\\n5. **Collaboration Between Small and Large Models**: The paper discusses how small models can complement large models by providing interpretable outputs in scenarios where understanding the model\\'s reasoning is crucial. This collaboration can help strike a balance between performance and interpretability.\\n\\nThese techniques are essential for enhancing the transparency and understanding of LLMs, which often operate as \"black boxes\" due to their complexity. By applying these methods, researchers and practitioners can work towards more accountable and ethical AI systems.\\n\\n**Source**: \"What is the Role of Small Models in the LLM Era: A Survey\" by Lihu Chen and GaÃ«l Varoquaux.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                            \\n\\nEvaluating generated text from large language models (LLMs) is complicated by the need for nuanced semantic understanding, as discussed in \"What is the Role of Small Models in the LLM Era: A Survey.\" The paper outlines several key challenges related to this complexity:\\n\\n1. **Limitations of Traditional Metrics**: Traditional evaluation metrics like BLEU and ROUGE primarily assess surface-form similarity. These metrics often overlook the deeper semantic meanings and the compositional diversity present in generated text. Consequently, they may not accurately reflect the quality or relevance of the output, leading to potential misjudgments in evaluation.\\n\\n2. **Complexity of Open-Ended Generation**: Open-ended text generation can produce a wide variety of meanings and contexts. Evaluating such diverse outputs necessitates a more sophisticated understanding of semantics than what traditional metrics can provide. This requirement for deeper semantic insight complicates the evaluation process, as it becomes challenging to determine the quality of generated text based solely on surface-level comparisons.\\n\\n3. **Compositional Diversity**: The inherent diversity in natural language expression means that valid and high-quality generated texts may not closely match reference outputs. This variability makes it difficult to apply conventional evaluation metrics effectively, as they may penalize outputs that are semantically correct but differ in phrasing or structure from expected responses.\\n\\n4. **Automated Evaluation Approaches**: The paper discusses the use of model-based evaluation methods that leverage smaller models to assess performance. While these approaches aim to capture semantic similarity more effectively, their ability to encompass the full range of quality in generated text remains a challenge. For instance, models like BERTSCORE and BARTSCORE attempt to evaluate texts from multiple perspectives, including informativeness and fluency, but they still face limitations in fully addressing the nuances of semantic understanding.\\n\\n5. **Uncertainty Estimation**: Some methods utilize small natural language inference (NLI) models to estimate the uncertainty of LLM responses. This adds another layer of complexity, as accurately gauging uncertainty in generated text is inherently difficult and requires careful consideration of the semantic context.\\n\\nIn summary, the evaluation of text generated by LLMs is complicated by the inadequacy of traditional metrics, the necessity for nuanced semantic understanding, and the challenges associated with automated evaluation methods. These factors collectively highlight the need for more sophisticated evaluation frameworks that can better capture the richness and complexity of generated text.\\n\\n**Source**: \"What is the Role of Small Models in the LLM Era: A Survey\" (Chang et al., 2024).<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                                     \\n\\nThe integration of small models in prompt-based learning significantly enhances the adaptability of large language models (LLMs) to new tasks. This is primarily achieved through several key mechanisms:\\n\\n1. **Optimization of Prompts**: Small models can autonomously generate or optimize prompts, which are crucial for few-shot or zero-shot learning scenarios. For instance, the Uprise method utilizes a lightweight retriever to automatically fetch prompts for zero-shot tasks, thereby reducing the manual effort involved in prompt engineering. This automation allows LLMs to quickly adapt to new tasks with minimal input.\\n\\n2. **Decomposition of Complex Problems**: Techniques such as DaSLaM leverage small models to break down complex tasks into simpler subproblems. This decomposition enables LLMs to tackle tasks that require fewer reasoning steps, which can lead to improved performance across various reasoning datasets. By simplifying the problem structure, small models help LLMs to better understand and execute new tasks.\\n\\n3. **Verification and Output Refinement**: Small models can also play a role in verifying or rewriting the outputs generated by LLMs. This verification process ensures that the responses are accurate and relevant, enhancing the overall performance of LLMs without necessitating fine-tuning. By refining outputs, small models contribute to the reliability and effectiveness of LLMs in new contexts.\\n\\n4. **Efficient Learning with Minimal Data**: The use of small models in prompt-based learning allows LLMs to generalize from a few examples embedded within prompts. This capability is particularly beneficial in situations where labeled data is scarce or difficult to obtain, enabling LLMs to adapt to new tasks with little to no additional training.\\n\\nIn summary, the integration of small models in prompt-based learning not only optimizes the prompt generation process but also aids in problem decomposition and output verification, all of which contribute to the enhanced adaptability of LLMs to new tasks.\\n\\nSource: \\'What is the Role of Small Models in the LLM Era: A Survey\\'<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                             \\n\\nResearchers can leverage insights on diminishing returns from increasing model sizes, as discussed in \"What is the Role of Small Models in the LLM Era: A Survey,\" to innovate in model training and architecture in several ways:\\n\\n1. **Focus on Efficiency**: The paper emphasizes that larger models yield only marginal performance improvements for certain tasks, such as text similarity and classification. This suggests that researchers should prioritize developing smaller, more efficient models that can deliver comparable performance without the high computational costs associated with larger models. This shift can lead to more sustainable AI practices and better resource management.\\n\\n2. **Task-Specific Model Development**: The findings indicate that not all tasks necessitate the complexity of large language models (LLMs). For simpler or more specialized tasks, smaller models can be tailored to perform effectively. Researchers can innovate by creating specialized models that are optimized for specific applications, enhancing performance while maintaining efficiency.\\n\\n3. **Utilization of Techniques like Knowledge Distillation**: The paper highlights that smaller models can achieve competitive results when enhanced by techniques such as knowledge distillation. This approach allows researchers to transfer knowledge from larger models to smaller ones, improving their performance without increasing their size. Exploring such techniques can lead to innovative training methodologies that maximize the utility of smaller models.\\n\\n4. **Emphasis on Interpretability**: Smaller models often provide better interpretability compared to their larger counterparts. In fields where understanding model decisions is critical, such as healthcare or finance, researchers can focus on developing smaller models that are easier to interpret, thereby increasing trust and usability in high-stakes applications.\\n\\n5. **Collaboration Between Model Types**: The paper discusses the potential for collaboration between small models (SMs) and large language models (LLMs). Researchers can explore hybrid approaches where SMs enhance LLMs by curating data or providing task-specific insights, while LLMs can support SMs by offering generalization capabilities. This collaborative framework can lead to innovative architectures that balance performance and efficiency.\\n\\n6. **Exploration of Alternative Architectures**: Given the diminishing returns from increasing model sizes, there is a clear need for research into alternative architectures that can enhance performance without simply scaling up. This could involve investigating novel neural network designs, leveraging ensemble methods, or integrating different types of models to create more effective systems.\\n\\nIn summary, the insights on diminishing returns encourage researchers to innovate by focusing on efficiency, task-specific development, knowledge distillation, interpretability, collaboration between model types, and exploring alternative architectures. This approach not only addresses the challenges posed by large models but also promotes a more sustainable and accessible AI landscape.\\n\\n**Source**: \"What is the Role of Small Models in the LLM Era: A Survey\"<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                                \\n\\nThe verification process by small models significantly impacts the accuracy of outputs generated by large language models (LLMs) as discussed in \"What is the Role of Small Models in the LLM Era: A Survey.\" Here are the key points regarding this impact:\\n\\n1. **Output Verification**: Small models can be employed to verify the outputs produced by LLMs. This verification process ensures that the responses generated are accurate and relevant, which is crucial for maintaining the quality of the outputs. By acting as a check on the larger models, small models help to filter out inaccuracies or irrelevant information.\\n\\n2. **Output Refinement**: Beyond verification, small models can also rewrite or refine the outputs of LLMs. This capability allows for the enhancement of the generated text, making it more coherent and aligned with the intended task. For instance, if an LLM generates a response that is technically correct but poorly articulated, a small model can improve the phrasing and clarity.\\n\\n3. **Addressing Hallucinations**: LLMs are known to sometimes produce \"hallucinated\" outputsâ€”responses that are plausible-sounding but factually incorrect. Small models can be trained to detect these hallucinations, thereby improving the reliability of the information provided by LLMs. This is particularly important in applications where accuracy is critical.\\n\\n4. **Cost-Effectiveness**: Utilizing small models for verification and refinement is a resource-efficient strategy. It allows organizations to leverage the strengths of LLMs while mitigating their weaknesses without incurring the high computational costs associated with further training or fine-tuning the larger models.\\n\\n5. **Improved Performance**: The collaboration between small models and LLMs can lead to better overall performance. By integrating small models into the workflow, the system can achieve a balance between the high accuracy of LLMs and the efficiency and interpretability of small models.\\n\\nIn summary, the verification process by small models enhances the accuracy and reliability of outputs generated by LLMs, addressing issues such as hallucinations and improving the overall quality of the generated text. This collaboration not only optimizes resource usage but also ensures that the outputs meet the necessary standards of accuracy and relevance.\\n\\nSource: \"What is the Role of Small Models in the LLM Era: A Survey\"<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                          \\n\\nThe paper \"What is the Role of Small Models in the LLM Era: A Survey\" addresses the \\'black box\\' nature of large language models (LLMs) by emphasizing the interpretability advantages of small models (SMs) and discussing various techniques to enhance understanding of both small and large models.\\n\\n1. **Interpretability of Small Models**: The paper highlights that small models are generally more interpretable than large models due to their simpler architectures and fewer parameters. This simplicity allows for easier analysis of how inputs are processed and outputs are generated, facilitating the identification of biases and errors.\\n\\n2. **Challenges with Large Models**: In contrast, LLMs, with their complex architectures and vast number of parameters, often function as \\'black boxes.\\' This complexity obscures the decision-making processes, making it difficult to understand how they arrive at specific outputs. The paper points out that this lack of transparency raises concerns regarding trust and accountability in the deployment of LLMs.\\n\\n3. **Techniques for Enhancing Interpretability**: The survey discusses various techniques that can be employed to improve the interpretability of both small and large models. Techniques such as attention visualization, feature importance analysis, and layer-wise relevance propagation are mentioned as methods that can help elucidate the decision-making processes of models, particularly large ones.\\n\\n4. **The Role of Small Models**: The paper also explores how small models can complement large models, especially in applications where interpretability is crucial. By utilizing small models for specific tasks, researchers can achieve a balance between performance and interpretability, ensuring that outputs are both accurate and understandable.\\n\\nIn summary, the paper suggests that while large models present significant interpretability challenges, small models offer a more transparent alternative. It advocates for the use of various interpretability techniques to enhance understanding of both model types, ultimately promoting responsible AI practices.\\n\\n**Source**: \"What is the Role of Small Models in the LLM Era: A Survey\" by Lihu Chen and GaÃ«l Varoquaux.<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                                  \\n\\nImplementing prompt-based learning in large language models (LLMs) presents several challenges, as discussed in \"What is the Role of Small Models in the LLM Era: A Survey.\" These challenges include:\\n\\n1. **Quality of Prompts**: The effectiveness of prompt-based learning heavily relies on the quality and structure of the prompts used. Poorly designed prompts can lead to suboptimal performance, as LLMs may misinterpret the task or fail to generate relevant outputs. To address this, small models can be employed to optimize prompts by reformulating questions and generating feedback, thereby enhancing the overall performance of LLMs without requiring parameter updates.\\n\\n2. **Complexity of Tasks**: LLMs may struggle with complex tasks that require multiple reasoning steps. This complexity can hinder their ability to provide accurate responses. Techniques like DaSLaM utilize small models to decompose complex problems into simpler subproblems, allowing LLMs to tackle these tasks more effectively and improve performance across various reasoning datasets.\\n\\n3. **Deficiency in Output Quality**: LLMs can generate outputs that are repeated, untruthful, or toxic. This poses a significant challenge in ensuring the reliability of the generated content. Small models can help repair these deficiencies through methods like contrastive decoding, which contrasts outputs from LLMs with those from smaller models to select the most appropriate tokens, thereby reducing issues like hallucinations and improving output quality.\\n\\n4. **Resource Intensity**: The computational costs associated with fine-tuning LLMs can be prohibitive. Instead of fine-tuning large models, small models can be fine-tuned and then used to contrast with LLM outputs, allowing for task adaptation without the extensive resource requirements typically associated with LLM fine-tuning.\\n\\n5. **Integration of Noisy Data**: When LLMs encounter noisy or out-of-vocabulary data, their performance can degrade. Small models can be trained to mimic the behavior of LLMs and handle unseen words, thus improving the robustness of the overall system.\\n\\nIn summary, while prompt-based learning in LLMs offers significant advantages, it also faces challenges related to prompt quality, task complexity, output reliability, resource intensity, and data integration. Employing small models can effectively address these challenges, enhancing the capabilities of LLMs in various applications.\\n\\nSource: \"What is the Role of Small Models in the LLM Era: A Survey\"<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                               \\n\\nThe limitations of using BLEU and ROUGE metrics for evaluating LLM-generated text, as discussed in \"What is the Role of Small Models in the LLM Era: A Survey,\" include the following key points:\\n\\n1. **Surface-Form Similarity**: Both BLEU and ROUGE primarily focus on surface-form similarity, which means they assess how closely the generated text matches reference outputs based on n-gram overlaps. This approach does not account for the deeper semantic meaning of the text, leading to potential misjudgments about the quality of the generated content.\\n\\n2. **Inability to Capture Nuanced Semantics**: The metrics often fail to capture the nuanced semantic meaning and compositional diversity inherent in open-ended text generation. This limitation is significant because the quality of generated text can be high even if it does not closely resemble the reference outputs, which traditional metrics would not recognize.\\n\\n3. **Challenges with Compositional Diversity**: Natural language allows for a vast range of expressions and ways to convey similar information. As a result, generated texts can be valid and high-quality despite not matching reference texts closely, making it difficult for BLEU and ROUGE to provide an accurate evaluation.\\n\\n4. **Need for Deeper Understanding**: Evaluating open-ended text generation requires a more profound understanding of semantics than what BLEU and ROUGE can provide. The complexity of meaning and context in generated text necessitates evaluation methods that go beyond simple surface-level comparisons.\\n\\n5. **Emergence of Alternative Approaches**: To address these limitations, the paper suggests the use of model-based evaluation approaches, such as BERTSCORE and BARTSCORE, which leverage smaller models to assess semantic similarity and other qualitative aspects of generated text. These methods aim to provide a more comprehensive evaluation by considering factors like informativeness, fluency, and factuality.\\n\\nOverall, the traditional metrics like BLEU and ROUGE are inadequate for evaluating the quality of LLM-generated text due to their focus on surface similarity and inability to capture the complexities of meaning and expression in natural language.\\n\\n**Source**: \"What is the Role of Small Models in the LLM Era: A Survey\" (Chang et al., 2024).<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                          \\n\\nEvaluating open-ended text generated by large language models (LLMs) is significantly impacted by the semantic complexity of the text, as discussed in \"What is the Role of Small Models in the LLM Era: A Survey.\" The paper outlines several key challenges associated with this complexity:\\n\\n1. **Limitations of Traditional Metrics**: Traditional evaluation metrics like BLEU and ROUGE primarily assess surface-form similarity, which means they often overlook the deeper semantic meanings and the compositional diversity inherent in open-ended text. This limitation can lead to an inaccurate assessment of the quality and relevance of the generated content.\\n\\n2. **Need for Nuanced Semantic Understanding**: Open-ended text generation can produce a wide range of meanings and contexts. Evaluating such diverse outputs necessitates a more sophisticated understanding of semantics than what traditional metrics provide. This complexity complicates the evaluation process, as it becomes challenging to determine the quality of the generated text effectively.\\n\\n3. **Compositional Diversity**: The variability in natural language expression adds another layer of difficulty. Texts can be valid and high-quality even if they do not closely match reference outputs, which makes conventional evaluation metrics inadequate for capturing the richness of generated content.\\n\\n4. **Automated Evaluation Approaches**: To mitigate these challenges, the paper discusses the use of model-based evaluation approaches that employ smaller models to automatically assess performance. However, the effectiveness of these approaches in fully capturing the quality spectrum of generated text remains a challenge.\\n\\n5. **Uncertainty Estimation**: Some methods utilize small natural language inference (NLI) models to estimate the uncertainty of LLM responses. This introduces additional complexity, as accurately gauging uncertainty in generated text is inherently difficult.\\n\\nIn summary, the semantic complexity of open-ended text generation complicates evaluation methods due to the inadequacy of traditional metrics, the necessity for nuanced semantic understanding, and the challenges associated with automated evaluation techniques.\\n\\n**Source**: \"What is the Role of Small Models in the LLM Era: A Survey\" (Chang et al., 2024).<|eot_id|><|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|>                                                \\n\\nData curation plays a significant role in mitigating the risks associated with harmful stereotypes in AI outputs, as discussed in \"What is the Role of Small Models in the LLM Era: A Survey.\" Here are the key points regarding this role:\\n\\n1. **Quality of Training Data**: The paper emphasizes that the quality of training data is crucial for the performance of small language models. High-quality data helps ensure that models learn relevant patterns and information, which is essential for effective instruction tuning. Poor quality data can lead to models that perpetuate harmful stereotypes or produce unreliable outputs.\\n\\n2. **Bias and Toxicity Reduction**: Data curation involves selecting datasets that are free from biases and toxic content. The paper highlights the importance of curating datasets that promote fairness and reduce the risk of perpetuating harmful stereotypes or misinformation. This is particularly important in the context of AI, where biased training data can lead to biased outputs.\\n\\n3. **Techniques for Data Selection**: The paper discusses various techniques for data curation, such as model-oriented data selection (MoDS) and the LESS framework, which utilize small models to evaluate'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
        "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "e-k53HMbuw6h",
        "outputId": "a85e1daf-7ab3-4a4a-a094-63b2cec7c38f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.564 GB.\n",
            "3.275 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXzbNdQk_wtZ"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "collapsed": true,
        "id": "hOPemaS5uw6h",
        "outputId": "c9bb70b0-08cc-4600-f40d-566b9eb6241f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 38 | Num Epochs = 10\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 1\n",
            "\\        /    Total batch size = 1 | Total steps = 380\n",
            " \"-____-\"     Number of trainable parameters = 194,510,848\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='380' max='380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [380/380 21:54, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.601800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.591100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.457800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.195900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.029300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.114600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.899500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.074100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.057100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.964400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.918700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.979800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.917900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.034500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.014800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.940400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.914900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.920200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.811600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.936900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.879200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.943500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.794400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.759700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.838000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.881200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.823000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.866600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.852200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.841700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.859600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.817000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.870000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.761400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.794400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.739800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.724700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.789700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.722500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.751600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.699500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.707900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.656000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.625600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.637500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.632500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.667200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.593900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.688600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.795800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.656700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.632600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.746000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.645500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.640500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.627700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.592100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.606300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.654800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.647700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.696700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.728100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.650500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.615800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.661600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.645700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.582100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.722900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.557000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.687700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.635100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.557900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.675700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.652700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.476100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.500400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.530400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.442600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.563400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.419700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.468400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.468800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.405800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.501600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.455700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.483800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.462800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.453200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.447200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.516000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.391300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.470300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.479900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.435000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.418900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.458000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.447500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.418700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.542000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.552000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.436700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.447300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.466500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.491200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.562500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.467300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.386500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.453300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.397300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.484600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.438300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.305700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.379600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.342200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.333600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.289300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.267100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.276500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.271300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.323600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.267000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.263300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.267100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.356100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.258600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.294600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.252500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.313000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.257200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.324600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.234900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.245200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.276500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.253700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.297000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.333500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.277500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.312700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.258100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.336000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.263600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.288300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.290100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.278800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.250700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.262300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.287300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.282300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.295200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.181200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.181400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.188000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.166000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.165200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.199300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.179400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.144200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.240500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.134700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>0.140500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.198300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.182900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.138100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.182200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.157300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>0.177800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.122500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.191400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.142500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>0.180000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.140300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.150200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.162500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.176600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.176800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.193900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.203200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>0.155800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.139800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.147400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.185800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.164500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.148700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.172700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.170500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>0.135000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.139000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>0.119900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.084600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>0.092700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>0.101000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.062400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.087600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>0.071600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>0.086300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>0.086100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.076700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>201</td>\n",
              "      <td>0.092200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>0.080200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>203</td>\n",
              "      <td>0.096600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>0.137900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>0.087000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>0.064300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>207</td>\n",
              "      <td>0.071300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>0.085300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>209</td>\n",
              "      <td>0.102500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.082600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>211</td>\n",
              "      <td>0.087800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>0.100800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>213</td>\n",
              "      <td>0.074300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>0.099800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>0.070400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>0.065800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>217</td>\n",
              "      <td>0.074600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>0.070200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>219</td>\n",
              "      <td>0.103500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.072200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>221</td>\n",
              "      <td>0.060600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>0.070600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>223</td>\n",
              "      <td>0.070700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.083300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.096600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>0.089400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>227</td>\n",
              "      <td>0.113600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.080600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>229</td>\n",
              "      <td>0.046500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.040400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>0.045000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>0.039100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>233</td>\n",
              "      <td>0.047400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>0.048800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>0.050600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>0.044800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>237</td>\n",
              "      <td>0.038000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>0.042000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>239</td>\n",
              "      <td>0.039600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.037400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>241</td>\n",
              "      <td>0.032900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>0.051600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>243</td>\n",
              "      <td>0.036900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>0.044600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>0.025900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>0.034500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>247</td>\n",
              "      <td>0.037100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>0.050900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>249</td>\n",
              "      <td>0.037600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.035500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>251</td>\n",
              "      <td>0.043200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.046800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>253</td>\n",
              "      <td>0.029300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>254</td>\n",
              "      <td>0.038100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>0.044000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>0.036900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>257</td>\n",
              "      <td>0.039300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>258</td>\n",
              "      <td>0.036600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>259</td>\n",
              "      <td>0.039700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.024500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>261</td>\n",
              "      <td>0.029400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>262</td>\n",
              "      <td>0.032200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>263</td>\n",
              "      <td>0.045100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>0.033200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>0.042400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>266</td>\n",
              "      <td>0.043200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>267</td>\n",
              "      <td>0.023700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>0.015300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>269</td>\n",
              "      <td>0.016700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.017000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>271</td>\n",
              "      <td>0.012700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>0.019100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>273</td>\n",
              "      <td>0.018500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>274</td>\n",
              "      <td>0.018900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.017200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>0.019000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>277</td>\n",
              "      <td>0.012000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>278</td>\n",
              "      <td>0.012800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>279</td>\n",
              "      <td>0.018200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.028300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>281</td>\n",
              "      <td>0.013400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>282</td>\n",
              "      <td>0.019600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>283</td>\n",
              "      <td>0.013000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>0.011000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.016400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>286</td>\n",
              "      <td>0.015900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>287</td>\n",
              "      <td>0.018000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>0.018000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>289</td>\n",
              "      <td>0.015800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.021100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>291</td>\n",
              "      <td>0.013700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>0.016900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>293</td>\n",
              "      <td>0.019500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>294</td>\n",
              "      <td>0.013300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>0.023200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>0.016800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>297</td>\n",
              "      <td>0.015200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>298</td>\n",
              "      <td>0.017100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>299</td>\n",
              "      <td>0.015000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.012300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>301</td>\n",
              "      <td>0.015300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>302</td>\n",
              "      <td>0.019900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>303</td>\n",
              "      <td>0.016600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>304</td>\n",
              "      <td>0.011200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>305</td>\n",
              "      <td>0.008500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>306</td>\n",
              "      <td>0.010100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>307</td>\n",
              "      <td>0.008100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.008600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>309</td>\n",
              "      <td>0.009100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.009000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>311</td>\n",
              "      <td>0.007000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>312</td>\n",
              "      <td>0.009100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>313</td>\n",
              "      <td>0.008200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>314</td>\n",
              "      <td>0.008200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>0.007700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>316</td>\n",
              "      <td>0.009100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>317</td>\n",
              "      <td>0.009700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>318</td>\n",
              "      <td>0.008800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>319</td>\n",
              "      <td>0.012100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.010700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>321</td>\n",
              "      <td>0.007300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>322</td>\n",
              "      <td>0.011000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>323</td>\n",
              "      <td>0.008900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>324</td>\n",
              "      <td>0.007600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.010800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>326</td>\n",
              "      <td>0.009500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>327</td>\n",
              "      <td>0.007800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>328</td>\n",
              "      <td>0.007700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>329</td>\n",
              "      <td>0.006800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.008300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>331</td>\n",
              "      <td>0.010800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>332</td>\n",
              "      <td>0.006500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>333</td>\n",
              "      <td>0.009600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>334</td>\n",
              "      <td>0.007800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>335</td>\n",
              "      <td>0.008200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.008500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>337</td>\n",
              "      <td>0.007200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>338</td>\n",
              "      <td>0.006900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>339</td>\n",
              "      <td>0.007300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.006100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>341</td>\n",
              "      <td>0.005600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>0.008700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>343</td>\n",
              "      <td>0.005500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>344</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>0.005200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>346</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>347</td>\n",
              "      <td>0.005200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>348</td>\n",
              "      <td>0.006300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>349</td>\n",
              "      <td>0.004500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.004800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>351</td>\n",
              "      <td>0.005300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>352</td>\n",
              "      <td>0.005100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>353</td>\n",
              "      <td>0.004600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>354</td>\n",
              "      <td>0.005400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>355</td>\n",
              "      <td>0.005600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>356</td>\n",
              "      <td>0.005600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>357</td>\n",
              "      <td>0.007900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>358</td>\n",
              "      <td>0.006100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>359</td>\n",
              "      <td>0.004800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.005700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>361</td>\n",
              "      <td>0.005300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>362</td>\n",
              "      <td>0.005600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>363</td>\n",
              "      <td>0.005600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.005100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>365</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>366</td>\n",
              "      <td>0.006500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>367</td>\n",
              "      <td>0.005100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>368</td>\n",
              "      <td>0.007700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>369</td>\n",
              "      <td>0.007700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.005700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>371</td>\n",
              "      <td>0.005800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>372</td>\n",
              "      <td>0.006800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>373</td>\n",
              "      <td>0.005600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>374</td>\n",
              "      <td>0.006400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.004700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>376</td>\n",
              "      <td>0.006300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>377</td>\n",
              "      <td>0.005600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>378</td>\n",
              "      <td>0.005500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>379</td>\n",
              "      <td>0.004800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.005600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "4ph91r3-uw6h",
        "outputId": "1fc87b3a-0f79-4734-b03c-0b4387bfeab1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1332.3681 seconds used for training.\n",
            "22.21 minutes used for training.\n",
            "Peak reserved memory = 6.566 GB.\n",
            "Peak reserved memory for training = 3.291 GB.\n",
            "Peak reserved memory % of max memory = 16.596 %.\n",
            "Peak reserved memory for training % of max memory = 8.318 %.\n"
          ]
        }
      ],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "w11qH3TPL1W2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "training_df = pd.DataFrame(trainer.state.log_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "x565dxlQL1W2",
        "outputId": "bdf0eeda-7b89-4c74-ad2e-548f248c05ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     loss  grad_norm  learning_rate     epoch  step  train_runtime  \\\n",
              "0  1.6018   8.078729        0.00004  0.026316     1            NaN   \n",
              "1  1.5911   8.664534        0.00008  0.052632     2            NaN   \n",
              "\n",
              "   train_samples_per_second  train_steps_per_second  total_flos  train_loss  \n",
              "0                       NaN                     NaN         NaN         NaN  \n",
              "1                       NaN                     NaN         NaN         NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e2d61a2d-1f73-4ba7-98ef-27d94d6b152a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>grad_norm</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>epoch</th>\n",
              "      <th>step</th>\n",
              "      <th>train_runtime</th>\n",
              "      <th>train_samples_per_second</th>\n",
              "      <th>train_steps_per_second</th>\n",
              "      <th>total_flos</th>\n",
              "      <th>train_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.6018</td>\n",
              "      <td>8.078729</td>\n",
              "      <td>0.00004</td>\n",
              "      <td>0.026316</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.5911</td>\n",
              "      <td>8.664534</td>\n",
              "      <td>0.00008</td>\n",
              "      <td>0.052632</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2d61a2d-1f73-4ba7-98ef-27d94d6b152a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e2d61a2d-1f73-4ba7-98ef-27d94d6b152a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e2d61a2d-1f73-4ba7-98ef-27d94d6b152a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e9ef04b2-32bd-44db-b05c-ba8d2dd719e1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e9ef04b2-32bd-44db-b05c-ba8d2dd719e1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e9ef04b2-32bd-44db-b05c-ba8d2dd719e1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "training_df",
              "summary": "{\n  \"name\": \"training_df\",\n  \"rows\": 381,\n  \"fields\": [\n    {\n      \"column\": \"loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3187408823027003,\n        \"min\": 0.0045,\n        \"max\": 1.6018,\n        \"num_unique_values\": 339,\n        \"samples\": [\n          0.0169,\n          0.3336,\n          0.3057\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"grad_norm\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6386063320003932,\n        \"min\": 0.026879124343395233,\n        \"max\": 8.664533615112305,\n        \"num_unique_values\": 380,\n        \"samples\": [\n          0.16913296282291412,\n          0.2644440233707428,\n          0.31072404980659485\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.7841968717014656e-05,\n        \"min\": 0.0,\n        \"max\": 0.0002,\n        \"num_unique_values\": 376,\n        \"samples\": [\n          4.5866666666666666e-05,\n          9.600000000000001e-06,\n          6.133333333333334e-05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"epoch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.8980247624910014,\n        \"min\": 0.02631578947368421,\n        \"max\": 10.0,\n        \"num_unique_values\": 380,\n        \"samples\": [\n          7.026315789473684,\n          6.894736842105263,\n          7.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"step\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 110,\n        \"min\": 1,\n        \"max\": 380,\n        \"num_unique_values\": 380,\n        \"samples\": [\n          267,\n          262,\n          266\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_runtime\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1332.3681,\n        \"max\": 1332.3681,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1332.3681\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_samples_per_second\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.285,\n        \"max\": 0.285,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.285\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_steps_per_second\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.285,\n        \"max\": 0.285,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.285\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_flos\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1.099236605952e+17,\n        \"max\": 1.099236605952e+17,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.099236605952e+17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.269844368503331,\n        \"max\": 0.269844368503331,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.269844368503331\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "training_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "nD77qGEFL1W2"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "if not 'google.colab' in sys.modules:\n",
        "  from helpers import create_training_plots\n",
        "  fig = create_training_plots(training_df)\n",
        "  fig.show()\n",
        "  training_df.to_csv(f\"training_logs/{OUTPUT_MODEL_NAME}.csv\", index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_17zqUsuw6h"
      },
      "source": [
        "## Run Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "7bVoC26xuw6h"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "def get_response(user_query):\n",
        "    messages = [\n",
        "    {\"role\": \"user\", \"content\": user_query},\n",
        "    ]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize = True,\n",
        "        add_generation_prompt = True, # Must add for generation\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
        "                            temperature = 1.5, min_p = 0.1)\n",
        "    return tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "HTYSzXcJuw6h",
        "outputId": "760ff9b8-f320-42a4-a85f-35af5176ed5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Summarize the concept of agentic flows and their application in generating higher quality data as mentioned in the paper 'AgentInstruct Toward Generative Teaching With Agentic Flows'.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "dataset_finetune['question'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYV5H1LEuw6h"
      },
      "source": [
        "Need to investigate how changing the question affects responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "LNxBufzGuw6h",
        "outputId": "2782bc71-5e2a-44b9-bad8-4af7523d3266",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Agentic flows refer to a series of structured techniques or workflows used in generating or augmenting data, particularly in the context of natural language processing (NLP) and related fields. The concept is centered around the idea of systematically creating higher quality data through various transformative processes. These processes can involve complex reasoning, argumentation,\n"
          ]
        }
      ],
      "source": [
        "resp = get_response(dataset_finetune['question'][0])\n",
        "print(resp[0].split(\"<|start_header_id|>assistant<|end_header_id|>\")[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNMdQqDSuw6h"
      },
      "source": [
        "## Save to HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "katiUBV9L1W3",
        "outputId": "809dcdef-ad61-41c6-fa31-95adb1bc5ad9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model dtype: torch.bfloat16\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "DKIN4QnLuw6h",
        "outputId": "a96a2c20-9ef8-403a-a19f-57b2b440336c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d03b5a957c80488a8924dfb948293642",
            "d9031d5b8ed446c4a540e416b83715cc",
            "f9b1905d9fe142aabf1f81b89d556cad",
            "ce364b63559641f3a7d64040bbd77f9a",
            "f519794eab0f415cbd3f061563281556",
            "c8a5b1d7449b456fadf0a0fdd986d683",
            "3875e530cb2d44878823fa4280d230c9",
            "f53e259c391041fc9c7b18ce1546712c",
            "b539a712ed6f4a67979b34928219276d",
            "48b9c2623c024ed796bc8c5ac78ecf9a",
            "b06b3a93081a400983ceb6e05a45b17d",
            "8b4fe72dc6624d4a92f26b28abd04dba",
            "99950a5433a24c03b87491625c928cfa",
            "c2629be850744bb39379ee87b04dad8a",
            "2ef191e19280456c8539feebfd85af6e",
            "11d679b008314614ab764f2c021d23b3",
            "509283d6ed564f9b833350248fee9462",
            "5f5b57a5f80649dab847a1b16c323e25",
            "9406bb1e2c9a4008a3287bf989cc0771",
            "301897bf659d44538b35a92afd6cea30",
            "f9fc4343551f462db8ec749c330991da",
            "97cd56021b9144a2a5599c2b145cb52f",
            "df309cb0ce714df19cacc90a0f743e79",
            "c32a5ddfb7fc4b098a10113b0eaa84de",
            "68b4e98c54b146e2b8d5d37d312c2696",
            "d2ffacd189714c74b2c5cc73db671fa9",
            "77ab488a3b474225a5577ddb4fd4eaeb",
            "f2fa2093e8e2410c9206ca98571c436b",
            "0685d13dc6f54d6a9e207ed66d9096c0",
            "842a82f18b584cf3ba8fe378107ab120",
            "6e132c348c3841eb82f0b9617e278eb5",
            "3f4231c453f04aefbdf3641f65bb1d23",
            "2734f4d645b64619ae21cb9fd1dd2148",
            "b6267b80d65c4b50b0a1f5847222ab82",
            "4dbca526f86640eaa2d76c050b2bb70a",
            "6e69a12b697846d59398e246a4f73d8e",
            "2915fba8acc94bda8fb139c534dee09e",
            "8e96ed23e9b844da8f3152bdf5988fbc",
            "e2e1d9c982e14b40a610e05d3096f567",
            "2c6d509996fa468789309064253b887b",
            "fff7d04c3133480c882a7d04891f4299",
            "f057b4bf4d29479fb93f484cafad3195",
            "152ce6a0337b436aa9ea3600363fb945",
            "675a62c8291349bab2284721b1795e89"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 2.2G\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 57.71 out of 83.48 RAM for saving.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:00<00:00, 40.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Converting llama model. Can use fast conversion = False.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits will take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m', 'q8_0', 'q5_k_m', 'bf16'] will take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
            "Unsloth: [1] Converting model at CPSC532/eval_no_sources_finetune into bf16 GGUF format.\n",
            "The output location will be /content/CPSC532/eval_no_sources_finetune/unsloth.BF16.gguf\n",
            "This will take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: eval_no_sources_finetune\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {3072, 128256}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> BF16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> BF16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 131072\n",
            "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
            "INFO:hf-to-gguf:gguf: head count = 24\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 32\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:gguf.vocab:Adding 280147 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 128000\n",
            "INFO:gguf.vocab:Setting special token type eos to 128009\n",
            "INFO:gguf.vocab:Setting special token type pad to 128004\n",
            "INFO:gguf.vocab:Setting chat_template to {{- bos_token }}\n",
            "{%- if custom_tools is defined %}\n",
            "    {%- set tools = custom_tools %}\n",
            "{%- endif %}\n",
            "{%- if not tools_in_user_message is defined %}\n",
            "    {%- set tools_in_user_message = true %}\n",
            "{%- endif %}\n",
            "{%- if not date_string is defined %}\n",
            "    {%- set date_string = \"26 July 2024\" %}\n",
            "{%- endif %}\n",
            "{%- if not tools is defined %}\n",
            "    {%- set tools = none %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
            "{%- if messages[0]['role'] == 'system' %}\n",
            "    {%- set system_message = messages[0]['content'] %}\n",
            "    {%- set messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set system_message = \"\" %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- System message + builtin tools #}\n",
            "{{- \"<|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "\" }}\n",
            "{%- if builtin_tools is defined or tools is not none %}\n",
            "    {{- \"Environment: ipython\n",
            "\" }}\n",
            "{%- endif %}\n",
            "{%- if builtin_tools is defined %}\n",
            "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\n",
            "\n",
            "\"}}\n",
            "{%- endif %}\n",
            "{{- \"Cutting Knowledge Date: December 2023\n",
            "\" }}\n",
            "{{- \"Today Date: \" + date_string + \"\n",
            "\n",
            "\" }}\n",
            "{%- if tools is not none and not tools_in_user_message %}\n",
            "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\n",
            "\n",
            "\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\n",
            "\n",
            "\" }}\n",
            "    {%- endfor %}\n",
            "{%- endif %}\n",
            "{{- system_message }}\n",
            "{{- \"<|eot_id|>\" }}\n",
            "\n",
            "{#- Custom tools are passed in a user message with some extra guidance #}\n",
            "{%- if tools_in_user_message and not tools is none %}\n",
            "    {#- Extract the first user message so we can plug it in here #}\n",
            "    {%- if messages | length != 0 %}\n",
            "        {%- set first_user_message = messages[0]['content'] %}\n",
            "        {%- set messages = messages[1:] %}\n",
            "    {%- else %}\n",
            "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
            "{%- endif %}\n",
            "    {{- '<|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "' -}}\n",
            "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
            "    {{- \"with its proper arguments that best answers the given prompt.\n",
            "\n",
            "\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\n",
            "\n",
            "\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\n",
            "\n",
            "\" }}\n",
            "    {%- endfor %}\n",
            "    {{- first_user_message + \"<|eot_id|>\"}}\n",
            "{%- endif %}\n",
            "\n",
            "{%- for message in messages %}\n",
            "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
            "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
            "\n",
            "'+ message['content'] + '<|eot_id|>' }}\n",
            "    {%- elif 'tool_calls' in message %}\n",
            "        {%- if not message.tool_calls|length == 1 %}\n",
            "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
            "        {%- endif %}\n",
            "        {%- set tool_call = message.tool_calls[0].function %}\n",
            "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
            "            {{- '<|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "' -}}\n",
            "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
            "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
            "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
            "                {%- if not loop.last %}\n",
            "                    {{- \", \" }}\n",
            "                {%- endif %}\n",
            "                {%- endfor %}\n",
            "            {{- \")\" }}\n",
            "        {%- else  %}\n",
            "            {{- '<|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "' -}}\n",
            "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
            "            {{- '\"parameters\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- \"}\" }}\n",
            "        {%- endif %}\n",
            "        {%- if builtin_tools is defined %}\n",
            "            {#- This means we're in ipython mode #}\n",
            "            {{- \"<|eom_id|>\" }}\n",
            "        {%- else %}\n",
            "            {{- \"<|eot_id|>\" }}\n",
            "        {%- endif %}\n",
            "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
            "        {{- \"<|start_header_id|>ipython<|end_header_id|>\n",
            "\n",
            "\" }}\n",
            "        {%- if message.content is mapping or message.content is iterable %}\n",
            "            {{- message.content | tojson }}\n",
            "        {%- else %}\n",
            "            {{- message.content }}\n",
            "        {%- endif %}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "' }}\n",
            "{%- endif %}\n",
            "\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/CPSC532/eval_no_sources_finetune/unsloth.BF16.gguf: n_tensors = 255, total_size = 6.4G\n",
            "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.43G/6.43G [00:29<00:00, 215Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/CPSC532/eval_no_sources_finetune/unsloth.BF16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/CPSC532/eval_no_sources_finetune/unsloth.BF16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\n",
            "main: build = 4168 (80acb7b4)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/CPSC532/eval_no_sources_finetune/unsloth.BF16.gguf' to '/content/CPSC532/eval_no_sources_finetune/unsloth.Q4_K_M.gguf' as Q4_K_M using 24 threads\n",
            "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /content/CPSC532/eval_no_sources_finetune/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3b Instruct Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = instruct-bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ä  Ä \", \"Ä  Ä Ä Ä \", \"Ä Ä  Ä Ä \", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
            "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type bf16:  197 tensors\n",
            "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =   bf16, converting to q6_K .. size =   751.50 MiB ->   308.23 MiB\n",
            "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "llama_model_quantize_internal: model size  =  6128.17 MB\n",
            "llama_model_quantize_internal: quant size  =  1918.35 MB\n",
            "\n",
            "main: quantize time = 55975.31 ms\n",
            "main:    total time = 55975.31 ms\n",
            "Unsloth: Conversion completed! Output location: /content/CPSC532/eval_no_sources_finetune/unsloth.Q4_K_M.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q8_0. This will take 20 minutes...\n",
            "main: build = 4168 (80acb7b4)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/CPSC532/eval_no_sources_finetune/unsloth.BF16.gguf' to '/content/CPSC532/eval_no_sources_finetune/unsloth.Q8_0.gguf' as Q8_0 using 24 threads\n",
            "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /content/CPSC532/eval_no_sources_finetune/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3b Instruct Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = instruct-bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ä  Ä \", \"Ä  Ä Ä Ä \", \"Ä Ä  Ä Ä \", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
            "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type bf16:  197 tensors\n",
            "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =   bf16, converting to q8_0 .. size =   751.50 MiB ->   399.23 MiB\n",
            "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "llama_model_quantize_internal: model size  =  6128.17 MB\n",
            "llama_model_quantize_internal: quant size  =  3255.90 MB\n",
            "\n",
            "main: quantize time =  8569.54 ms\n",
            "main:    total time =  8569.54 ms\n",
            "Unsloth: Conversion completed! Output location: /content/CPSC532/eval_no_sources_finetune/unsloth.Q8_0.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q5_k_m. This will take 20 minutes...\n",
            "main: build = 4168 (80acb7b4)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/CPSC532/eval_no_sources_finetune/unsloth.BF16.gguf' to '/content/CPSC532/eval_no_sources_finetune/unsloth.Q5_K_M.gguf' as Q5_K_M using 24 threads\n",
            "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /content/CPSC532/eval_no_sources_finetune/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3b Instruct Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = instruct-bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ä  Ä \", \"Ä  Ä Ä Ä \", \"Ä Ä  Ä Ä \", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
            "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type bf16:  197 tensors\n",
            "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =   bf16, converting to q6_K .. size =   751.50 MiB ->   308.23 MiB\n",
            "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =   bf16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =   bf16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =   bf16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "llama_model_quantize_internal: model size  =  6128.17 MB\n",
            "llama_model_quantize_internal: quant size  =  2207.10 MB\n",
            "\n",
            "main: quantize time = 45997.59 ms\n",
            "main:    total time = 45997.59 ms\n",
            "Unsloth: Conversion completed! Output location: /content/CPSC532/eval_no_sources_finetune/unsloth.Q5_K_M.gguf\n",
            "Unsloth: Saved Ollama Modelfile to CPSC532/eval_no_sources_finetune/Modelfile\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "unsloth.BF16.gguf:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d03b5a957c80488a8924dfb948293642"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved GGUF to https://huggingface.co/CPSC532/eval_no_sources_finetune\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "unsloth.Q4_K_M.gguf:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b4fe72dc6624d4a92f26b28abd04dba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved GGUF to https://huggingface.co/CPSC532/eval_no_sources_finetune\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "unsloth.Q8_0.gguf:   0%|          | 0.00/3.42G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df309cb0ce714df19cacc90a0f743e79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved GGUF to https://huggingface.co/CPSC532/eval_no_sources_finetune\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "unsloth.Q5_K_M.gguf:   0%|          | 0.00/2.32G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6267b80d65c4b50b0a1f5847222ab82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved GGUF to https://huggingface.co/CPSC532/eval_no_sources_finetune\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Ollama Modelfile to https://huggingface.co/CPSC532/eval_no_sources_finetune\n"
          ]
        }
      ],
      "source": [
        "model.push_to_hub_gguf(\n",
        "        f\"CPSC532/{OUTPUT_MODEL_NAME}\",\n",
        "        tokenizer,\n",
        "        quantization_method = finetune_config[\"quantization_methods\"],\n",
        "        token = HF_TOKEN\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "e9MpX2xJL1W3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "unsloth_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e9a46fdf97c441979dcf2e00f6824600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6ceafd970774d78be3a2b63fdd95c7b",
              "IPY_MODEL_eb6987c57de64d069613d40ce52c1816",
              "IPY_MODEL_ccf0607ae2c348ed8aad469b8027c92e"
            ],
            "layout": "IPY_MODEL_90b5deeb15bc41bdaf15b67a3470e912"
          }
        },
        "c6ceafd970774d78be3a2b63fdd95c7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad3852c150ba4d46a12b0ebbfe16c66f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a3b483f123ac42d59c632e41b9ad9c5d",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "eb6987c57de64d069613d40ce52c1816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_baaaa4709e0e4995814aba3308e6059b",
            "max": 2242762780,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f3fdb9c3d4c4682a83d4e45ec3ee70f",
            "value": 2242762780
          }
        },
        "ccf0607ae2c348ed8aad469b8027c92e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8a2eb9064534c7ebfd24dbfb7d3a990",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8077ab26468f4dc78d643c5f25c2579a",
            "value": "â€‡2.24G/2.24Gâ€‡[00:53&lt;00:00,â€‡42.6MB/s]"
          }
        },
        "90b5deeb15bc41bdaf15b67a3470e912": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad3852c150ba4d46a12b0ebbfe16c66f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3b483f123ac42d59c632e41b9ad9c5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "baaaa4709e0e4995814aba3308e6059b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f3fdb9c3d4c4682a83d4e45ec3ee70f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8a2eb9064534c7ebfd24dbfb7d3a990": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8077ab26468f4dc78d643c5f25c2579a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8bc02adbf30844b9ad6ab85c5d62a23b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b673803a166459994ea1b50dad22fc2",
              "IPY_MODEL_ae9b4b75c80b4cedb2c3ad0a78c4243a",
              "IPY_MODEL_e14da6f6f598400e8d460c17c8d6866a"
            ],
            "layout": "IPY_MODEL_8f85779bcbee4413949f7fb93d170eec"
          }
        },
        "0b673803a166459994ea1b50dad22fc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7579e7aba99c448e96a592f417664ef0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7cae9cf2f8fb4739b2dd9cf7afcc9043",
            "value": "generation_config.json:â€‡100%"
          }
        },
        "ae9b4b75c80b4cedb2c3ad0a78c4243a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ec70383f0ff40c2934b88866c814d00",
            "max": 184,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f32ecba698d4c648a23c651d307f348",
            "value": 184
          }
        },
        "e14da6f6f598400e8d460c17c8d6866a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e198c9b107a429eb341b78a82a9b2a0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f477deecf5684ab5b6acfeeac55c769a",
            "value": "â€‡184/184â€‡[00:00&lt;00:00,â€‡16.6kB/s]"
          }
        },
        "8f85779bcbee4413949f7fb93d170eec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7579e7aba99c448e96a592f417664ef0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cae9cf2f8fb4739b2dd9cf7afcc9043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ec70383f0ff40c2934b88866c814d00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f32ecba698d4c648a23c651d307f348": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e198c9b107a429eb341b78a82a9b2a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f477deecf5684ab5b6acfeeac55c769a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8164a37749e84cda9c4ea4ccfe397cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74dba62e01f743998d712feb1c85cb30",
              "IPY_MODEL_79965e275de84efdb5eaed196cdd85d3",
              "IPY_MODEL_f9afa4bb9a504cf89a3070da1cbdfe42"
            ],
            "layout": "IPY_MODEL_deeadc1851e94c0da8424df0924cba21"
          }
        },
        "74dba62e01f743998d712feb1c85cb30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b90b10e650bf4df39a64b6cc2bfa6405",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_36dc98de0d2e4e0e8204b0cda22a46e2",
            "value": "tokenizer_config.json:â€‡100%"
          }
        },
        "79965e275de84efdb5eaed196cdd85d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_118adf57673444c6a805007c498d7b4d",
            "max": 54598,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af35caf25e16410f9f726a6b755bcf8d",
            "value": 54598
          }
        },
        "f9afa4bb9a504cf89a3070da1cbdfe42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f792370b78146a3a891f8fbfee0859d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9146e776d68b49c9a0f3479fe63f987c",
            "value": "â€‡54.6k/54.6kâ€‡[00:00&lt;00:00,â€‡4.24MB/s]"
          }
        },
        "deeadc1851e94c0da8424df0924cba21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b90b10e650bf4df39a64b6cc2bfa6405": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36dc98de0d2e4e0e8204b0cda22a46e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "118adf57673444c6a805007c498d7b4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af35caf25e16410f9f726a6b755bcf8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f792370b78146a3a891f8fbfee0859d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9146e776d68b49c9a0f3479fe63f987c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36fe8a18e39d443fb1aebbb597b96360": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_778b14dd7f734cb7b234311e0f3ac723",
              "IPY_MODEL_b91f6b3be0fa4e8f8f758dcd771d9a0d",
              "IPY_MODEL_83db2ff8e4bd4c28b5c3f11aea5d84e8"
            ],
            "layout": "IPY_MODEL_9ecc8a3e039e41f08982e8fc5346290e"
          }
        },
        "778b14dd7f734cb7b234311e0f3ac723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_954a2d0bbd06428abd2f332fd1e2b8be",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bad9ca03cf1d46fd8ca50da074625bc2",
            "value": "tokenizer.json:â€‡100%"
          }
        },
        "b91f6b3be0fa4e8f8f758dcd771d9a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e943cedfed045719f7886fc7b00b1d5",
            "max": 9085657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6bca23b51576402da16d64ec97c3496d",
            "value": 9085657
          }
        },
        "83db2ff8e4bd4c28b5c3f11aea5d84e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_deaa3a796f2144ba8dca806925942f54",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a56e234f043c44feb9ef4e92291af29a",
            "value": "â€‡9.09M/9.09Mâ€‡[00:00&lt;00:00,â€‡42.4MB/s]"
          }
        },
        "9ecc8a3e039e41f08982e8fc5346290e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "954a2d0bbd06428abd2f332fd1e2b8be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bad9ca03cf1d46fd8ca50da074625bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e943cedfed045719f7886fc7b00b1d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bca23b51576402da16d64ec97c3496d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "deaa3a796f2144ba8dca806925942f54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a56e234f043c44feb9ef4e92291af29a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "546cff63d364467d8b5827543c7677f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc784f8401a84b1aba3467afb69c37f8",
              "IPY_MODEL_7d2acda1543b407dbd16cff4dc99564e",
              "IPY_MODEL_4b983dbce2d44985994af742b95b7aef"
            ],
            "layout": "IPY_MODEL_4b3beb1e8f6a435cba06c92bfecc4870"
          }
        },
        "dc784f8401a84b1aba3467afb69c37f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c1755477b364719b3e72cb2babd3988",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_67c6b0b81b6648869d81ced62d0bd111",
            "value": "special_tokens_map.json:â€‡100%"
          }
        },
        "7d2acda1543b407dbd16cff4dc99564e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03ced17f64bb4b86b932227413d8bcd0",
            "max": 454,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b418fb0b1a094c4190825a56b3cd6a0b",
            "value": 454
          }
        },
        "4b983dbce2d44985994af742b95b7aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e78c61b4cfef460a8777486e9e1d0255",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_128976ec3e6940ada32c848b892714b8",
            "value": "â€‡454/454â€‡[00:00&lt;00:00,â€‡36.7kB/s]"
          }
        },
        "4b3beb1e8f6a435cba06c92bfecc4870": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c1755477b364719b3e72cb2babd3988": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67c6b0b81b6648869d81ced62d0bd111": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03ced17f64bb4b86b932227413d8bcd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b418fb0b1a094c4190825a56b3cd6a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e78c61b4cfef460a8777486e9e1d0255": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "128976ec3e6940ada32c848b892714b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "439aa16185c445e693ebfd9df5a7348e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47bdcbe51bd44169b29803237ee6fd3a",
              "IPY_MODEL_626b2d9ab4134d469f93150328461801",
              "IPY_MODEL_46cdf69ee8054827a73432ba836eb113"
            ],
            "layout": "IPY_MODEL_9a4088900ee34774917802eafacefe52"
          }
        },
        "47bdcbe51bd44169b29803237ee6fd3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f00e138570c9425cb51c08e29e530df9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a32c72ff644147ef9018a464c2d90aa7",
            "value": "Map:â€‡100%"
          }
        },
        "626b2d9ab4134d469f93150328461801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39641dfd24e240afb6b83e1f7e864078",
            "max": 1259,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3d064a1665342bb82f9e5826ac7c090",
            "value": 1259
          }
        },
        "46cdf69ee8054827a73432ba836eb113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96521fb94d2e48fe9f22385d9cb2f3af",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f579599bc03f4654a0e3c8cc5f9b9f79",
            "value": "â€‡1259/1259â€‡[00:00&lt;00:00,â€‡6244.67â€‡examples/s]"
          }
        },
        "9a4088900ee34774917802eafacefe52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f00e138570c9425cb51c08e29e530df9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a32c72ff644147ef9018a464c2d90aa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39641dfd24e240afb6b83e1f7e864078": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3d064a1665342bb82f9e5826ac7c090": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96521fb94d2e48fe9f22385d9cb2f3af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f579599bc03f4654a0e3c8cc5f9b9f79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "929f70350bfe4a33a74886f42b790553": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eabc8b40075242aa9020d66b80eb961e",
              "IPY_MODEL_17c954735f8643c8a69f8aad9f88221b",
              "IPY_MODEL_4865409cfd7e4b4697892af5ae26bdcc"
            ],
            "layout": "IPY_MODEL_39b08054fda74628b5a803139f784afc"
          }
        },
        "eabc8b40075242aa9020d66b80eb961e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1146fa8cf68456295bd29aa97fd5f2a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_77f22353b65047bf815b51ed9ca0c008",
            "value": "Map:â€‡100%"
          }
        },
        "17c954735f8643c8a69f8aad9f88221b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54434d4da82644b6ac9f02a13ccae91d",
            "max": 1259,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3154704dd55b4f14b68eda14e845778a",
            "value": 1259
          }
        },
        "4865409cfd7e4b4697892af5ae26bdcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90093e86d91b4a49a0553755be3c3b37",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_32a817bbfd854b5bae32bb31278c63c5",
            "value": "â€‡1259/1259â€‡[00:00&lt;00:00,â€‡6691.70â€‡examples/s]"
          }
        },
        "39b08054fda74628b5a803139f784afc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1146fa8cf68456295bd29aa97fd5f2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77f22353b65047bf815b51ed9ca0c008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54434d4da82644b6ac9f02a13ccae91d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3154704dd55b4f14b68eda14e845778a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90093e86d91b4a49a0553755be3c3b37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32a817bbfd854b5bae32bb31278c63c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1618e769a6a74adf974941dc9e26484d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67923dd2e6ef4bb0a37c119a3f09ecc5",
              "IPY_MODEL_ce346334ba9342ab9a2f148683ecf729",
              "IPY_MODEL_296b2b148bf74b15bdec21add629ac74"
            ],
            "layout": "IPY_MODEL_5e20bfb51684488189f3c7dcdb77bbfa"
          }
        },
        "67923dd2e6ef4bb0a37c119a3f09ecc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f76ea79a0e234b41bc27b18c91545927",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5af6a967bb2343e7ae204db4d82ab2e4",
            "value": "Generatingâ€‡trainâ€‡split:â€‡"
          }
        },
        "ce346334ba9342ab9a2f148683ecf729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9e76fb52e094264a4c97ea71533dee3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a92527d38a749c4a03e5296b49c73d9",
            "value": 1
          }
        },
        "296b2b148bf74b15bdec21add629ac74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e094d98d59c54dd0af90854281837f2d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e851d802f4934835aa44011ca3d96bd3",
            "value": "â€‡38/0â€‡[00:00&lt;00:00,â€‡â€‡1.78â€‡examples/s]"
          }
        },
        "5e20bfb51684488189f3c7dcdb77bbfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f76ea79a0e234b41bc27b18c91545927": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5af6a967bb2343e7ae204db4d82ab2e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9e76fb52e094264a4c97ea71533dee3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1a92527d38a749c4a03e5296b49c73d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e094d98d59c54dd0af90854281837f2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e851d802f4934835aa44011ca3d96bd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5424158385084a368de8f3c68fe67b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e36410af31964e268c17b597d16172b7",
              "IPY_MODEL_6477412953374157a8e5944d3863b5d2",
              "IPY_MODEL_6b87f6cd8457421f83ea2b38f5da80c6"
            ],
            "layout": "IPY_MODEL_d164403dc6c643cdb6a5c032c062494b"
          }
        },
        "e36410af31964e268c17b597d16172b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d26c4c5fc62649d59e9b46c1ed383060",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4be973a3aa784146a5a70893b1b5ef4d",
            "value": "Map:â€‡100%"
          }
        },
        "6477412953374157a8e5944d3863b5d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1423f13b4034629a0d2897a1fe08d49",
            "max": 38,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f02cfa4b9d5d4536af1fae502e40af96",
            "value": 38
          }
        },
        "6b87f6cd8457421f83ea2b38f5da80c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4edce530ad004200b9dc077bc06fffb8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e097aa8f91b843859b16dde90bd781ea",
            "value": "â€‡38/38â€‡[00:00&lt;00:00,â€‡66.66â€‡examples/s]"
          }
        },
        "d164403dc6c643cdb6a5c032c062494b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d26c4c5fc62649d59e9b46c1ed383060": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4be973a3aa784146a5a70893b1b5ef4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1423f13b4034629a0d2897a1fe08d49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f02cfa4b9d5d4536af1fae502e40af96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4edce530ad004200b9dc077bc06fffb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e097aa8f91b843859b16dde90bd781ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d03b5a957c80488a8924dfb948293642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9031d5b8ed446c4a540e416b83715cc",
              "IPY_MODEL_f9b1905d9fe142aabf1f81b89d556cad",
              "IPY_MODEL_ce364b63559641f3a7d64040bbd77f9a"
            ],
            "layout": "IPY_MODEL_f519794eab0f415cbd3f061563281556"
          }
        },
        "d9031d5b8ed446c4a540e416b83715cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8a5b1d7449b456fadf0a0fdd986d683",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3875e530cb2d44878823fa4280d230c9",
            "value": "unsloth.BF16.gguf:â€‡100%"
          }
        },
        "f9b1905d9fe142aabf1f81b89d556cad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f53e259c391041fc9c7b18ce1546712c",
            "max": 6433688384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b539a712ed6f4a67979b34928219276d",
            "value": 6433688384
          }
        },
        "ce364b63559641f3a7d64040bbd77f9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48b9c2623c024ed796bc8c5ac78ecf9a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b06b3a93081a400983ceb6e05a45b17d",
            "value": "â€‡6.43G/6.43Gâ€‡[02:26&lt;00:00,â€‡54.7MB/s]"
          }
        },
        "f519794eab0f415cbd3f061563281556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8a5b1d7449b456fadf0a0fdd986d683": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3875e530cb2d44878823fa4280d230c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f53e259c391041fc9c7b18ce1546712c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b539a712ed6f4a67979b34928219276d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48b9c2623c024ed796bc8c5ac78ecf9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b06b3a93081a400983ceb6e05a45b17d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b4fe72dc6624d4a92f26b28abd04dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99950a5433a24c03b87491625c928cfa",
              "IPY_MODEL_c2629be850744bb39379ee87b04dad8a",
              "IPY_MODEL_2ef191e19280456c8539feebfd85af6e"
            ],
            "layout": "IPY_MODEL_11d679b008314614ab764f2c021d23b3"
          }
        },
        "99950a5433a24c03b87491625c928cfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_509283d6ed564f9b833350248fee9462",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5f5b57a5f80649dab847a1b16c323e25",
            "value": "unsloth.Q4_K_M.gguf:â€‡100%"
          }
        },
        "c2629be850744bb39379ee87b04dad8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9406bb1e2c9a4008a3287bf989cc0771",
            "max": 2019377984,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_301897bf659d44538b35a92afd6cea30",
            "value": 2019377984
          }
        },
        "2ef191e19280456c8539feebfd85af6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9fc4343551f462db8ec749c330991da",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_97cd56021b9144a2a5599c2b145cb52f",
            "value": "â€‡2.02G/2.02Gâ€‡[00:42&lt;00:00,â€‡49.3MB/s]"
          }
        },
        "11d679b008314614ab764f2c021d23b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "509283d6ed564f9b833350248fee9462": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f5b57a5f80649dab847a1b16c323e25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9406bb1e2c9a4008a3287bf989cc0771": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "301897bf659d44538b35a92afd6cea30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9fc4343551f462db8ec749c330991da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97cd56021b9144a2a5599c2b145cb52f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df309cb0ce714df19cacc90a0f743e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c32a5ddfb7fc4b098a10113b0eaa84de",
              "IPY_MODEL_68b4e98c54b146e2b8d5d37d312c2696",
              "IPY_MODEL_d2ffacd189714c74b2c5cc73db671fa9"
            ],
            "layout": "IPY_MODEL_77ab488a3b474225a5577ddb4fd4eaeb"
          }
        },
        "c32a5ddfb7fc4b098a10113b0eaa84de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2fa2093e8e2410c9206ca98571c436b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0685d13dc6f54d6a9e207ed66d9096c0",
            "value": "unsloth.Q8_0.gguf:â€‡100%"
          }
        },
        "68b4e98c54b146e2b8d5d37d312c2696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_842a82f18b584cf3ba8fe378107ab120",
            "max": 3421899584,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e132c348c3841eb82f0b9617e278eb5",
            "value": 3421899584
          }
        },
        "d2ffacd189714c74b2c5cc73db671fa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f4231c453f04aefbdf3641f65bb1d23",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2734f4d645b64619ae21cb9fd1dd2148",
            "value": "â€‡3.42G/3.42Gâ€‡[01:12&lt;00:00,â€‡51.4MB/s]"
          }
        },
        "77ab488a3b474225a5577ddb4fd4eaeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2fa2093e8e2410c9206ca98571c436b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0685d13dc6f54d6a9e207ed66d9096c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "842a82f18b584cf3ba8fe378107ab120": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e132c348c3841eb82f0b9617e278eb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f4231c453f04aefbdf3641f65bb1d23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2734f4d645b64619ae21cb9fd1dd2148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6267b80d65c4b50b0a1f5847222ab82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4dbca526f86640eaa2d76c050b2bb70a",
              "IPY_MODEL_6e69a12b697846d59398e246a4f73d8e",
              "IPY_MODEL_2915fba8acc94bda8fb139c534dee09e"
            ],
            "layout": "IPY_MODEL_8e96ed23e9b844da8f3152bdf5988fbc"
          }
        },
        "4dbca526f86640eaa2d76c050b2bb70a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2e1d9c982e14b40a610e05d3096f567",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2c6d509996fa468789309064253b887b",
            "value": "unsloth.Q5_K_M.gguf:â€‡100%"
          }
        },
        "6e69a12b697846d59398e246a4f73d8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fff7d04c3133480c882a7d04891f4299",
            "max": 2322154304,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f057b4bf4d29479fb93f484cafad3195",
            "value": 2322154304
          }
        },
        "2915fba8acc94bda8fb139c534dee09e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_152ce6a0337b436aa9ea3600363fb945",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_675a62c8291349bab2284721b1795e89",
            "value": "â€‡2.32G/2.32Gâ€‡[00:49&lt;00:00,â€‡63.9MB/s]"
          }
        },
        "8e96ed23e9b844da8f3152bdf5988fbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2e1d9c982e14b40a610e05d3096f567": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c6d509996fa468789309064253b887b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fff7d04c3133480c882a7d04891f4299": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f057b4bf4d29479fb93f484cafad3195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "152ce6a0337b436aa9ea3600363fb945": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "675a62c8291349bab2284721b1795e89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}