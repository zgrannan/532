\clearpage
\appendix
\section{Appendix}

\subsection{Work Split}

\textbf{Zack:}
\begin{itemize}
    \item Framework API design
    \item Performance Comparison Evaluation
    \item LLM-As Judge Implementation
\end{itemize}

\vspace{1em}

\textbf{Owen:}
\begin{itemize}
    \item QA Pair Pipeline Logic
    \item Fine-tuning implementation
    \item Hallucination and Token Usage Evaluation
    \item RAG Implementation
\end{itemize}

\subsection{Additional Tables}

\begin{table}[h]
   \centering
   \caption{Papers, number of questions generated, and their corresponding page counts.}
   \label{tab:questions_and_pages}
   \begin{tabular}{p{8cm} r r}
   \toprule
   \textbf{Paper Name} & \textbf{Questions Generated} & \textbf{Pages} \\
   \midrule
   What is the Role of Small Models in the LLM Era: A Survey & 283 & 25 \\
   Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations? & 233 & 20 \\
   AgentInstruct Toward Generative Teaching With Agentic Flows & 218 & 32 \\
   Small Language Models: Survey, Measurements, and Insights & 215 & 22 \\
   LONGCITE: ENABLING LLMS TO GENERATE FINEGRAINED CITATIONS IN LONG-CONTEXT QA & 182 & 24 \\
   WHEN SCALING MEETS LLM FINETUNING: THE EFFECT OF DATA, MODEL, AND FINETUNING METHOD & 150 & 20 \\
   Finetuning LLMs for Enterprise: Practical Guidelines and Recommendations & 146 & 17 \\
   Orca-Math: Unlocking the potential of SLMs in Grade School Math & 113 & 14 \\
   From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data & 105 & 13 \\
   RAFT: Adapting Language Model to Domain Specific RAG & 104 & 12 \\
   \bottomrule
   \end{tabular}
\end{table}

\begin{table}[h]
    \centering
    \caption{Cosine Similarity Statistics for Model and Data Combinations Across Different LoRA rank R}
    \label{tab:cosine_similarity_context_sizes}
    \begin{tabular}{clccccc}
    \toprule
    \textbf{Test Set} & \textbf{R} & \textbf{Model} & \textbf{Mean} & \textbf{Std Dev} & \textbf{Min} & \textbf{Max} \\
    \midrule
    \centering Dataset 2 & 16  & Model 1 & 0.6983 & 0.1188 & 0.3872 & 0.9691 \\
    & 16  & Model 2 & 0.7670 & 0.1265 & 0.4052 & 0.9722 \\
    \addlinespace[0.5em]
    & 32  & Model 1 & 0.7710 & 0.1234 & 0.3907 & 0.9596 \\
    & 32  & Model 2 & 0.7120 & 0.1111 & 0.4141 & 0.9403 \\
    \addlinespace[0.5em]
    & 64  & Model 1 & 0.7016 & 0.1136 & 0.4067 & 0.9521 \\
    & 64  & Model 2 & 0.7077 & 0.1228 & 0.3803 & 0.9720 \\
    \addlinespace[0.5em]
    & 128 & Model 1 & 0.6949 & 0.1127 & 0.3900 & 0.9661 \\
    & 128 & Model 2 & 0.7200 & 0.1211 & 0.3914 & 0.9708 \\
    \midrule
    \centering Dataset 1 & 16  & Model 1 & 0.7669 & 0.1164 & 0.4498 & 0.9656 \\
    & 16  & Model 2 & 0.8120 & 0.1128 & 0.4776 & 0.9678 \\
    \addlinespace[0.5em]
    & 32  & Model 1 & 0.8201 & 0.1160 & 0.4627 & 0.9790 \\
    & 32  & Model 2 & 0.7633 & 0.1204 & 0.4601 & 0.9748 \\
    \addlinespace[0.5em]
    & 64  & Model 1 & 0.7643 & 0.1134 & 0.4982 & 0.9780 \\
    & 64  & Model 2 & 0.7681 & 0.1130 & 0.4603 & 0.9816 \\
    \addlinespace[0.5em]
    & 128 & Model 1 & 0.7662 & 0.1165 & 0.5043 & 0.9830 \\
    & 128 & Model 2 & 0.7747 & 0.1162 & 0.5186 & 0.9726 \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[t]
    \centering
    \caption{Unsloth LoRA Fine-Tuning Parameters for Llama 3.2 3B}
    \label{tab:lora-parameters}
    \begin{tabular}{l p{2cm} p{8.2cm}}
    \toprule
    \textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
    \midrule
    \texttt{r} & 16, 32, 64, 128 & Rank of the low-rank matrices. Higher values retain more information but increase computational load. \\
    \addlinespace[3pt]
    \texttt{target\_modules} & q\_proj, k\_proj, v\_proj, o\_proj, gate\_proj, up\_proj, down\_proj &
    Layers targeted for LoRA adaptation. \\
    \addlinespace[3pt]
    \texttt{lora\_alpha} & 16 & Scaling factor for the LoRA updates. Higher values can speed up convergence but may risk instability. \\
    \addlinespace[3pt]
    \texttt{lora\_dropout} & 0 & Probability of zeroing out elements in LoRA layers during training for regularization. A value of 0 means no dropout. \\
    \addlinespace[3pt]
    \texttt{bias} & none & Determines how biases are handled in LoRA layers. Setting to ``none'' excludes biases, optimizing memory usage. \\
    \addlinespace[3pt]
    \texttt{use\_rslora} & True & Enables Rank-Stabilized LoRA, which adjusts the scaling factor to \texttt{lora\_alpha / sqrt(r)}, improving training stability. \\
    \bottomrule
    \end{tabular}
 \end{table}
