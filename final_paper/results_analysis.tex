\section{Results}\label{sec:results}

\begin{itemize}
    \item Results from RAG system for judge
    \item Results from different model quantization methods
    \item Results from different LoRA training parameters (time permitting)
    \item Results from comparing with and without source information
\end{itemize}

\begin{table}[h]
\centering
\caption{Comparison of Fine-tuned Model vs. RAG-based Model}
\begin{tabular}{lrr}
\hline
Model & Wins & Losses \\
\hline
Finetuned - No Sources & 76 & 63 \\
\hline
\end{tabular}
\label{tab:finetuned-vs-base}
\end{table}

\subsection{Comparing Performance of finetuning with and without source information}


As discussed in Section~\ref{sec:question-generator}, we hypothesize that by including source information 
in the question when generating question-answer pairs, the resulting fine-tuned model will perform 
better than a model that was fine-tuned without source information. 

To test this hypothesis, we generated two datasets of question-answer pairs using our framework. 
To facilitate generating question-answer pairs without source information, we removed components in our pipeline 
that instructed the model to include source information in the question.
\begin{description}
    \item[Dataset 1:] Question-answer pairs generated with source information included in the question.
    \item[Dataset 2:] Question-answer pairs generated without source information included in the question.
\end{description}

We then fine-tuned Llama 3.2 3B on each dataset and evaluated the performance of the models on each of the 
test sets. 
\begin{description}
    \item[Model 1:] Fine-tuned on Dataset 1.
    \item[Model 2:] Fine-tuned on Dataset 2.
\end{description}

To evaluate the performance of the models, we leverage the cosine similarity metric to compare the semantic similarity
between the generated answer of the model and the ground truth answer generated during our synthetic data generation framework. 
We calculate the distribution of the cosine similarity scores for each model on the test set.

\begin{table}[h!]
\centering
\caption{Cosine Similarity Statistics for Model and Data Combinations}
\label{tab:cosine_similarity}
\begin{tabular}{lcccccc}
\hline
\textbf{Combination} & \textbf{Mean} & \textbf{Median} & \textbf{Std Dev} & \textbf{Min} & \textbf{Max} \\
\hline
1: Model 1 + Dataset 1 & 0.7662 & 0.7691 & 0.1165 & 0.5043 & 0.9830 \\
2: Model 2 + Dataset 1 & 0.7747 & 0.7613 & 0.1162 & 0.5186 & 0.9726 \\
3: Model 1 + Dataset 2 & 0.6940 & 0.6871 & 0.1125 & 0.3900 & 0.9661 \\
4: Model 2 + Dataset 2 & 0.7165 & 0.7179 & 0.1195 & 0.3914 & 0.9708 \\
\hline
\end{tabular}
\end{table}

Empirically, we find that when the model is fine-tuned on questions including source information (combination 1), 
the model performs ~10\% better than when the model is fine-tuned on questions without source information (combination 4)
when evaluating the mean and median cosine similarity scores. Furthermore the 20\% decrease in minimum cosine 
similarity scores in combination 3 and 4 compared to combination 1 and 2 suggests that the model may be generating 
more incorrect answers or hallucinating when source information is not included in the question.

The results support the notion that providing contextual information in the question leads to better performance than without it. 
We also find that Model 2 is able to outperform Model 1 when evaluated on Dataset 1 by a slim margin, 
suggesting that the model generalizes better than Model 1. We perform this experiment on a fine-tuned Llama 3.2 3B model with 
LoRA rank = 128. See the appendix for further results on different LoRA ranks.

\begin{table}[h!]
    \centering
    \caption{Cosine Similarity Statistics for Model and Data Combinations Across Different LoRA rank R}
    \label{tab:cosine_similarity_context_sizes}
    \begin{tabular}{clccccc}
    \toprule
    \textbf{Test Set} & \textbf{R} & \textbf{Model} & \textbf{Mean} & \textbf{Std Dev} & \textbf{Min} & \textbf{Max} \\
    \midrule
    \centering Dataset 2 & 16  & Model 1 & 0.6983 & 0.1188 & 0.3872 & 0.9691 \\ 
    & 16  & Model 2 & 0.7670 & 0.1265 & 0.4052 & 0.9722 \\ 
    \addlinespace[0.5em]
    & 32  & Model 1 & 0.7710 & 0.1234 & 0.3907 & 0.9596 \\ 
    & 32  & Model 2 & 0.7120 & 0.1111 & 0.4141 & 0.9403 \\ 
    \addlinespace[0.5em]
    & 64  & Model 1 & 0.7016 & 0.1136 & 0.4067 & 0.9521 \\ 
    & 64  & Model 2 & 0.7077 & 0.1228 & 0.3803 & 0.9720 \\ 
    \addlinespace[0.5em]
    & 128 & Model 1 & 0.6949 & 0.1127 & 0.3900 & 0.9661 \\ 
    & 128 & Model 2 & 0.7200 & 0.1211 & 0.3914 & 0.9708 \\ 
    \midrule
    \centering Dataset 1 & 16  & Model 1 & 0.7669 & 0.1164 & 0.4498 & 0.9656 \\ 
    & 16  & Model 2 & 0.8120 & 0.1128 & 0.4776 & 0.9678 \\ 
    \addlinespace[0.5em]
    & 32  & Model 1 & 0.8201 & 0.1160 & 0.4627 & 0.9790 \\ 
    & 32  & Model 2 & 0.7633 & 0.1204 & 0.4601 & 0.9748 \\ 
    \addlinespace[0.5em]
    & 64  & Model 1 & 0.7643 & 0.1134 & 0.4982 & 0.9780 \\ 
    & 64  & Model 2 & 0.7681 & 0.1130 & 0.4603 & 0.9816 \\ 
    \addlinespace[0.5em]
    & 128 & Model 1 & 0.7662 & 0.1165 & 0.5043 & 0.9830 \\ 
    & 128 & Model 2 & 0.7747 & 0.1162 & 0.5186 & 0.9726 \\ 
    \bottomrule
    \end{tabular}
\end{table}