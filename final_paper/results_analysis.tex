\section{Results}\label{sec:results}

\subsection{Comparing Performance of finetuning with and without source information}\label{sec:source-results}


As discussed in Section~\ref{sec:question-generator}, we hypothesize that by including source information
in the question when generating question-answer pairs, the resulting fine-tuned model will perform
better than a model that was fine-tuned without source information.

To test this hypothesis, we generated two datasets of question-answer pairs using our framework.
To facilitate generating question-answer pairs without source information, we removed components in our pipeline
that instructed the model to include source information in the question.
\begin{description}
    \item[Dataset 1:] Question-answer pairs generated with source information included in the question.
    \item[Dataset 2:] Question-answer pairs generated without source information included in the question.
\end{description}

We then fine-tuned Llama 3.2 3B on each dataset and evaluated the performance of the models on each of the
test sets.
\begin{description}
    \item[Model 1:] Fine-tuned on Dataset 1.
    \item[Model 2:] Fine-tuned on Dataset 2.
\end{description}

To evaluate the performance of the models, we leverage the cosine similarity metric to compare the semantic similarity
between the generated answer of the model and the ground truth answer generated during our synthetic data generation framework.
We calculate the distribution of the cosine similarity scores for each model on the test set.

\begin{table}[h!]
\centering
\caption{Cosine Similarity Statistics for Model and Data Combinations}
\label{tab:cosine_similarity}
\begin{tabular}{lcccccc}
\hline
\textbf{Combination} & \textbf{Mean} & \textbf{Median} & \textbf{Std Dev} & \textbf{Min} & \textbf{Max} \\
\hline
1: Model 1 + Dataset 1 & 0.7662 & 0.7691 & 0.1165 & 0.5043 & 0.9830 \\
2: Model 2 + Dataset 1 & 0.7747 & 0.7613 & 0.1162 & 0.5186 & 0.9726 \\
3: Model 1 + Dataset 2 & 0.6940 & 0.6871 & 0.1125 & 0.3900 & 0.9661 \\
4: Model 2 + Dataset 2 & 0.7165 & 0.7179 & 0.1195 & 0.3914 & 0.9708 \\
\hline
\end{tabular}
\end{table}

Empirically, we find that when the model is fine-tuned on questions including source information (combination 1),
the model performs ~10\% better than when the model is fine-tuned on questions without source information (combination 4)
when evaluating the mean and median cosine similarity scores. Furthermore the 20\% decrease in minimum cosine
similarity scores in combination 3 and 4 compared to combination 1 and 2 suggests that the model may be generating
more incorrect answers or hallucinating when source information is not included in the question.

The results support the notion that providing contextual information in the
question leads to better performance than without it. We also find that Model 2
is able to outperform Model 1 when evaluated on Dataset 1 by a slim margin,
suggesting that the model generalizes better than Model 1. We perform this
experiment on a fine-tuned Llama 3.2 3B model with LoRA rank = 128. See the
appendix (Table A. \ref{tab:cosine_similarity_context_sizes}) for further
results on different LoRA ranks.

\subsection{Results of QA Performance Evaluation}
\begin{table}[h]
\centering
\caption{Results of our QA performance evaluation. The \emph{Wins} column shows
the number of questions for which the LLM judge determined that the output of
the fine-tuned model was more similar to the true answer than the output of the
RAG-based approach. The \emph{Losses} column shows the number of times when the
RAG-based approach output was determined to be more similar. Because the
synthetic data generation process is not deterministic, the total number of
questions differs between setups.}
\begin{tabular}{lcrrrr}
\hline
Data Generation Model & Source Included & \texttt{r} & Wins & Losses & Win Percentage \\
\hline
gpt-4o-mini & yes & 128 & 61 & 75 & 44.9\% \\
gpt-4o-mini & yes & 16 & 56 & 80 & 41.2\% \\
gpt-4o-mini & no & 128 & 63 & 87 & 42.0\% \\
gpt-4o-mini & no & 16 & 54 & 96 & 36.0\% \\
Llama-3.1-8B-Instruct & yes & 128 & 23 & 60 & 27.7\% \\
Llama-3.1-8B-Instruct & yes & 16 & 18 & 65 & 21.7\% \\
Llama-3.1-8B-Instruct & no & 128 & 27 & 75 & 26.5\% \\
Llama-3.1-8B-Instruct & no & 16 & 25 & 77 & 24.5\% \\
\hline
\end{tabular}
\label{tab:finetuned-vs-base}
\end{table}

Table \ref{tab:finetuned-vs-base} shows the result of the QA performance
comparison described in \secref{eval:performance}. Our results show that the
performance of models that have been fine-tuned using our approach can be
competitive to RAG-based systems: the output of the fine-tuned model output was
judged to be superior to the RAG based output almost 45\% of the time in one
setup. We note that the choice of model used for synthetic data generation has a
significant impact on performance: the generated answers from setups where
gpt-4o-mini was used as the data generation model were preferred almost twice as
much compared to setups that used Llama-3.1-8B instruct. We also observed a
small but consistent impact on performance by changing LoRA rank and source
inclusion.
