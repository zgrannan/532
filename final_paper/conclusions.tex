\section{Conclusion}

In this work, we developed an agentic framework for synthetically generating
fine-tuning data. We evaluated our approach by creating a synthetic data
generation pipeline for answering QA tasks, and comparing its performance
against a RAG-based approach. Our results suggest that models finetuned using
our framework can achieve comparable performance compared to the RAG-based
approach.

As future work, we could consider evaluating our approach to generate finetuned
models for additional datasets to determine whether our technique applicable
across a range of domains. Additionally, there are several improvements we could
make to our "LLM-As-Judge" framework. In particular, enabling the judge to
return a numerical outcome to compare answer quality may yield more accurate
results compared to the current implementation which returns a binary decision.
