
@inproceedings{joshi_triviaqa_2017,
	address = {Vancouver, Canada},
	title = {{TriviaQA}: {A} {Large} {Scale} {Distantly} {Supervised} {Challenge} {Dataset} for {Reading} {Comprehension}},
	shorttitle = {{TriviaQA}},
	url = {http://aclweb.org/anthology/P17-1147},
	doi = {10.18653/v1/P17-1147},
	language = {en},
	urldate = {2024-09-27},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for           {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Joshi, Mandar and Choi, Eunsol and Weld, Daniel and Zettlemoyer, Luke},
	year = {2017},
	pages = {1601--1611},
	file = {Joshi et al. - 2017 - TriviaQA A Large Scale Distantly Supervised Chall.pdf:/Users/zgrannan/Zotero/storage/87VYT95C/Joshi et al. - 2017 - TriviaQA A Large Scale Distantly Supervised Chall.pdf:application/pdf},
}

@misc{abdin_phi-3_2024,
	title = {Phi-3 {Technical} {Report}: {A} {Highly} {Capable} {Language} {Model} {Locally} on {Your} {Phone}},
	shorttitle = {Phi-3 {Technical} {Report}},
	url = {http://arxiv.org/abs/2404.14219},
	abstract = {We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69\% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75\%, 78\% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.},
	language = {en},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and others}, 
	month = aug,
	year = {2024},
	note = {arXiv:2404.14219 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Abdin et al. - 2024 - Phi-3 Technical Report A Highly Capable Language .pdf:/Users/zgrannan/Zotero/storage/74BCMCFH/Abdin et al. - 2024 - Phi-3 Technical Report A Highly Capable Language .pdf:application/pdf},
}

@misc{yang_hotpotqa_2018,
	title = {{HotpotQA}: {A} {Dataset} for {Diverse}, {Explainable} {Multi}-hop {Question} {Answering}},
	shorttitle = {{HotpotQA}},
	url = {http://arxiv.org/abs/1809.09600},
	abstract = {Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HOTPOTQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require ﬁnding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems’ ability to extract relevant facts and perform necessary comparison. We show that HOTPOTQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.},
	language = {en},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W. and Salakhutdinov, Ruslan and Manning, Christopher D.},
	month = sep,
	year = {2018},
	note = {arXiv:1809.09600 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Yang et al. - 2018 - HotpotQA A Dataset for Diverse, Explainable Multi.pdf:/Users/zgrannan/Zotero/storage/2J4FPF49/Yang et al. - 2018 - HotpotQA A Dataset for Diverse, Explainable Multi.pdf:application/pdf},
}

@misc{jin_pubmedqa_2019,
	title = {{PubMedQA}: {A} {Dataset} for {Biomedical} {Research} {Question} {Answering}},
	shorttitle = {{PubMedQA}},
	url = {http://arxiv.org/abs/1909.06146},
	abstract = {We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial ﬁbrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artiﬁcially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. PubMedQA is the ﬁrst QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase ﬁne-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1\% accuracy, compared to single human performance of 78.0\% accuracy and majority-baseline of 55.2\% accuracy, leaving much room for improvement. PubMedQA is publicly available at https://pubmedqa.github.io.},
	language = {en},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Jin, Qiao and Dhingra, Bhuwan and Liu, Zhengping and Cohen, William W. and Lu, Xinghua},
	month = sep,
	year = {2019},
	note = {arXiv:1909.06146 [cs, q-bio]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Quantitative Biology - Quantitative Methods},
	file = {Jin et al. - 2019 - PubMedQA A Dataset for Biomedical Research Questi.pdf:/Users/zgrannan/Zotero/storage/6ETCAY5E/Jin et al. - 2019 - PubMedQA A Dataset for Biomedical Research Questi.pdf:application/pdf},
}

@article{dubey_llama_2024,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and
Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil
and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}


@article{lewis_retrieval-augmented_2020,
	title = {Retrieval-augmented generation for knowledge-intensive nlp tasks},
	volume = {33},
	journal = {Advances in Neural Information Processing Systems},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and {others}},
	year = {2020},
	pages = {9459--9474},
}

@misc{guu_realm_2020,
	title = {{REALM}: {Retrieval}-{Augmented} {Language} {Model} {Pre}-{Training}},
	shorttitle = {{REALM}},
	url = {http://arxiv.org/abs/2002.08909},
	abstract = {Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pretraining with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, ﬁne-tuning and inference. For the ﬁrst time, we show how to pretrain such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by ﬁne-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-theart models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and ﬁnd that we outperform all previous methods by a signiﬁcant margin (4-16\% absolute accuracy), while also providing qualitative beneﬁts such as interpretability and modularity.},
	language = {en},
	urldate = {2024-09-28},
	publisher = {arXiv},
	author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
	month = feb,
	year = {2020},
	note = {arXiv:2002.08909 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Guu et al. - 2020 - REALM Retrieval-Augmented Language Model Pre-Trai.pdf:/Users/zgrannan/Zotero/storage/IIJHCHNW/Guu et al. - 2020 - REALM Retrieval-Augmented Language Model Pre-Trai.pdf:application/pdf},
}

@misc{xiong_artificial_2024,
	title = {From {Artificial} {Needles} to {Real} {Haystacks}: {Improving} {Retrieval} {Capabilities} in {LLMs} by {Finetuning} on {Synthetic} {Data}},
	shorttitle = {From {Artificial} {Needles} to {Real} {Haystacks}},
	url = {http://arxiv.org/abs/2406.19292},
	abstract = {Recent studies have shown that Large Language Models (LLMs) struggle to accurately retrieve information and maintain reasoning capabilities when processing long-context inputs. To address these limitations, we propose a finetuning approach utilizing a carefully designed synthetic dataset comprising numerical key-value retrieval tasks. Our experiments on models like GPT-3.5 Turbo and Mistral 7B demonstrate that finetuning LLMs on this dataset significantly improves LLMs’ information retrieval and reasoning capabilities in longer-context settings. We present an analysis of the finetuned models, illustrating the transfer of skills from synthetic to real task evaluations (e.g., 10.5\% improvement on 20 documents MDQA at position 10 for GPT-3.5 Turbo). We also find that finetuned LLMs’ performance on general benchmarks remains almost constant while LLMs finetuned on other baseline long-context augmentation data can encourage hallucination (e.g., on TriviaQA, Mistral 7B finetuned on our synthetic data cause no performance drop while other baseline data can cause a drop that ranges from 2.33\% to 6.19\%). Our study highlights the potential of finetuning on synthetic data for improving the performance of LLMs on longer-context tasks.},
	language = {en},
	urldate = {2024-09-28},
	publisher = {arXiv},
	author = {Xiong, Zheyang and Papageorgiou, Vasilis and Lee, Kangwook and Papailiopoulos, Dimitris},
	month = jun,
	year = {2024},
	note = {arXiv:2406.19292 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Xiong et al. - 2024 - From Artificial Needles to Real Haystacks Improvi.pdf:/Users/zgrannan/Zotero/storage/F4PUFUGV/Xiong et al. - 2024 - From Artificial Needles to Real Haystacks Improvi.pdf:application/pdf},
}

@article{liu_what_2024,
	title = {{WHAT} {MAKES} {GOOD} {DATA} {FOR} {ALIGNMENT}? {A} {COMPREHENSIVE} {STUDY} {OF} {AUTOMATIC} {DATA} {SELECTION} {IN} {INSTRUCTION} {TUNING}},
	language = {en},
	author = {Liu, Wei and Zeng, Weihao and He, Keqing and Jiang, Yong and He, Junxian},
	year = {2024},
	file = {Liu et al. - 2024 - WHAT MAKES GOOD DATA FOR ALIGNMENT A COMPREHENSIV.pdf:/Users/zgrannan/Zotero/storage/DP5HF6VH/Liu et al. - 2024 - WHAT MAKES GOOD DATA FOR ALIGNMENT A COMPREHENSIV.pdf:application/pdf},
}

@misc{wu_bloomberggpt_2023,
	title = {{BloombergGPT}: {A} {Large} {Language} {Model} for {Finance}},
	shorttitle = {{BloombergGPT}},
	url = {http://arxiv.org/abs/2303.17564},
	doi = {10.48550/arXiv.2303.17564},
	abstract = {The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.},
	urldate = {2024-09-29},
	publisher = {arXiv},
	author = {Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
	month = dec,
	year = {2023},
	note = {arXiv:2303.17564 [cs, q-fin]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Quantitative Finance - General Finance},
	file = {arXiv Fulltext PDF:/Users/zgrannan/Zotero/storage/BDJG96UR/Wu et al. - 2023 - BloombergGPT A Large Language Model for Finance.pdf:application/pdf;arXiv.org Snapshot:/Users/zgrannan/Zotero/storage/TKGRJCNL/2303.html:text/html},
}

@misc{yang_fingpt_2023,
	title = {{FinGPT}: {Open}-{Source} {Financial} {Large} {Language} {Models}},
	shorttitle = {{FinGPT}},
	url = {http://arxiv.org/abs/2306.06031},
	doi = {10.48550/arXiv.2306.06031},
	abstract = {Large language models (LLMs) have shown the potential of revolutionizing natural language processing tasks in diverse domains, sparking great interest in finance. Accessing high-quality financial data is the first challenge for financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize Internet-scale financial data. In this paper, we present an open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a data-centric approach, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we showcase several potential applications as stepping stones for users, such as robo-advising, algorithmic trading, and low-code development. Through collaborative efforts within the open-source AI4Finance community, FinGPT aims to stimulate innovation, democratize FinLLMs, and unlock new opportunities in open finance. Two associated code repos are {\textbackslash}url\{https://github.com/AI4Finance-Foundation/FinGPT\} and {\textbackslash}url\{https://github.com/AI4Finance-Foundation/FinNLP\}},
	urldate = {2024-09-29},
	publisher = {arXiv},
	author = {Yang, Hongyang and Liu, Xiao-Yang and Wang, Christina Dan},
	month = jun,
	year = {2023},
	note = {arXiv:2306.06031 [cs, q-fin]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Quantitative Finance - Statistical Finance, Quantitative Finance - Trading and Market Microstructure},
	file = {arXiv Fulltext PDF:/Users/zgrannan/Zotero/storage/NMXHX6GQ/Yang et al. - 2023 - FinGPT Open-Source Financial Large Language Model.pdf:application/pdf;arXiv.org Snapshot:/Users/zgrannan/Zotero/storage/ZQEA2R36/2306.html:text/html},
}

@misc{wu_pmc-llama_2023,
	title = {{PMC}-{LLaMA}: {Towards} {Building} {Open}-source {Language} {Models} for {Medicine}},
	shorttitle = {{PMC}-{LLaMA}},
	url = {http://arxiv.org/abs/2304.14454},
	doi = {10.48550/arXiv.2304.14454},
	abstract = {Recently, Large Language Models (LLMs) have showcased remarkable capabilities in natural language understanding. While demonstrating proficiency in everyday conversations and question-answering situations, these models frequently struggle in domains that require precision, such as medical applications, due to their lack of domain-specific knowledge. In this paper, we describe the procedure for building a powerful, open-source language model specifically designed for medicine applications, termed as PMC-LLaMA. Our contributions are threefold: (i) we systematically investigate the process of adapting a general-purpose foundation language model towards medical domain, this involves data-centric knowledge injection through the integration of 4.8M biomedical academic papers and 30K medical textbooks, as well as comprehensive fine-tuning for alignment with domain-specific instructions; (ii) we contribute a large-scale, comprehensive dataset for instruction tuning. This dataset encompasses medical question-answering (QA), rationale for reasoning, and conversational dialogues, comprising a total of 202M tokens; (iii) we conduct thorough ablation studies to demonstrate the effectiveness of each proposed component. While evaluating on various public medical question-answering benchmarks, our lightweight PMCLLaMA, which consists of only 13 billion parameters, exhibits superior performance, even surpassing ChatGPT. All models, codes, datasets can be found in https://github.com/chaoyi-wu/PMC-LLaMA.},
	urldate = {2024-09-29},
	publisher = {arXiv},
	author = {Wu, Chaoyi and Lin, Weixiong and Zhang, Xiaoman and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
	month = aug,
	year = {2023},
	note = {arXiv:2304.14454 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/zgrannan/Zotero/storage/33AJBE8C/Wu et al. - 2023 - PMC-LLaMA Towards Building Open-source Language M.pdf:application/pdf;arXiv.org Snapshot:/Users/zgrannan/Zotero/storage/XGX8CUPI/2304.html:text/html},
}

@article{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	volume = {35},
	journal = {Advances in neural information processing systems},
	author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and {others}},
	year = {2022},
	pages = {27730--27744},
}

@misc{balaguer_rag_2024,
	title = {{RAG} vs {Fine}-tuning: {Pipelines}, {Tradeoffs}, and a {Case} {Study} on {Agriculture}},
	shorttitle = {{RAG} vs {Fine}-tuning},
	url = {http://arxiv.org/abs/2401.08406},
	doi = {10.48550/arXiv.2401.08406},
	abstract = {There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47\% to 72\%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.},
	urldate = {2024-09-29},
	publisher = {arXiv},
	author = {Balaguer, Angels and Benara, Vinamra and Cunha, Renato Luiz de Freitas and Filho, Roberto de M. Estevão and Hendry, Todd and Holstein, Daniel and Marsman, Jennifer and Mecklenburg, Nick and Malvar, Sara and Nunes, Leonardo O. and Padilha, Rafael and Sharp, Morris and Silva, Bruno and Sharma, Swati and Aski, Vijay and Chandra, Ranveer},
	month = jan,
	year = {2024},
	note = {arXiv:2401.08406 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/zgrannan/Zotero/storage/9R595D6E/Balaguer et al. - 2024 - RAG vs Fine-tuning Pipelines, Tradeoffs, and a Ca.pdf:application/pdf;arXiv.org Snapshot:/Users/zgrannan/Zotero/storage/3KRJNDY5/2401.html:text/html},
}

@inproceedings{shao_synthetic_2023,
	title = {Synthetic prompting: {Generating} chain-of-thought demonstrations for large language models},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Huang, Minlie and Duan, Nan and Chen, Weizhu},
	year = {2023},
	pages = {30706--30775},
}

@misc{wang_self-instruct_2023,
	title = {Self-{Instruct}: {Aligning} {Language} {Models} with {Self}-{Generated} {Instructions}},
	shorttitle = {Self-{Instruct}},
	url = {http://arxiv.org/abs/2212.10560},
	doi = {10.48550/arXiv.2212.10560},
	abstract = {Large "instruction-tuned" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at https://github.com/yizhongw/self-instruct.},
	urldate = {2024-09-30},
	publisher = {arXiv},
	author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
	month = may,
	year = {2023},
	note = {arXiv:2212.10560 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/zgrannan/Zotero/storage/XX7ARNTY/Wang et al. - 2023 - Self-Instruct Aligning Language Models with Self-.pdf:application/pdf;arXiv.org Snapshot:/Users/zgrannan/Zotero/storage/FBARW273/2212.html:text/html},
}

@misc{bai_qwen_2023,
	title = {Qwen {Technical} {Report}},
	url = {https://arxiv.org/abs/2309.16609v1},
	abstract = {Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.},
	language = {en},
	urldate = {2024-10-01},
	journal = {arXiv.org},
	author = {Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and Hui, Binyuan and Ji, Luo and Li, Mei and Lin, Junyang and Lin, Runji and Liu, Dayiheng and Liu, Gao and Lu, Chengqiang and Lu, Keming and Ma, Jianxin and Men, Rui and Ren, Xingzhang and Ren, Xuancheng and Tan, Chuanqi and Tan, Sinan and Tu, Jianhong and Wang, Peng and Wang, Shijie and Wang, Wei and Wu, Shengguang and Xu, Benfeng and Xu, Jin and Yang, An and Yang, Hao and Yang, Jian and Yang, Shusheng and Yao, Yang and Yu, Bowen and Yuan, Hongyi and Yuan, Zheng and Zhang, Jianwei and Zhang, Xingxuan and Zhang, Yichang and Zhang, Zhenru and Zhou, Chang and Zhou, Jingren and Zhou, Xiaohuan and Zhu, Tianhang},
	month = sep,
	year = {2023},
	file = {Full Text PDF:/Users/zgrannan/Zotero/storage/UFJKYZ2E/Bai et al. - 2023 - Qwen Technical Report.pdf:application/pdf},
}

@misc{gekhman_does_2024,
	title = {Does {Fine}-{Tuning} {LLMs} on {New} {Knowledge} {Encourage} {Hallucinations}?},
	url = {http://arxiv.org/abs/2405.05904},
	doi = {10.48550/arXiv.2405.05904},
	abstract = {When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model's knowledge. However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model's tendency to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently.},
	urldate = {2024-10-01},
	publisher = {arXiv},
	author = {Gekhman, Zorik and Yona, Gal and Aharoni, Roee and Eyal, Matan and Feder, Amir and Reichart, Roi and Herzig, Jonathan},
	month = may,
	year = {2024},
	note = {arXiv:2405.05904 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/zgrannan/Zotero/storage/B6T8WKRZ/Gekhman et al. - 2024 - Does Fine-Tuning LLMs on New Knowledge Encourage H.pdf:application/pdf;arXiv.org Snapshot:/Users/zgrannan/Zotero/storage/4UT3GX53/2405.html:text/html},
}

@article{zheng_judging_2023,
	title = {Judging llm-as-a-judge with mt-bench and chatbot arena},
	volume = {36},
	journal = {Advances in Neural Information Processing Systems},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and {others}},
	year = {2023},
	pages = {46595--46623},
}

@misc{ragas,
	title = {{Ragas}},
	url = {https://docs.ragas.io/en/stable/#},
	language = {en},
	urldate = {2024-10-01},
}

@misc{crewai,
	title = {{crewAI}},
	url = {https://www.crewai.com/},
	language = {en},
	urldate = {2024-10-01},
}

@misc{unslothai,
	title = {{unsloth}},
	url = {https://unsloth.ai/},
	language = {en},
	urldate = {2024-10-01},
}



@misc{mitra_agentinstruct_2024,
	title = {{AgentInstruct}: {Toward} {Generative} {Teaching} with {Agentic} {Flows}},
	url = {https://www.microsoft.com/en-us/research/publication/agentinstruct-toward-generative-teaching-with-agentic-flows/},
	abstract = {Synthetic data is becoming increasingly important for accelerating the development of language models, both large and small. Despite several successful use cases, researchers also raised concerns around model collapse and drawbacks of imitating other models. This discrepancy can be attributed to the fact that synthetic data varies in quality and diversity. Effective use of synthetic data usually requires significant human effort in curating the data. We focus on using synthetic data for post-training, specifically creating data by powerful models to teach a new skill or behavior to another model, we refer to this setting as Generative Teaching. We introduce AgentInstruct, an extensible agentic framework for automatically creating large amounts of diverse and high-quality synthetic data. AgentInstruct can create both the prompts and responses, using only raw data sources like text documents and code files as seeds. We demonstrate the utility of AgentInstruct by creating a post training dataset of 25M pairs to teach language models different skills, such as text editing, creative writing, tool usage, coding, reading comprehension, etc. The dataset can be used for instruction tuning of any base model. We post-train Mistral-7b with the data. When comparing the resulting model Orca-3 to Mistral-7b-Instruct (which uses the same base model), we observe significant improvements across many benchmarks. For example, 40\% improvement on AGIEval, 19\% improvement on MMLU, 54\% improvement on GSM8K, 38\% improvement on BBH and 45\% improvement on AlpacaEval. Additionally, it consistently outperforms other models such as LLAMA-8B-instruct and GPT-3.5-turbo.},
	author = {Mitra, Arindam and Del Corro, Luciano and Zheng, Guoqing and Mahajan, Shweti and Rouhana, Dany and Codas, Andres and Lu, Yadong and Chen, Wei-ge and Vrousgou, Olga and Rosset, Corby and Silva, Fillipe and Khanpour, Hamed and Lara, Yash and Awadallah, Ahmed},
	month = jul,
	year = {2024},
}

@misc{zhang_when_2024,
	title = {When {Scaling} {Meets} {LLM} {Finetuning}: {The} {Effect} of {Data}, {Model} and {Finetuning} {Method}},
	shorttitle = {When {Scaling} {Meets} {LLM} {Finetuning}},
	url = {https://arxiv.org/abs/2402.17193v1},
	abstract = {While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning -- full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative joint scaling law between finetuning data size and each other scaling factor; 2) LLM finetuning benefits more from LLM model scaling than pretraining data scaling, and PET parameter scaling is generally ineffective; and 3) the optimal finetuning method is highly task- and finetuning data-dependent. We hope our findings could shed light on understanding, selecting and developing LLM finetuning methods.},
	language = {en},
	urldate = {2024-10-01},
	journal = {arXiv.org},
	author = {Zhang, Biao and Liu, Zhongtao and Cherry, Colin and Firat, Orhan},
	month = feb,
	year = {2024},
	file = {Full Text PDF:/Users/zgrannan/Zotero/storage/PYSW7ZQC/Zhang et al. - 2024 - When Scaling Meets LLM Finetuning The Effect of D.pdf:application/pdf},
}

@article{zhang2024raft,
  title={Raft: Adapting language model to domain specific rag},
  author={Zhang, Tianjun and Patil, Shishir G and Jain, Naman and Shen, Sheng and Zaharia, Matei and Stoica, Ion and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2403.10131},
  year={2024}
}

@article{gupta2023targen,
  title={Targen: Targeted data generation with large language models},
  author={Gupta, Himanshu and Scaria, Kevin and Anantheswaran, Ujjwala and Verma, Shreyas and Parmar, Mihir and Sawant, Saurabh Arjun and Mishra, Swaroop and Baral, Chitta},
  journal={arXiv preprint arXiv:2310.17876},
  year={2023}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{chen2024rolesmallmodelsllm,
      title={What is the Role of Small Models in the LLM Era: A Survey}, 
      author={Lihu Chen and Gaël Varoquaux},
      year={2024},
      eprint={2409.06857},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.06857}, 
}

@misc{xu2024surveyknowledgedistillationlarge,
      title={A Survey on Knowledge Distillation of Large Language Models}, 
      author={Xiaohan Xu and Ming Li and Chongyang Tao and Tao Shen and Reynold Cheng and Jinyang Li and Can Xu and Dacheng Tao and Tianyi Zhou},
      year={2024},
      eprint={2402.13116},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.13116}, 
}

@misc{lu2024smalllanguagemodelssurvey,
      title={Small Language Models: Survey, Measurements, and Insights}, 
      author={Zhenyan Lu and Xiang Li and Dongqi Cai and Rongjie Yi and Fangming Liu and Xiwen Zhang and Nicholas D. Lane and Mengwei Xu},
      year={2024},
      eprint={2409.15790},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.15790}, 
}

@misc{wu2023autogenenablingnextgenllm,
      title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation}, 
      author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Beibin Li and Erkang Zhu and Li Jiang and Xiaoyun Zhang and Shaokun Zhang and Jiale Liu and Ahmed Hassan Awadallah and Ryen W White and Doug Burger and Chi Wang},
      year={2023},
      eprint={2308.08155},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2308.08155}, 
}

@misc{tam2024letspeakfreelystudy,
      title={Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models}, 
      author={Zhi Rui Tam and Cheng-Kuang Wu and Yi-Lin Tsai and Chieh-Yen Lin and Hung-yi Lee and Yun-Nung Chen},
      year={2024},
      eprint={2408.02442},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.02442}, 
}