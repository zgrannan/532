\section{Implementation}\label{sec:implementation}


\subsection{Synthetic Data Generation Framework}

We chose Llama 3.1 8B as our LLM in our synthetic data generation framework, which is one the largest model that could fit on a single NVIDIA-3080. Initial experiments
were also attempted with META's recent Llama 3.2 3B model <Add source> but was found to hallucinate more frequently than the 8B model and couldn't follow the instruction prompts as well.
To run inference locally, we deployed the models locally using LMStudio, an application that allows for easy deployment of large language models on local machines.

For question generation in our framework, we used JSON Mode to allow for easy extraction of the generated questions. While for answer generation, we used the default
mode to allow for diverse answers. Initial experiments were also attempted with JSON mode for both question and answer extraction but the question and answers
were found to be very basic and not diverse enough. This follows the study from \cite{tam2024letspeakfreelystudy} that found that structured generation and format constraints
generally lead to greater performance degradation in LLMs.

The embedding models and the vector database that we used were nomic-embed-text-v1.5 and ChromaDB respectively.

\subsection{Fine-tuning}

\begin{itemize}
    \item Talk about technologies used, unsloth, LMStudio, etc.
    \item Dive into Unsloth, parameters chosen for finetuning, etc.
\end{itemize}

\subsection{Evaluation Approach}
\begin{itemize}
\item How we evaluated the validity of our framework -> Hallucination check on generated data
\item LM as a judge to compare finetuned LLM vs baseline RAG
\end{itemize}

\subsubsection{Hallucination Check}

As Large Language Models are increasingly relevant in Generative AI applications, the issue of hallucination has become a growing concern.
Hallucination refers to the generation of text that is not supported by the input data, leading to incorrect or misleading information.
To ensure the validity of our synthetic data generation framework, we implemented a hallucination check to evaluate the quality of the generated data.
Specifically, we leverage HHEM-2.1-Open % https://huggingface.co/vectara/hallucination_evaluation_model#using-hhem-21-open
as our hallucination detection model, which has been shown to perform better than GPT-4 on hallucination detection tasks.

This model outputs a factual consistency score between 0 and 1, in which the higher the score, the more consistent the generated text is with the input data.
To produce scores, both a response and the context used to generate the response are required as inputs to the model. We used the answer generated by 
our framework as the response and the chunk of text from the source paper as the context. The factual consistency score is then calculated by the model.

\begin{table}[h]
\centering
\caption{Factual Consistency Score Distribution}
\begin{tabular}{lr}
\hline
Metric & Score \\
\hline
Mean & 0.78 \\
Median & 0.81 \\
Standard Deviation & 0.14 \\
Minimum & 0.51 \\
Maximum & 0.99 \\
\hline
\end{tabular}
\label{tab:factual-consistency-scores}
\end{table}

\subsubsection{Evaluating Fine-tuned Model}

