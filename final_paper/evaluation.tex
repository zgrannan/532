\section{Evalution Setup}\label{sec:evaluation}

\subsection{Hallucination Check}

As Large Language Models are increasingly relevant in Generative AI applications, the issue of hallucination has become a growing concern.
Hallucination refers to the generation of text that is not supported by the input data, leading to incorrect or misleading information.
To ensure the validity of our synthetic data generation framework, we implemented a hallucination check to evaluate the quality of the generated data.
Specifically, we leverage HHEM-2.1-Open % https://huggingface.co/vectara/hallucination_evaluation_model#using-hhem-21-open
as our hallucination detection model, which has been shown to perform better than GPT-4 on hallucination detection tasks.

This model outputs a factual consistency score between 0 and 1, in which the higher the score, the more consistent the generated text is with the input data.
To produce scores, both a response and the context used to generate the response are required as inputs to the model. We used the answer generated by
our framework as the response and the chunk of text from the source paper as the context. The factual consistency score is then calculated by the model.

\begin{table}[h]
\centering
\caption{Factual Consistency Score Distribution}
\begin{tabular}{lr}
\hline
Metric & Score \\
\hline
Mean & 0.78 \\
Median & 0.81 \\
Standard Deviation & 0.14 \\
Minimum & 0.51 \\
Maximum & 0.99 \\
\hline
\end{tabular}
\label{tab:factual-consistency-scores}
\end{table}

\subsection{Evaluating QA Performance of Fine-tuned Models}\label{sec:eval:performance}

To evaluate the performance of the fine-tuned models produced by our approach, we
constructed a test set of questions based on the source data, and compared the
answer results from the fine-tuned models against a RAG-based alternative. For
the alternative approach, we loaded the source documents into a vector store and
injected data from the store into the prompt using an embeddings similarity
metric. We used Llama 3.2 3B as the LLM model in the RAG setup (i.e. the same
model used as a base for generating fine-tuned models).

We used gpt-4o as a "judge" to determine which answer is more correct. In
particular, the judge model received the following inputs:
\begin{enumerate}
    \item The question $q$ and \emph{correct} answer $a_\mathit{true}$ from the test set
    \item The answer $a_\mathit{FT}$ generated by instructing the finetuned model to answer $q$
    \item The answer $a_\mathit{RAG}$ generated by instructing the RAG system to answer $q$
\end{enumerate}

The judge is prompted to do chain-of-thought reasoning, and then return a binary
decision that either $a_\mathit{FT}$ is closer to $a_\mathit{true}$ (a "win"),
or if $a_\mathit{RAG}$ is closer to $a_\mathit{true}$ (a "loss").

For our evaluation, we evaluated the performance of eight different fine-tuned
models, constructed by varying three parameters:

\begin{enumerate}
    \item \textbf{Data Generation Model} The model used for the data generation
    agents: either gpt-4o-mini or Llama-3.1-8B-Instruct
    \item \textbf{Source Inclusion} Whether the pipeline included a step to
    augment questions with information about the source of the question (e.g.
    the paper which the question refers to)
    \item \textbf{LoRA fine-tuning rank} The rank parameter (\texttt{r}) used in
    LoRA: either 16 or 128
\end{enumerate}
