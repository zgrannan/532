\section{Introduction}
Many commercial applications of large language models (LLMs) involve
domain-specific tasks requiring information not present in the LLMâ€™s
training data. Retrieval-augmented generation (RAG)
\citep{lewis_retrieval-augmented_2020} is a popular technique for making such
information available. In RAG, the application injects data (e.g., from a
vector database) into the system prompt, with the intention that the LLM can use this
information to produce a response that incorporates relevant domain-specific
data. An advantage of the RAG-based approach is its flexibility and ease of use:
the application can use its own logic to determine how to enhance the prompts.
However, RAG systems have corresponding disadvantages. In particular,
incorporating a RAG architecture increases application and infrastructure
complexity.

Fine-tuning presents another approach. In the fine-tuning paradigm, the
parameters of the LLM itself are modified by training the LLM on additional
content. The fine-tuned LLM can be used as a "drop-in" replacement for a base
language model: no changes to architecture or software are required.

Previous work \citep{balaguer_rag_2024,yang_fingpt_2023,wu_pmc-llama_2023} has
proposed methods for fine-tuning specialised LLMs, and techniques have been
developed to improve general reasoning abilities with synthetic data
\citep{shao_synthetic_2023,wang_self-instruct_2023}. However, there is
relatively less literature evaluating the performance of state-of-the-art
synthetic data generation techniques for fine-tuning domain-specific LLMs.
AgentInstruct \citep{mitra_agentinstruct_2024} provides a promising agent-based
approach, however the authors did not perform any evaluation for domain-specific
cases, and the AgentInstruct code is not readily available.

For this project, we make the following contributions:

\begin{enumerate}
\item We develop an agentic framework (in the style of AgentInstruct) for the purpose
of generating synthetic data for LLM fine-tuning (\secref{framework})
\item We use our framework to develop a synthetic data generation pipeline for question-answering tasks
(\secref{pipeline})
\item We use our pipeline to fine-tune various models to perform QA tasks (\secref{implementation}), and show that
the resulting models performance is comparable to RAG-based approaches (Sec. \ref{sec:evaluation}, \ref{sec:results})
\end{enumerate}

Our implementation, including the pipeline and scripts used used in our
evaluation is available at \url{https://github.com/zgrannan/532}.

