\section{Evaluation}
We will evaluate our approach with respect to two research questions:

\begin{itemize}
\item RQ1: How does our approach compare to RAG-based approach w.r.t. QA tasks?
\item RQ2: How does our approach compare to RAG-based approach w.r.t. hallucinations?
\end{itemize}

To evaluate performance, we will compare responses on a curated test dataset
against a baseline retrieval-augmented generation system and evaluate the
responses using an oracle model: previous research has shown that LLMs are
effective at approximating human preferences related to chatbot output
\citep{zheng_judging_2023}. We will leverage existing tools such as Ragas
\citep{ragas} for evaluation.

To evaluate our approach, we will use off-the-shelf evaluation suites. For example, we could consider one of the following domain-specific QA benchmarks:

\begin{itemize}
\item TriviaQA \citep{joshi_triviaqa_2017} - A dataset consisting of 650k question-answer-evidence triples, sourced from trivia enthusiasts.
\item HotpotQA \citep{yang_hotpotqa_2018} - Contains 113K question-answer pairs from Wikipedia, alongside supporting evidence.
\item PubmedQA \citep{jin_pubmedqa_2019} - Nearly 300k QA instances derived from PubMed abstracts. Each instance takes the form of a research question with possible answers yes/no/maybe.
\end{itemize}

During our evaluation, we will fine-tune existing open-source models such as
Phi-3 \citep{abdin_phi-3_2024} and Llama 3 \citep{dubey_llama_2024}, etc. We
will perform our approach on different sizes (e.g., for the Phi-3 series, 3.8B,
7B, and 14B models are available), considering how the different models perform
for RAG and fine-tuning approaches. We will also consider different quantization
levels.
